"""
PerceptionEngine: å¤šæ¨¡æ€ç‰¹å¾æå–å±‚
- è§†è§‰ç‰¹å¾: MediaPipe Face Landmarker (EAR, MAR, å¤´éƒ¨å§¿æ€)
- éŸ³é¢‘ç‰¹å¾: openSMILE eGeMAPS (å“åº¦, éŸ³è°ƒ)
- è¾“å‡ºæ ¼å¼: ä¸¥æ ¼éµå¾ªé¡¹ç›®åè®®
"""

import cv2
import opensmile
import numpy as np
from datetime import datetime
import os
import time

class PerceptionEngine:
    """å©‰æ™´çš„æ„ŸçŸ¥å™¨å®˜ - ç‰¹å¾å·¥ç¨‹æ ¸å¿ƒç±»ï¼ˆé€‚é…æ–°ç‰ˆMediaPipeï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ„ŸçŸ¥å¼•æ“"""
        # ---------- è§†è§‰æ¨¡å—åˆå§‹åŒ–ï¼ˆæ–°ç‰ˆMediaPipe Tasks APIï¼‰----------
        import mediapipe as mp
        from mediapipe.tasks import python
        from mediapipe.tasks.python.vision import FaceLandmarker, FaceLandmarkerOptions, RunningMode
        import urllib.request
        import os
        
        self.mp = mp
        
        # ä¸‹è½½ Face Landmarker æ¨¡å‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        model_url = "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
        model_path = os.path.join(os.path.dirname(__file__), "face_landmarker.task")
        
        if not os.path.exists(model_path):
            print("ğŸ“¥ ä¸‹è½½ Face Landmarker æ¨¡å‹ï¼ˆçº¦ 2MBï¼‰...")
            urllib.request.urlretrieve(model_url, model_path)
            print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
        
        # åˆ›å»ºäººè„¸ç‰¹å¾ç‚¹æ£€æµ‹å™¨
        options = FaceLandmarkerOptions(
            base_options=python.BaseOptions(model_asset_path=model_path),
            running_mode=RunningMode.IMAGE,
            output_face_blendshapes=False,
            output_facial_transformation_matrixes=False,
            num_faces=1
        )
        self.face_landmarker = FaceLandmarker.create_from_options(options)
        
        # ---------- éŸ³é¢‘æ¨¡å—åˆå§‹åŒ– ----------
        try:
            self.smile = opensmile.Smile(
                feature_set=opensmile.FeatureSet.eGeMAPSv02,
                feature_level=opensmile.FeatureLevel.Functionals,
            )
            self.audio_ready = True
        except Exception as e:
            print(f"âš ï¸ openSMILEåˆå§‹åŒ–å¤±è´¥: {e}")
            print("éŸ³é¢‘ç‰¹å¾å°†è¿”å›é»˜è®¤å€¼")
            self.audio_ready = False
        
        # ---------- çŠ¶æ€å˜é‡ï¼ˆè‡ªé€‚åº”çœ¨çœ¼ï¼‰----------
        self.ear_history = []          # EARå†å²ï¼Œç”¨äºé˜ˆå€¼è®¡ç®—
        self.ear_baseline = 0.3        # æ­£å¸¸ççœ¼åŸºçº¿
        self.ear_threshold = 0.2       # å½“å‰çœ¨çœ¼é˜ˆå€¼
        self.blink_events = []         # çœ¨çœ¼æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆ1åˆ†é’Ÿçª—å£ï¼‰
        self.last_ear = 0.0            # ä¸Šä¸€å¸§EAR
        self.ear_history_size = 100    # å†å²çª—å£å¤§å°ï¼ˆå¸§æ•°ï¼‰

    def extract_visual(self, image_path):
        """
        ä»å›¾ç‰‡æå–è§†è§‰ç‰¹å¾
        è¾“å…¥: å›¾ç‰‡è·¯å¾„
        è¾“å‡º: ç¬¦åˆåè®®çš„visualå­—å…¸
        """
        visual = {
            "ear": 0.0,
            "mar": 0.0,
            "blink_count": 0,
            "head_pose": {
                "pitch": 0.0,
                "yaw": 0.0,
                "roll": 0.0
            }
        }
        
        if not os.path.exists(image_path):
            return visual
        
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(image_path)
        if img is None:
            return visual
        
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img.shape[:2]
        
        # è½¬æ¢ä¸ºMediaPipeå›¾åƒæ ¼å¼
        mp_image = self.mp.Image(image_format=self.mp.ImageFormat.SRGB, data=img_rgb)
        
        # æ£€æµ‹äººè„¸ç‰¹å¾ç‚¹
        detection_result = self.face_landmarker.detect(mp_image)
        
        if detection_result.face_landmarks:
            face_landmarks = detection_result.face_landmarks[0]
                    
            # è®¡ç®—EARï¼ˆåŒçœ¼å¹³å‡ï¼‰
            left_ear = self._calculate_ear(face_landmarks, [33, 160, 158, 133, 153, 144], w, h)
            right_ear = self._calculate_ear(face_landmarks, [362, 385, 387, 263, 373, 380], w, h)
            current_ear = (left_ear + right_ear) / 2.0
            visual["ear"] = round(current_ear, 4)
            
            # è®¡ç®—MAR
            visual["mar"] = round(self._calculate_mar_robust(face_landmarks, w, h), 4)
            
            # æ›´æ–°çœ¨çœ¼è®¡æ•°
            blink_count = self._update_blink_count(current_ear)
            visual["blink_count"] = blink_count
            
            visual["head_pose"] = self._estimate_head_pose(face_landmarks, w, h)
        
        return visual
    
    def extract_audio(self, audio_path):
        """
        ä»éŸ³é¢‘æå–å£°å­¦ç‰¹å¾ï¼ˆä¿®æ­£ç‰ˆï¼‰
        """
        audio = {
            "is_speaking": False,
            "loudness": 0.0,
            "pitch_avg": 0.0
        }
        
        if not self.audio_ready or not os.path.exists(audio_path):
            return audio
        
        try:
            features = self.smile.process_file(audio_path)
            
            if 'loudness_sma3_amean' in features.columns:
                raw_loudness = float(features['loudness_sma3_amean'].iloc[0])
                # print(f"  ğŸ”Š [Audio Debug] åŸå§‹å“åº¦: {raw_loudness:.4f}")
                
                # é’ˆå¯¹ä½å¢ç›Šéº¦å…‹é£ä¼˜åŒ–çš„é˜ˆå€¼
                if raw_loudness < 0.02:  # åªæœ‰æå°çš„æ³¢åŠ¨æ‰ç®—ç»å¯¹é™éŸ³
                    loudness_norm = 0.0
                elif raw_loudness < 0.2: # ä½ çš„ 0.11 ä¼šè½åœ¨è¿™é‡Œ
                    # æ˜ å°„ 0.02~0.2 åˆ° 0.1~0.5 ä¹‹é—´
                    loudness_norm = 0.1 + (raw_loudness - 0.02) / 0.18 * 0.4
                else:
                    # è¶…è¿‡ 0.2 å°±ç®—å¤§å£°äº†
                    loudness_norm = min(1.0, 0.5 + (raw_loudness - 0.2) / 0.8)
                
                audio["loudness"] = round(float(loudness_norm), 4)
                
                # åˆ¤å®šè¯´è¯çš„é˜ˆå€¼ä¹Ÿç›¸åº”ä¸‹è°ƒ
                # åªè¦åŸå§‹å“åº¦å¤§äº 0.05 ä¸”æœ‰éŸ³è°ƒï¼Œå°±è®¤ä¸ºæ˜¯åœ¨è¯´è¯
                audio["is_speaking"] = raw_loudness > 0.05
            
            # 2. å¤„ç†éŸ³è°ƒ (Pitch)
            if 'F0semitoneFrom27.5Hz_sma3nz_amean' in features.columns:
                pitch_semitone = float(features['F0semitoneFrom27.5Hz_sma3nz_amean'].iloc[0])
                # å¦‚æœ pitch_semitone ä¸º 0ï¼Œè¯´æ˜æ²¡æ£€æµ‹åˆ°åŸºé¢‘ï¼ˆå¯èƒ½åªæ˜¯æ‚éŸ³ï¼‰
                if pitch_semitone > 0:
                    pitch_hz = 27.5 * (2 ** (pitch_semitone / 12))
                    audio["pitch_avg"] = round(pitch_hz, 2)
                    # åŒé‡ç¡®è®¤ï¼šæœ‰éŸ³è°ƒæ‰ç®—è¯´è¯
                    if audio["loudness"] > 0.1:
                        audio["is_speaking"] = True
                
        except Exception as e:
            print(f"éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
        
        return audio
    
    def analyze(self, image_path=None, audio_path=None):
        perception = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "visual": {
                "ear": 0.0,
                "mar": 0.0,
                "blink_count": 0,
                "head_pose": {"pitch": 0.0, "yaw": 0.0, "roll": 0.0}
            },
            "audio": {
                "is_speaking": False,
                "loudness": 0.0,
                "pitch_avg": 0.0
            }
        }
        
        if image_path:
            perception["visual"] = self.extract_visual(image_path)  # è¿™é‡Œè¦†ç›–blink_count
        
        if audio_path:
            perception["audio"] = self.extract_audio(audio_path)
        
        return perception
    
    # ---------- ç§æœ‰è¾…åŠ©æ–¹æ³• ----------
    def _calculate_ear(self, landmarks, eye_idx, w, h):
        """è®¡ç®—çœ¼ç›çºµæ¨ªæ¯” (EAR)"""
        points = []
        for idx in eye_idx:
            lm = landmarks[idx]
            points.append([lm.x * w, lm.y * h])
        points = np.array(points)
        
        ear = (np.linalg.norm(points[1] - points[5]) + 
               np.linalg.norm(points[2] - points[4])) / \
              (2.0 * np.linalg.norm(points[0] - points[3]) + 1e-6)
        return ear
    
    def _calculate_mar_robust(self, landmarks, w, h):
        """
        é²æ£’æ€§MARè®¡ç®— - 6ç‚¹å¹³å‡ + IQRå¼‚å¸¸å€¼å‰”é™¤
        """
       # ä¸Šå”‡å…³é”®ç‚¹ï¼ˆ6ä¸ªï¼‰- åŒ…å«å†…ä¾§ç‚¹308
        upper_points = [13, 312, 308, 318, 14]  # 308é‡å¤æ˜¯ä¸ºäº†å¯¹é½
        # ä¸‹å”‡å…³é”®ç‚¹ï¼ˆ6ä¸ªï¼‰- åŒ…å«å†…ä¾§ç‚¹78
        lower_points = [14, 85, 78, 314, 13]    # 78æ˜¯ä¸‹å”‡å†…ä¾§
        
        vertical_dists = []
        for up, low in zip(upper_points, lower_points):
            up_pt = np.array([landmarks[up].x * w, landmarks[up].y * h])
            low_pt = np.array([landmarks[low].x * w, landmarks[low].y * h])
            dist = np.linalg.norm(up_pt - low_pt)
            vertical_dists.append(dist)
        
        # IQRå¼‚å¸¸å€¼å‰”é™¤
        if len(vertical_dists) > 0:
            q1, q3 = np.percentile(vertical_dists, [25, 75])
            iqr = q3 - q1
            valid_dists = [d for d in vertical_dists 
                        if d > q1 - 1.5 * iqr and d < q3 + 1.5 * iqr]
            vertical = np.mean(valid_dists) if valid_dists else np.mean(vertical_dists)
        else:
            vertical = 0
        
        # å˜´è§’æ°´å¹³è·ç¦»ï¼ˆä¸å˜ï¼‰
        left_pt = np.array([landmarks[61].x * w, landmarks[61].y * h])
        right_pt = np.array([landmarks[291].x * w, landmarks[291].y * h])
        horizontal = np.linalg.norm(left_pt - right_pt)
        
        mar = vertical / (horizontal + 1e-6)
        return mar
    
    def _estimate_head_pose(self, face_landmarks, w, h):
        """
        å¤´éƒ¨å§¿æ€ä¼°è®¡
        """
        # 1. å®šä¹‰æ ‡å‡† 3D æ¨¡å‹ç‚¹ (ä»¥é¼»å°–ä¸ºåŸç‚¹)
        # é‡‡ç”¨æ›´ç¬¦åˆ MediaPipe æŠ•å½±æ¯”ä¾‹çš„åæ ‡
        model_points = np.array([
            [0.0, 0.0, 0.0],             # 1. é¼»å°–
            [0.0, 38.0, -15.0],          # 152. ä¸‹å·´ (å‘ä¸‹ä¸ºæ­£)
            [-28.0, -32.0, -25.0],       # 33. å·¦çœ¼å¤–è§’ (å‘ä¸Šä¸ºè´Ÿ, å‘å·¦ä¸ºè´Ÿ)
            [28.0, -32.0, -25.0],        # 263. å³çœ¼å¤–è§’
            [-20.0, 18.0, -15.0],        # 61. å·¦å˜´è§’
            [20.0, 18.0, -15.0]          # 291. å³å˜´è§’
        ], dtype=np.float64)

        # 2. æå–å¯¹åº”çš„ 2D åƒç´ ç‚¹ (å¿…é¡»æ˜¯ 6 ä¸ª)
        image_points = np.array([
            [face_landmarks[1].x * w, face_landmarks[1].y * h],
            [face_landmarks[152].x * w, face_landmarks[152].y * h],
            [face_landmarks[33].x * w, face_landmarks[33].y * h],
            [face_landmarks[263].x * w, face_landmarks[263].y * h],
            [face_landmarks[61].x * w, face_landmarks[61].y * h],
            [face_landmarks[291].x * w, face_landmarks[291].y * h]
        ], dtype=np.float64)

        # 3. ç›¸æœºå†…å‚
        focal_length = w
        center = (w/2, h/2)
        camera_matrix = np.array([
            [focal_length, 0, center[0]],
            [0, focal_length, center[1]],
            [0, 0, 1]
        ], dtype="double")
        dist_coeffs = np.zeros((4, 1))

        # 4. æ±‚è§£ PnP
        success, rot_vec, trans_vec = cv2.solvePnP(
            model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE
        )

        if success:
            # å°†æ—‹è½¬å‘é‡è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
            rmat, _ = cv2.Rodrigues(rot_vec)
            
            # è®¡ç®—æ¬§æ‹‰è§’ (Pitch, Yaw, Roll)
            # ä½¿ç”¨æ›´ç›´æ¥çš„ä¸‰è§’å‡½æ•°åˆ†è§£ï¼Œé¿å… decomposeProjectionMatrix çš„å¤šè§£å¹²æ‰°
            sy = np.sqrt(rmat[0,0] * rmat[0,0] +  rmat[1,0] * rmat[1,0])
            singular = sy < 1e-6

            if not singular:
                pitch = np.arctan2(rmat[2,1] , rmat[2,2]) * (180/np.pi)
                yaw = np.arctan2(-rmat[2,0], sy) * (180/np.pi)
                roll = np.arctan2(rmat[1,0], rmat[0,0]) * (180/np.pi)
            else:
                pitch = np.arctan2(-rmat[1,2], rmat[1,1]) * (180/np.pi)
                yaw = np.arctan2(-rmat[2,0], sy) * (180/np.pi)
                roll = 0

            return {
                "pitch": round(float(pitch), 2),
                "yaw": round(float(yaw), 2),
                "roll": round(float(roll), 2)
            }

        return {"pitch": 0.0, "yaw": 0.0, "roll": 0.0}
    
    def _update_blink_count(self, ear):
        """
        çœ¨çœ¼æ£€æµ‹ï¼šè‡ªé€‚åº”é˜ˆå€¼ + 1åˆ†é’Ÿæ»‘åŠ¨çª—å£
        """
        current_time = time.time()
        
        # 1. æ›´æ–°EARå†å²
        self.ear_history.append(ear)
        if len(self.ear_history) > self.ear_history_size:
            self.ear_history.pop(0)
        
        # 2. åŠ¨æ€æ›´æ–°é˜ˆå€¼ï¼ˆæ¯30å¸§ï¼‰
        if len(self.ear_history) >= 30:
            recent_ears = self.ear_history[-30:]
            self.ear_baseline = np.percentile(recent_ears, 75)
            self.ear_threshold = self.ear_baseline * 0.6
        
        # 3. çœ¨çœ¼æ£€æµ‹ï¼ˆä¸‹é™æ²¿è§¦å‘ï¼‰
        if ear < self.ear_threshold and self.last_ear >= self.ear_threshold:
            self.blink_events.append(current_time)
            # print(f"ğŸ‘ï¸ çœ¨çœ¼: EAR={ear:.3f}, é˜ˆå€¼={self.ear_threshold:.3f}")
        
        # 4. åªä¿ç•™æœ€è¿‘60ç§’
        self.blink_events = [t for t in self.blink_events 
                            if current_time - t <= 60]
        
        self.last_ear = ear
        return len(self.blink_events)