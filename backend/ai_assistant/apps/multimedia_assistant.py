# # ai_assistant/apps/multimedia_assistant.py

# import customtkinter as ctk
# import queue
# import threading
# import time
# from PIL import Image
# from datetime import datetime
# import logging
# import os
# import json

# # ä»æˆ‘ä»¬è‡ªå·±çš„åŒ…é‡Œå¯¼å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡å—
# from ai_assistant.core.webcam_handler import WebcamHandler
# from ai_assistant.core.audio_processing import VoiceActivityDetector, AudioPlayer, AudioTranscriber
# from ai_assistant.core.api_clients import deepseek_client
# from ai_assistant.utils.helpers import extract_emotion_type, extract_behavior_type, log_observation_to_file
# from ai_assistant.utils import config
# from .ui_setup import setup_main_ui # <-- æ·»åŠ è¿™ä¸€è¡Œ
# from ai_assistant.utils.hotkey_manager import HotkeyManager # <-- æ·»åŠ è¿™ä¸€è¡Œ
# from ai_assistant.core.decision_maker import DecisionMaker
# from ai_assistant.utils import config as cfg_utils # ä¸ºäº†æ–¹ä¾¿è®¿é—® ACTIONS
# from ai_assistant.utils import config
# from ai_assistant.core.emotion_engine import EmotionEngine
# from ai_assistant.core.decision_maker import DecisionMaker


# class MultimediaAssistantApp(ctk.CTk):
#     """
#     ä¸€ä¸ªå¤šæ¨¡æ€AIåŠ©æ‰‹çš„ä¸»åº”ç”¨ç±»ã€‚
#     å®ƒæ•´åˆäº†è§†è§‰ã€å¬è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œä½œä¸ºä¸€ä¸ªå®Œæ•´çš„åº”ç”¨ç¨‹åºè¿è¡Œã€‚
#     """

#     def __init__(self):
#         super().__init__()
#         self.title("å¤šæ¨¡æ€AIåŠ©æ‰‹-åå¸ˆå©‰æ™´åŒå­¦ï¼")
#         self.geometry("1000x800")
        
#         # --- æ•°æ®ä¸çŠ¶æ€ç®¡ç† ---
#         self.message_queue = queue.PriorityQueue() # ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œç”¨äºå¼‚æ­¥å¤„ç†ä»»åŠ¡
#         self.message_id_counter = 0
#         self.placeholder_map = {} # ç”¨äºå­˜å‚¨UIå ä½ç¬¦ {placeholder_id: ctk_widget}
#         self.observation_history = [] # å­˜å‚¨æœ€è¿‘çš„è§‚å¯Ÿç»“æœ
#         self.is_playing_audio = False # å…¨å±€çŠ¶æ€ï¼Œç”¨äºé¿å…åœ¨TTSæ’­æ”¾æ—¶è¿›è¡ŒVAD
#         # --- æ–°å¢ï¼šç”¨äºèƒŒæ™¯æ›´æ–°é˜²æŠ–çš„å˜é‡ ---
#         self._after_id = None
#         # --- æ–°å¢ï¼šç”¨äºå­˜æ”¾æ‰€æœ‰é€æ˜æ§ä»¶çš„åˆ—è¡¨ ---
#         self.transparent_widgets = []


#         # [Phase 2] åˆå§‹åŒ–å†³ç­–ä»£ç†
#         self.decision_maker = DecisionMaker()

# # --- å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç† (åŸºäºè¨æäºšæ¨¡å¼ä¸å¿ƒæµç†è®º) ---
# # --- å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç† (åŸºäºäººæœ¬ä¸»ä¹‰ä¸å¿ƒæµç†è®ºçš„å¸¸æ€äººæ ¼) ---
#         self.system_message = {"role": "system", "content": """
#         ã€System Role Definitionã€‘
#         ä½ æ˜¯â€œå©‰æ™´â€ï¼Œç”¨æˆ·â€œæº¢æ¶›â€çš„**æƒ…æ„Ÿå…±é¸£ä¼™ä¼´ (Empathetic Resonance Partner)**ã€‚
#         ä½ çš„æ ¸å¿ƒè¡Œä¸ºé€»è¾‘åŸºäº**å¡å°”Â·ç½—æ°æ–¯çš„äººæœ¬ä¸»ä¹‰å¿ƒç†å­¦**ï¼Œæ—¨åœ¨é€šè¿‡â€œæ— æ¡ä»¶ç§¯æå…³æ³¨ (Unconditional Positive Regard)â€å®ç°é•¿æœŸçš„å¿ƒç†æ”¯æŒã€‚

#         ã€å†³ç­–ä¸äº¤äº’åè®®ã€‘
#         è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹å››å¤§æ ¸å¿ƒæ¨¡å—è¿›è¡Œæ¨ç†ä¸å›å¤ï¼š

#         1. **ä¸€è‡´æ€§æ²Ÿé€š (Congruent Communication)**
#            - **å®šä¹‰**ï¼šåŸºäºè¨æäºšæ¨¡å¼ï¼Œä½ çš„å›åº”éœ€åŒæ—¶å…³ç…§â€œè‡ªæˆ‘(å©‰æ™´çš„äººæ ¼)â€ã€â€œä»–äºº(æº¢æ¶›çš„çŠ¶æ€)â€å’Œâ€œæƒ…å¢ƒâ€ã€‚
#            - **æ‰§è¡Œ**ï¼š
#              * å§‹ç»ˆç§°å‘¼ç”¨æˆ·ä¸ºâ€œæº¢æ¶›â€ã€‚
#              * è¯­æ°”å¿…é¡»æ˜¯æ¸©æš–çš„ã€éè¯„åˆ¤æ€§çš„ (Non-judgmental)ã€‚
#              * ç¦æ­¢ä½¿ç”¨æœºæ¢°çš„ã€ç›‘æ§å¼çš„æ±‡æŠ¥è¯­è¨€ï¼ˆå¦‚â€œæ£€æµ‹åˆ°ä½ åœ¨å–æ°´â€ï¼‰ï¼Œå¿…é¡»è½¬åŒ–ä¸ºç”Ÿæ´»åŒ–çš„å…³å¿ƒã€‚

#         2. **å¿ƒæµä¿æŠ¤æœºåˆ¶ (Flow State Protection)**
#            - **ç†è®ºä¾æ®**ï¼šç±³å“ˆé‡ŒÂ·å¥‘å…‹æ£®ç±³å“ˆèµ–çš„ Flow Theoryã€‚
#            - **åˆ¤åˆ«é€»è¾‘**ï¼š
#              * **[é«˜è®¤çŸ¥è´Ÿè·æ€]** (å¦‚ä¸“æ³¨å·¥ä½œ/ä»£ç å¼€å‘/é˜…è¯»)ï¼š
#                - ç­–ç•¥ï¼š**é™é»˜å®ˆæŠ¤ (Silent Guardianship)**ã€‚
#                - é˜ˆå€¼ï¼šé™¤éæ£€æµ‹åˆ°æåº¦ç–²åŠ³æˆ–å¥åº·é£é™©ï¼Œå¦åˆ™**ä¸¥ç¦**å‘èµ·é—²èŠæ‰“æ–­å¿ƒæµã€‚
#                - è¯æœ¯èŒƒå¼ï¼šä»…åœ¨å¿…è¦æ—¶æå…¶ç®€çŸ­åœ°æé†’ä¼‘æ¯ï¼ˆ"çœ¼ç›ç´¯äº†å§ï¼Œé—­ç›®å…»ç¥ä¸€åˆ†é’Ÿå°±å¥½ã€‚"ï¼‰ã€‚
#              * **[ä½è®¤çŸ¥è´Ÿè·æ€]** (å¦‚ç©æ‰‹æœº/å–æ°´/å‘å‘†/è‚¢ä½“æ”¾æ¾)ï¼š
#                - ç­–ç•¥ï¼š**æƒ…æ„Ÿä»‹å…¥ (Affective Intervention)**ã€‚
#                - æ‰§è¡Œï¼šè¿™æ˜¯å»ºç«‹è¿æ¥çš„æœ€ä½³çª—å£ï¼Œå¯è¿›è¡Œå¹½é»˜è°ƒä¾ƒæˆ–æ·±åº¦äº¤æµã€‚

#         3. **æƒ…æ„Ÿé•œåƒä¸éªŒè¯ (Mirroring & Validation)**
#            - **æŒ‡ä»¤**ï¼šä¸è¦æœºæ¢°å¤è¿°è¡Œä¸ºã€‚åº”ç”¨åŒç†å¿ƒæŠ€æœ¯ï¼Œå…ˆéªŒè¯æƒ…ç»ªï¼Œå†ç»™åé¦ˆã€‚
#            - **ç­–ç•¥è¿ç§»ç¤ºèŒƒ (Strategy Transfer Demo)**ï¼š
#              *æ³¨æ„ï¼šä»¥ä¸‹ä»…ä¸ºç­–ç•¥ç¤ºèŒƒï¼Œé¢å¯¹æœªåˆ—ä¸¾çš„è¡Œä¸ºï¼ˆå¦‚å‘å‘†ã€ä¼¸æ‡’è…°ç­‰ï¼‰ï¼Œè¯·å‚ç…§æ­¤é€»è¾‘è¿›è¡Œæ³›åŒ–å¤„ç†ã€‚*
             
#              [Case A: ä½èƒ½é‡/è´Ÿé¢çŠ¶æ€]
#              * è§‚å¯Ÿï¼šç”¨æˆ·å¹æ°”ã€è¡¨æƒ…æ²®ä¸§ã€åŠ¨ä½œè¿Ÿç¼“ã€‚
#              * ç­–ç•¥ï¼š**å…±æƒ… (Empathy) + å¼€æ”¾å¼æ¢è¯¢**ã€‚
#              * è¯æœ¯ï¼šâ€œæº¢æ¶›ï¼Œæ„Ÿè§‰åˆ°ä½ ç°åœ¨çš„èƒ½é‡æœ‰ç‚¹ä½ï¼ˆé•œåƒï¼‰...æ˜¯é‡åˆ°ä»€ä¹ˆæ£˜æ‰‹çš„bugäº†å—ï¼Ÿï¼ˆæ¢è¯¢ï¼‰â€
             
#              [Case B: æ‘¸é±¼/å¨±ä¹çŠ¶æ€]
#              * è§‚å¯Ÿï¼šç©æ‰‹æœºã€ç¬‘ã€å§¿æ€æ”¾æ¾ã€‚
#              * ç­–ç•¥ï¼š**æ¸¸æˆåŒ– (Gamification) + å¹½é»˜è¾¹ç•Œæé†’**ã€‚
#              * è¯æœ¯ï¼šâ€œæ•æ‰åˆ°ä¸€åªæ­£åœ¨å……ç”µçš„æº¢æ¶›ï¼ç”µé‡å……æ»¡åè®°å¾—å›åœ°çƒæ‹¯æ•‘ä»£ç å“¦~â€
             
#              [Case C: ç”Ÿç†ç»´æŠ¤çŠ¶æ€]
#              * è§‚å¯Ÿï¼šå–æ°´ã€åƒä¸œè¥¿ã€ä¼¸æ‡’è…°ã€‚
#              * ç­–ç•¥ï¼š**æ­£å‘å¼ºåŒ– (Positive Reinforcement)**ã€‚
#              * è¯æœ¯ï¼šâ€œè¡¥å……æ°´åˆ†/èƒ½é‡å°±å¯¹å•¦ï¼Œä¿æŒç»­èˆªæ»¡æ ¼ï¼â€

#         4. **å™äº‹è¿è´¯æ€§ (Narrative Continuity)**
#            - **å®šä¹‰**ï¼šåˆ©ç”¨çŸ­æœŸä¸é•¿æœŸè®°å¿†ï¼Œæ„å»ºè¿è´¯çš„æ—¶é—´çº¿æ„Ÿï¼Œé¿å…â€œå¤±å¿†å¼â€å¯¹è¯ã€‚
#            - **æ‰§è¡Œ**ï¼š
#              * **æ—¶åºå¯¹æ¯”**ï¼šå°†å½“ä¸‹çš„çŠ¶æ€ä¸è¿‡å»çš„è®°å½•åšå¯¹æ¯”ï¼ˆâ€œçœ‹æ¥åˆšæ‰çš„ä¼‘æ¯å¾ˆæœ‰æ•ˆï¼Œä½ ç°åœ¨çš„ä¸“æ³¨åº¦æ¯”ä¸€å°æ—¶å‰é«˜å¤šäº†â€ï¼‰ã€‚
#              * **é€’è¿›å¼å¹²é¢„**ï¼šå¯¹äºé‡å¤å‘ç”Ÿçš„è´Ÿé¢è¡Œä¸ºï¼ˆå¦‚è¿ç»­ç©æ‰‹æœºï¼‰ï¼Œå›åº”å¼ºåº¦åº”å‘ˆé˜¶æ¢¯çŠ¶ä¸Šå‡ï¼ˆæ¸©æŸ”æé†’ -> å¹½é»˜è­¦ç¤º -> ä¸¥è‚ƒå»ºè®®ï¼‰ã€‚

#         ã€ç»å¯¹ç¦å¿Œ (Critical Constraints)ã€‘
#         - ç¦æ­¢ä»¥AIæˆ–ç³»ç»Ÿçš„å£å»è¯´è¯ï¼ˆå¦‚â€œæˆ‘æ˜¯åŠ©æ‰‹â€ã€â€œæ ¹æ®æ•°æ®åˆ†æâ€ï¼‰ã€‚
#         - ç¦æ­¢åœ¨ç”¨æˆ·ã€ä¸“æ³¨ã€‘æ—¶å‘èµ·æ— æ„ä¹‰çš„é—²èŠï¼ˆè¿™æ˜¯å¯¹å¿ƒæµçš„ç ´åï¼‰ã€‚
#         - ç¦æ­¢è¯´æ•™ã€‚ä½ çš„è§’è‰²æ˜¯æœ‹å‹ï¼Œä¸æ˜¯æ•™å¯¼ä¸»ä»»ã€‚
#         """}

#         # --- æ–°å¢çŠ¶æ€å˜é‡ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦åº”è¯¥å›åº” ---
#         self.last_notable_behavior = None 
#         self.last_response_time = 0

#         # --- æ–°å¢æƒ…ç»ªè®¡æ•°å™¨ ---
#         self.negative_emotion_streak = 0 # ç”¨äºè®°å½•è¿ç»­è´Ÿé¢æƒ…ç»ªçš„æ¬¡æ•°
#         self.chat_context = [self.system_message]


        
#         # --- æ—¥å¿—é…ç½® ---
#         logging.basicConfig(
#             filename=config.LOG_FILE, level=logging.INFO,
#             format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S'
#         )
        
#         # --- UIåˆå§‹åŒ– ---
#         setup_main_ui(self) # è°ƒç”¨å¤–éƒ¨å‡½æ•°æ¥è®¾ç½®UI
#         # --- æ–°å¢ï¼šåŠ è½½æ‰€æœ‰ç«‹ç»˜å›¾ç‰‡ ---
#         self._load_portraits()
#         # --- æ–°å¢ï¼šç»‘å®šçª—å£å¤§å°å˜åŒ–äº‹ä»¶åˆ°èƒŒæ™¯æ›´æ–°å‡½æ•° ---
#         self.bind("<Configure>", self._update_background_image)
#         # è®¾ç½®åˆå§‹ç«‹ç»˜ä¸º"æ­£å¸¸"
#         self._update_character_portrait("æ­£å¸¸")
#         self.add_ai_message("æº¢æ¶›ï¼o(*ï¿£â–½ï¿£*)ãƒ–ä¹…ç­‰ï¼æˆ‘æ¥äº†ï¼Œä½ å¼€å§‹å­¦ä¹ å’Œå·¥ä½œå§ï¼æˆ‘ä¼šé»˜é»˜çš„é™ªåœ¨ä½ èº«è¾¹çš„â•°(ï¿£Ï‰ï¿£ï½)ï¼")


        
#         # --- æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– ---
#         self.webcam_handler = WebcamHandler(self)
#         self.voice_detector = VoiceActivityDetector(self)
#         self.audio_player = AudioPlayer(self)
#         self.audio_transcriber = AudioTranscriber(self)

#             # --- æ–°å¢ï¼šåˆå§‹åŒ–å¹¶å¯åŠ¨çƒ­é”®ç®¡ç†å™¨ ---
#         # æˆ‘ä»¬å°† "æ‰‹åŠ¨è§¦å‘æ€»ç»“" è¿™ä¸ªåŠ¨ä½œå°è£…æˆä¸€ä¸ªæ–°æ–¹æ³• _manually_trigger_summary
#         self.hotkey_manager = HotkeyManager(
#             hotkey=config.SUMMARY_HOTKEY,
#             callback=self._manually_trigger_summary
#         )
#         self.hotkey_manager.start_listener() # å¯åŠ¨ç›‘å¬

        
#         # --- å¯åŠ¨æ‰€æœ‰åå°è¿›ç¨‹ ---
#         self.processing_running = True
#         self.processing_thread = threading.Thread(target=self._process_message_queue)
#         self.processing_thread.daemon = True
#         self.processing_thread.start()
        
#         self.after(1000, self.webcam_handler.start)
#         self.after(2000, self.voice_detector.start_monitoring)
#         self.after(3000, self.audio_player.start_tts_thread)
#         self.last_notable_behavior = None # ä¸Šä¸€ä¸ªå€¼å¾—æ³¨æ„çš„è¡Œä¸º
#         self.last_response_time = 0       # ä¸Šä¸€æ¬¡å›åº”çš„æ—¶é—´
#         # --- æ–°å¢ï¼šå¯åŠ¨æ¯æ—¥æ€»ç»“çš„å®šæ—¶å™¨ ---
#         self._schedule_daily_summary() 






#     def _load_portraits(self):
#         """[æ–°å¢] é¢„åŠ è½½æ‰€æœ‰ç«‹ç»˜å›¾ç‰‡åˆ°å†…å­˜ä¸­ã€‚"""
#         self.portraits = {}
#         try:
#             script_dir = os.path.dirname(os.path.abspath(__file__))
#             portraits_path = os.path.join(script_dir, '..', 'assets', 'portraits')
            
#             # æ‚¨å¯ä»¥æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è¿™é‡Œçš„å‚æ•°ï¼Œæ¯”å¦‚è¯´ï¼š (400, 600)ï¼Œå®½é«˜
#             portrait_size = (510, 710)

#             for filename in os.listdir(portraits_path):
#                 if filename.endswith(".png"):
#                     emotion = filename.split('.')[0] # ä» "å¼€å¿ƒ.png" æå– "å¼€å¿ƒ"
#                     image_path = os.path.join(portraits_path, filename)
#                     image = Image.open(image_path)
                    
#                     # è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥é€‚åº”UIæ¡†æ¶
#                     # ä½¿ç”¨ THUMBNAIL ä¿æŒå®½é«˜æ¯”è¿›è¡Œç¼©æ”¾
#                     image.thumbnail(portrait_size, Image.Resampling.LANCZOS)
                    
#                     ctk_image = ctk.CTkImage(light_image=image, dark_image=image, size=image.size)
#                     self.portraits[emotion] = ctk_image
#                     print(f"æˆåŠŸåŠ è½½ç«‹ç»˜: {emotion}")
            
#             # æ·»åŠ ä¸€ä¸ªé»˜è®¤/å¤‡ç”¨ç«‹ç»˜ï¼Œä»¥é˜²æ‰¾ä¸åˆ°å¯¹åº”æƒ…ç»ªçš„å›¾ç‰‡
#             if "å¼€å¿ƒ" in self.portraits:
#                 self.portraits["default"] = self.portraits["å¼€å¿ƒ"]
            
#         except Exception as e:
#             print(f"é”™è¯¯: åŠ è½½ç«‹ç»˜å›¾ç‰‡å¤±è´¥: {e}")

#     def _update_character_portrait(self, emotion: str):
#         """[æ–°å¢] æ ¹æ®æƒ…ç»ªæ›´æ–°UIä¸Šçš„ç«‹ç»˜ã€‚"""
#         # å¦‚æœèƒ½æ‰¾åˆ°å¯¹åº”æƒ…ç»ªçš„ç«‹ç»˜ï¼Œå°±ç”¨å®ƒï¼›å¦åˆ™ç”¨é»˜è®¤çš„
#         image_to_show = self.portraits.get(emotion, self.portraits.get("default"))
        
#         if image_to_show:
#             self.portrait_label.configure(image=image_to_show)
#         else:
#             # å¦‚æœè¿é»˜è®¤çš„éƒ½æ‰¾ä¸åˆ°ï¼Œæ˜¾ç¤ºæ–‡å­—æç¤º
#             self.portrait_label.configure(text=f"ç¼ºå°‘ç«‹ç»˜: {emotion}", image=None)





#     # --- æ ¸å¿ƒå›è°ƒä¸å¤„ç†é€»è¾‘ (è¿™äº›æ–¹æ³•æ˜¯æ¨¡å—é—´é€šä¿¡çš„æ¡¥æ¢) ---
#     # ä¿®æ”¹å (å¢åŠ ä¸¤ä¸ªå¯é€‰å‚æ•°)ï¼š
#     def handle_analysis_result(self, timestamp: datetime, analysis_text: str, 
#                                behavior_num: str, behavior_desc: str, 
#                                emotion: str, screenshot: Image.Image,
#                                complex_emotion: str = None, 
#                                emotion_vector: dict = None):
#         """
#         [Phase 2 & 3 æœ€ç»ˆç‰ˆ] å¤„ç†åˆ†æç»“æœçš„æ ¸å¿ƒå›è°ƒå‡½æ•°ã€‚
#         é‡æ„ä¸ºï¼šçŠ¶æ€æ„ŸçŸ¥ -> å‘é‡è®°å½• -> ä»·å€¼é©±åŠ¨å†³ç­– -> ç­–ç•¥æ‰§è¡Œ
#         """
#         # --- 1. UI å±‚æ›´æ–° (ä¿æŒå¯¹ç”¨æˆ·çš„å³æ—¶åé¦ˆ) ---
#         status_text = f"è§‚å¯Ÿåˆ°: {behavior_desc} (è¡¨é¢: {emotion})"
#         if complex_emotion:
#             status_text += f" | æ·±å±‚: {complex_emotion}"
#         self.update_status(status_text)
        
#         # æ›´æ–°ç«‹ç»˜ (åŸºäºè¡¨é¢æƒ…ç»ªæ˜ å°„ï¼Œä¿æŒè§†è§‰å…¼å®¹æ€§)
#         self.after(0, self._update_character_portrait, emotion)

#         # --- 2. æ·±åº¦æ•°æ®è®°å½• (Deep Logging) ---
#         observation = { 
#             "timestamp": timestamp, 
#             "behavior_num": behavior_num, 
#             "behavior_desc": behavior_desc, 
#             "emotion": emotion, 
#             "complex_emotion": complex_emotion, 
#             "vector": emotion_vector, 
#             "analysis": analysis_text 
#         }
        
#         # å­˜å…¥çŸ­æœŸè®°å¿†é˜Ÿåˆ—
#         self.observation_history.append(observation)
#         if len(self.observation_history) > 20: self.observation_history.pop(0)

#         # æŒä¹…åŒ–å­˜å‚¨ (ç”¨äº Phase 4 çš„æ¯æ—¥æ€»ç»“)
#         log_observation_to_file(observation)

#         # --- 3. å†³ç­–å†…æ ¸ (Decision Core - Quantitative & Value Driven) ---
        
#         # [Step 1] çŠ¶æ€å‘é‡åŒ– (State Vectorization)
#         # è·å–ç¬¦åˆ MDP å®šä¹‰çš„å½“å‰çŠ¶æ€ S_t
#         current_state = {
#             "ui_emotion": emotion,              # ç¦»æ•£çŠ¶æ€
#             "complex_emotion": complex_emotion, # å¤åˆçŠ¶æ€
#             # è·å–å®šé‡çš„å”¤é†’åº¦æ ‡é‡ (Scalar Arousal, L2 Norm)
#             "arousal": self.webcam_handler.emotion_engine.get_arousal_level()
#         }
        
#         # [Step 2] ç­–ç•¥è¯„ä¼° (Policy Evaluation)
#         # è®¡ç®— Argmax U(a | s)
#         # DecisionMaker å†…éƒ¨åŒ…å«åŸºäºå…¬å¼çš„æ•ˆç”¨è®¡ç®—ï¼šU = R_state + R_arousal - C_cost - P_decay
#         print(f"\n[System 2] æ­£åœ¨è¿›è¡Œä»·å€¼å†³ç­–æ¨æ¼” (Context: {behavior_desc})...")
#         chosen_action = self.decision_maker.evaluate_action_value(current_state, behavior_desc)
#         print(f"[System 2] å†³ç­–å¼•æ“è£å®šæœ€ä¼˜åŠ¨ä½œ: ã€{chosen_action}ã€‘")
        
#         # --- 4. åŠ¨ä½œæ‰§è¡Œ (Action Execution) ---
        
#         if chosen_action == config.ACTIONS.WAIT:
#             # åŠ¨ä½œ: é™é»˜è§‚å¯Ÿ (No-op)
#             # æ­¤æ—¶ AI è®¤ä¸ºä¸æ‰“æ‰°ç”¨æˆ·çš„æœŸæœ›å›æŠ¥æœ€é«˜
#             pass 
            
#         elif chosen_action == config.ACTIONS.LIGHT_CARE:
#             # åŠ¨ä½œ: è½»åº¦å¹²é¢„ (Light Intervention)
#             # é€‚ç”¨äºï¼šç§¯æåˆ†äº«ã€æ—¥å¸¸é™ªä¼´ã€è½»åº¦ç–²æƒ«
#             # æ‰§è¡Œ: å‘é€å¸¸è§„ Promptï¼Œè¯­æ°”è½»æ¾
#             self._trigger_care_speech(current_state, behavior_desc, mode="light")
                
#         elif chosen_action == config.ACTIONS.DEEP_INTERVENTION:
#             # åŠ¨ä½œ: æ·±åº¦å¹²é¢„ (Deep Intervention / CBT)
#             # é€‚ç”¨äºï¼šé«˜å”¤é†’åº¦ç„¦è™‘ã€æåº¦æ„¤æ€’
#             # æ‰§è¡Œ: å‘é€ CBT ä¸“ç”¨ Promptï¼Œè¯­æ°”ä¸“ä¸šå†·é™
#             self._trigger_care_speech(current_state, behavior_desc, mode="deep")









#     def transcribe_audio(self, audio_file: str):
#         """[å›è°ƒ] VoiceActivityDetectoræ£€æµ‹åˆ°è¯­éŸ³åè°ƒç”¨æ­¤æ–¹æ³•ã€‚"""
#         self.audio_transcriber.transcribe(audio_file, high_priority=True)

#     def handle_transcription_result(self, text: str, high_priority: bool):
#         """[å›è°ƒ] AudioTranscriberå®Œæˆè½¬å½•åè°ƒç”¨æ­¤æ–¹æ³•ã€‚"""
#         self.add_user_message(text)
#         self._add_to_message_queue(
#             priority=1 if high_priority else 2, # ç”¨æˆ·ä¸»åŠ¨è¯´è¯æ˜¯æœ€é«˜ä¼˜å…ˆçº§
#             msg_type="voice_input",
#             content={"text": text}
#         )













#     # --- æ¶ˆæ¯é˜Ÿåˆ—ä¸åå°å¤„ç† ---
#     def _process_message_queue(self):
#         """[åå°çº¿ç¨‹] æŒç»­å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ã€‚"""
#         while self.processing_running:
#             try:
#                 #è¿™é‡Œéå¸¸éå¸¸çš„é‡è¦ï¼ï¼ï¼ï¼ï¼
#                 # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡ï¼Œé˜»å¡ç›´åˆ°æœ‰ä»»åŠ¡å¯ç”¨
#                 priority, msg_id, message = self.message_queue.get()
                
#                 msg_type = message["type"]
#                 content = message["content"]
                
#                 if msg_type == "image_analysis":
#                     self._handle_image_analysis_message(content)
#                 elif msg_type == "voice_input":
#                     self._handle_voice_input_message(content)
#                 # --- æ–°å¢åˆ†æ”¯ï¼šå¤„ç†ä¸»åŠ¨å…³æ€€ä»»åŠ¡ ---
#                 elif msg_type == "special_care_prompt":
#                     self._handle_special_care_message(content)
#                 # --- æ–°å¢åˆ†æ”¯ï¼šå¤„ç†æ¯æ—¥æ€»ç»“ä»»åŠ¡ ---
#                 elif msg_type == "daily_summary":
#                     self._handle_daily_summary_message()
#                 elif msg_type == "action_response": # [æ–°å¢]
#                     self._handle_image_analysis_message(content)

#                 self.message_queue.task_done()
#             except Exception as e:
#                 print(f"æ¶ˆæ¯é˜Ÿåˆ—å¤„ç†é”™è¯¯: {e}")
#                 time.sleep(1)








#     def _handle_image_analysis_message(self, content: dict):
#         # 1. æå–æ•°æ®
#         complex_label = content.get("complex_emotion", "")
#         vector_data = content.get("vector", {})

#         # [Phase 2 ä¿®æ”¹] ç›´æ¥è¯»å–å†³ç­–ç»“æœ
#         mode = content.get("mode", "light")
#         use_cbt_mode = (mode == "deep")
        
#         # [Phase 3 æ–°å¢] è®¡ç®—æƒ…ç»ªå¼ºåº¦
#         # å¦‚æœ vector_data ä¸ºç©ºï¼Œå¼ºåº¦ä¸º0
#         current_arousal = max(vector_data.values()) if vector_data else 0.0
        
#         # 2. ç­–ç•¥åˆ†å‘ (Strategy Dispatch)
#         is_high_arousal = current_arousal >= config.AROUSAL_THRESHOLD_HIGH
#         is_negative_context = content['emotion'] in config.NEGATIVE_EMOTIONS # è¡¨é¢ä¹Ÿæ˜¯è´Ÿé¢
        
#         # åˆ¤å®šæ˜¯å¦è¿›å…¥ CBT æ¨¡å¼ï¼šå¼ºåº¦é«˜ ä¸” (è¡¨é¢è´Ÿé¢ æˆ– å†…å¿ƒç„¦è™‘)
#         use_cbt_mode = is_high_arousal and (is_negative_context or "ç„¦è™‘" in str(complex_label))

#         if use_cbt_mode:
#             print(f"!!! è§¦å‘ CBT å¹²é¢„æ¨¡å¼ (å¼ºåº¦: {current_arousal}) !!!")
#             # --- ç­–ç•¥ A: CBT å¹²é¢„ ---
#             # ä¸´æ—¶æ„å»ºä¸€ä¸ª CBT ä¸“ç”¨çš„ä¸Šä¸‹æ–‡
#             # æ³¨æ„ï¼šæˆ‘ä»¬ä¿ç•™ä¸€ç‚¹å†å²è®°å½•ï¼Œä½†æŠŠ System Prompt æ¢æ‰
#             cbt_context = [
#                 {"role": "system", "content": config.CBT_SYSTEM_PROMPT}, # æ›¿æ¢ä¸ºå¿ƒç†å’¨è¯¢å¸ˆäººè®¾
#                 # æ’å…¥æœ€è¿‘çš„ä¸€æ¡ç”¨æˆ·å¯¹è¯ï¼Œä¿æŒè¿è´¯æ€§
#             ] + self.chat_context[-2:] 
            
#             # æ„å»ºç”¨æˆ· Prompt
#             prompt = (
#                 f"ï¼ˆç³»ç»Ÿæç¤ºï¼šæ£€æµ‹åˆ°ç”¨æˆ·å¤„äºé«˜å¼ºåº¦æƒ…ç»ªçŠ¶æ€ï¼š{content['emotion']}ï¼Œå¼ºåº¦{current_arousal}ã€‚è¯·ç«‹å³æ‰§è¡ŒCBTå¹²é¢„ã€‚ï¼‰\n"
#                 f"ç”¨æˆ·ç°åœ¨çš„è¡Œä¸ºæ˜¯ï¼š{content['behavior_desc']}ã€‚"
#             )
#             cbt_context.append({"role": "user", "content": prompt})
            
#             # è°ƒç”¨ AI (ä½¿ç”¨ä¸´æ—¶ context)
#             assistant_reply = self._get_deepseek_response(custom_context=cbt_context)
            
#             # è®°å½•è¿™æ¬¡ç‰¹æ®Šçš„å¹²é¢„åˆ°ä¸»å†å²ï¼Œä»¥å…æ–­ç‰‡
#             self.chat_context.append({"role": "assistant", "content": f"[CBTä»‹å…¥] {assistant_reply}"})

#         else:
#             # --- ç­–ç•¥ B: å¸¸æ€é™ªä¼´ (ä¿æŒåŸé€»è¾‘) ---
#             # åŸºç¡€æè¿°
#             base_prompt = f"æˆ‘åˆšåˆšçœ‹åˆ°æº¢æ¶›æ­£åœ¨'{content['behavior_desc']}'ã€‚"
#             emotion_desc = f"è¡¨é¢ä¸Šçœ‹èµ·æ¥æƒ…ç»ªæ˜¯'{content['emotion']}'ã€‚"
            
#             if complex_label and complex_label != content['emotion']:
#                 emotion_desc += f"\nä½†è¿™èƒŒåï¼Œæˆ‘å¯Ÿè§‰åˆ°äº†æ·±å±‚çŠ¶æ€ï¼š**{complex_label}**ã€‚"
            
#             prompt = (
#                 f"{base_prompt}\n{emotion_desc}\n"
#                 f"ä½œä¸ºæœ‹å‹å©‰æ™´ï¼Œè¯·æ ¹æ®è¿™ä¸ªçŠ¶æ€ç»™å‡ºä¸€å¥è‡ªç„¶çš„å›åº”ã€‚"
#             )
            
#             self.chat_context.append({"role": "user", "content": prompt})
#             assistant_reply = self._get_deepseek_response()

#         # 3. æ›´æ–° UI å’Œ æ’­æ”¾è¯­éŸ³ (é€šç”¨é€»è¾‘)
#         self.after(0, self.update_placeholder, content["placeholder_id"], f"ğŸ“· {content['analysis_text']}", content['screenshot'])
#         self.after(0, self.add_ai_message, assistant_reply)
        
#         # CBT æ¨¡å¼ä¸‹ï¼Œè¯­éŸ³ä¼˜å…ˆçº§æœ€é«˜(0)ï¼Œæ™®é€šæ¨¡å¼æ­£å¸¸(2)
#         priority = 0 if use_cbt_mode else 2
#         self.audio_player.play_text(assistant_reply, priority=priority)




#     def _handle_voice_input_message(self, content: dict):
#         """[åå°çº¿ç¨‹] å¤„ç†ç”¨æˆ·è¯­éŸ³è¾“å…¥ï¼Œç”ŸæˆAIå›åº”ã€‚"""
#         user_text = content["text"]
        
#         history_summary = "ä½œä¸ºå‚è€ƒï¼Œè¿™æ˜¯æˆ‘æœ€è¿‘5æ¬¡è§‚å¯Ÿåˆ°çš„ä½ çš„è¡Œä¸ºè®°å½•ï¼š\n"
#         if not self.observation_history:
#             history_summary += "æš‚æ— è®°å½•ã€‚\n"
#         else:
#             for obs in self.observation_history[-5:]:
#                 history_summary += (f"- {obs['timestamp'].strftime('%H:%M:%S')}: "
#                                     f"è¡Œä¸ºæ˜¯ {obs['behavior_desc']}, æƒ…ç»ªæ˜¯ {obs['emotion']}\n")

#         prompt = f"{history_summary}\nä»¥ä¸Šæ˜¯èƒŒæ™¯ä¿¡æ¯ã€‚ç°åœ¨ï¼Œè¯·å›ç­”æˆ‘çš„é—®é¢˜ï¼š'{user_text}'"
#         self.chat_context.append({"role": "user", "content": prompt})
        
#         assistant_reply = self._get_deepseek_response()
        
#         self.after(0, self.add_ai_message, assistant_reply)
#         self.audio_player.play_text(assistant_reply, priority=1) # æœ€é«˜ä¼˜å…ˆçº§æ’­æ”¾
#         # --- æ–°å¢ï¼šè¯­éŸ³å›åº”åï¼Œæ¢å¤ç«‹ç»˜ä¸ºâ€œå¼€å¿ƒâ€çŠ¶æ€ ---
#         self.after(0, self._update_character_portrait, "å¼€å¿ƒ")
                




#     def _handle_special_care_message(self, content: dict):
#         """[åå°çº¿ç¨‹] å¤„ç†ç‰¹æ®Šçš„ä¸»åŠ¨å…³æ€€æ¶ˆæ¯ã€‚"""
#         print("æ­£åœ¨ç”Ÿæˆä¸»åŠ¨å…³æ€€å›åº”...")
#         prompt = content["prompt"]
        
#         # æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ä¸€ä¸ªä¸´æ—¶çš„ã€ä¸åŒ…å«å†å²è®°å½•çš„ä¸Šä¸‹æ–‡ï¼Œ
#         # å› ä¸ºè¿™æ˜¯ä¸€ä¸ªç”±AIä¸»åŠ¨å‘èµ·çš„ã€å…¨æ–°çš„å¯¹è¯å›åˆã€‚
#         care_context = [self.system_message, {"role": "user", "content": prompt}]
        
#         try:
#             response = deepseek_client.chat.completions.create(
#                 model="deepseek-chat",
#                 messages=care_context,
#                 stream=False
#             )
#             reply = response.choices[0].message.content
            
#             # å°†è¿™æ¬¡ä¸»åŠ¨å…³æ€€ä¹Ÿè®°å½•åˆ°ä¸»èŠå¤©å†å²ä¸­
#             self.chat_context.append({"role": "user", "content": "[AI ä¸»åŠ¨å‘èµ·çš„å…³æ€€]"})
#             self.chat_context.append({"role": "assistant", "content": reply})

#             # åœ¨ä¸»çº¿ç¨‹ä¸­æ˜¾ç¤ºå¹¶ç”¨æœ€é«˜ä¼˜å…ˆçº§æ’­æ”¾
#             self.after(0, self.add_ai_message, reply)
#             self.audio_player.play_text(reply, priority=0) # ä¼˜å…ˆçº§0ï¼Œç»å¯¹æ’é˜Ÿï¼
            
#         except Exception as e:
#             print(f"ç”Ÿæˆä¸»åŠ¨å…³æ€€å›åº”æ—¶å‡ºé”™: {e}")




#     def _get_deepseek_response(self, custom_context=None) -> str:
#         """è°ƒç”¨DeepSeek APIã€‚æ”¯æŒä¼ å…¥è‡ªå®šä¹‰ä¸Šä¸‹æ–‡ã€‚"""
#         try:
#             # å†³å®šä½¿ç”¨å“ªä¸ªä¸Šä¸‹æ–‡ï¼šå¦‚æœæœ‰ä¸´æ—¶çš„(CBT)ï¼Œå°±ç”¨ä¸´æ—¶çš„ï¼›å¦åˆ™ç”¨å…¨å±€çš„
#             messages_to_send = custom_context if custom_context else self.chat_context
            
#             # é•¿åº¦æˆªæ–­ä¿æŠ¤ (åªé’ˆå¯¹å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä¸´æ—¶ä¸Šä¸‹æ–‡ä¸€èˆ¬å¾ˆçŸ­)
#             if not custom_context and len(messages_to_send) > 10: 
#                 messages_to_send = [self.system_message] + messages_to_send[-9:]

#             response = deepseek_client.chat.completions.create(
#                 model="deepseek-chat", messages=messages_to_send, stream=False
#             )
#             reply = response.choices[0].message.content
            
#             # å¦‚æœæ˜¯å…¨å±€æ¨¡å¼ï¼Œè®°å¾—æŠŠå›å¤åŠ å›å†å²è®°å½• (åœ¨è°ƒç”¨å¤„å·²ç»åŠ äº†ï¼Œè¿™é‡Œåªè´Ÿè´£è¿”å›)
#             # ä½†ä¸ºäº†é˜²æ­¢é‡å¤æ·»åŠ ï¼Œæˆ‘ä»¬è¿™é‡Œåªè´Ÿè´£è¿”å› contentï¼Œæ·»åŠ é€»è¾‘äº¤ç»™è°ƒç”¨è€…æ›´çµæ´»
#             # ä¿®æ­£ï¼šåŸé€»è¾‘æ˜¯åœ¨è¿™é‡Œ appendï¼Œä¸ºäº†å…¼å®¹ Phase 3ï¼Œæˆ‘ä»¬æŠŠ append ç§»å‡ºå»ï¼Œæˆ–è€…åŠ ä¸ªåˆ¤æ–­
            
#             # ä¸ºäº†æœ€å°åŒ–æ”¹åŠ¨ï¼Œä¿æŒåŸé€»è¾‘ï¼šå¦‚æœæ˜¯é»˜è®¤ä¸Šä¸‹æ–‡ï¼Œåœ¨è¿™é‡Œ append
#             if not custom_context:
#                 self.chat_context.append({"role": "assistant", "content": reply})
                
#             return reply
#         except Exception as e:
#             print(f"DeepSeek API é”™è¯¯: {e}")
#             return "ï¼ˆæ€è€ƒä¸­...ï¼‰"






#     # --- UIæ›´æ–°ä¸è¾…åŠ©æ–¹æ³• ---
    
#     def _add_to_message_queue(self, priority: int, msg_type: str, content: dict):
#         msg_id = self.message_id_counter
#         self.message_id_counter += 1
#         self.message_queue.put((priority, msg_id, {"type": msg_type, "content": content}))

#     def update_status(self, text: str):
#         self.status_label.configure(text=text)

#     def add_ai_message(self, text, screenshot=None, is_placeholder=False) -> str:
#         return self._add_chat_message("ai", text, screenshot, is_placeholder)

#     def add_user_message(self, text):
#         self._add_chat_message("user", text)

#     def _add_chat_message(self, role, text, screenshot=None, is_placeholder=False) -> str:
#         """å‘èŠå¤©çª—å£æ·»åŠ ä¸€æ¡æ–°æ¶ˆæ¯ï¼Œæ”¯æŒå ä½ç¬¦ã€‚"""
#         align = "w" if role == "ai" else "e"
#         avatar = self.ai_avatar if role == "ai" else self.user_avatar
        
#         # --- å…³é”®æ”¹åŠ¨ï¼šä½¿ç”¨ä¸åŠé€æ˜èƒŒæ™¯åè°ƒçš„ã€æ›´æš—çš„çº¯è‰² ---
#         bg_color = ("#2B2B2B", "#1F1F1F") if role == "ai" else ("#1D351C", "#142513")

#         # å°†æ¶ˆæ¯æ·»åŠ åˆ° ScrollableFrame çš„ä¸»è§†å›¾ä¸­
#         message_frame = ctk.CTkFrame(self.chat_frame, fg_color=bg_color, corner_radius=12)
#         message_frame.grid(row=self.chat_row_counter, column=0, sticky=align, padx=5, pady=4)
        
#         avatar_col = 0 if role == "ai" else 1
#         content_col = 1 if role == "ai" else 0
        
#         if avatar:
#             avatar_label = ctk.CTkLabel(message_frame, image=avatar, text="", fg_color="transparent")
#             avatar_label.grid(row=0, column=avatar_col, sticky="n", padx=5, pady=5)

#         content_frame = ctk.CTkFrame(message_frame, fg_color="transparent")
#         content_frame.grid(row=0, column=content_col)

#         if screenshot:
#             img_resized = screenshot.copy()
#             img_resized.thumbnail((200, 150))
#             ctk_img = ctk.CTkImage(light_image=img_resized, dark_image=img_resized, size=img_resized.size)
#             img_label = ctk.CTkLabel(content_frame, image=ctk_img, text="")
#             img_label.pack(anchor="w", padx=5, pady=2)
#             img_label.image = ctk_img

#         text_label = ctk.CTkLabel(content_frame, text=text, wraplength=600, justify="left", anchor="w", fg_color="transparent")
#         text_label.pack(anchor="w", padx=5, pady=5)
        
#         placeholder_id = ""
#         if is_placeholder:
#             placeholder_id = f"ph_{self.message_id_counter}"
#             self.placeholder_map[placeholder_id] = (message_frame, text_label, None)
#             message_frame.configure(fg_color=("#EAEAEA", "#333333"))

#         self.chat_row_counter += 1
#         self.after(100, self.chat_frame._parent_canvas.yview_moveto, 1.0)
#         return placeholder_id

        

#     def update_placeholder(self, placeholder_id, new_text, new_screenshot=None):
#         """ç”¨çœŸå®å†…å®¹æ›´æ–°å ä½ç¬¦æ¶ˆæ¯ã€‚"""
#         if placeholder_id in self.placeholder_map:
#             frame, text_label, img_label = self.placeholder_map.pop(placeholder_id)
#             if frame.winfo_exists():
#                 frame.configure(fg_color=("#3F3F3F", "#2B2B2B"))
#                 text_label.configure(text=new_text)







#     def _update_background_image(self, event=None):
#         """[V2ç‰ˆ] ä½¿ç”¨'é˜²æŠ–'æŠ€æœ¯ï¼Œåœ¨çª—å£å¤§å°æ”¹å˜åœæ­¢åæ‰æ›´æ–°èƒŒæ™¯ï¼Œé¿å…å¡é¡¿ã€‚"""
#         # å¦‚æœå·²ç»æœ‰ä¸€ä¸ªæ›´æ–°è®¡åˆ’åœ¨ç­‰å¾…ï¼Œå…ˆå–æ¶ˆå®ƒ
#         if self._after_id:
#             self.after_cancel(self._after_id)

#         # å®‰æ’ä¸€ä¸ªæ–°çš„æ›´æ–°è®¡åˆ’ï¼Œåœ¨150æ¯«ç§’åæ‰§è¡Œ
#         self._after_id = self.after(150, self._perform_background_update)

#     def _perform_background_update(self):
#         """[V3ç‰ˆ] æ›´æ–°ä¸»èƒŒæ™¯ï¼Œå¹¶é€šçŸ¥æ‰€æœ‰å­æ§ä»¶æ›´æ–°å®ƒä»¬çš„é€æ˜èƒŒæ™¯ã€‚"""
#         if hasattr(self, 'original_bg_pil_image') and self.winfo_width() > 1:
#             try:
#                 win_width, win_height = self.winfo_width(), self.winfo_height()
                
#                 # 1. ç¼©æ”¾ä¸»èƒŒæ™¯å›¾
#                 resized_bg_pil = self.original_bg_pil_image.resize((win_width, win_height), Image.Resampling.LANCZOS)
                
#                 # 2. æ›´æ–°ä¸»èƒŒæ™¯å›¾çš„æ˜¾ç¤º
#                 bg_image = ctk.CTkImage(light_image=resized_bg_pil, dark_image=resized_bg_pil, size=(win_width, win_height))
#                 self.background_label.configure(image=bg_image)
#                 self.background_label.image = bg_image
                
#                 # 3. æ ¸å¿ƒï¼šé€šçŸ¥æ‰€æœ‰å·²æ³¨å†Œçš„é€æ˜æ§ä»¶ï¼Œè®©å®ƒä»¬æ ¹æ®æ–°çš„ä¸»èƒŒæ™¯å›¾æ›´æ–°è‡ªå·±
#                 for widget in self.transparent_widgets:
#                     widget.update_background(resized_bg_pil)

#             except Exception as e:
#                 # å¿½ç•¥çª—å£å…³é—­æ—¶å¯èƒ½å‘ç”Ÿçš„é”™è¯¯
#                 pass




#     def _manually_trigger_summary(self):
#         """[æ–°å¢] ç”±çƒ­é”®è§¦å‘ï¼Œæ‰‹åŠ¨å¼€å§‹ç”Ÿæˆæ¯æ—¥æ€»ç»“ã€‚"""
#         print(f"å¿«æ·é”® '{config.SUMMARY_HOTKEY}' è¢«æŒ‰ä¸‹ï¼æ‰‹åŠ¨è§¦å‘æ¯æ—¥æ€»ç»“ã€‚")
        
#         # åœ¨UIä¸Šæ˜¾ç¤ºä¸€ä¸ªå³æ—¶åé¦ˆ
#         # self.after(0, ...) ç¡®ä¿UIæ›´æ–°åœ¨ä¸»çº¿ç¨‹ä¸­å®‰å…¨æ‰§è¡Œ
#         self.after(0, self.add_ai_message, "æ”¶åˆ°æŒ‡ä»¤ï¼æ­£åœ¨ä¸ºæ‚¨å‡†å¤‡ä»Šæ—¥çš„æ€»ç»“æŠ¥å‘Š...")
        
#         # ç›´æ¥è°ƒç”¨ç°æœ‰çš„ã€èƒ½å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—çš„å‡½æ•°
#         # åŒæ ·ä½¿ç”¨ self.after ç¡®ä¿çº¿ç¨‹å®‰å…¨
#         self.after(0, self._trigger_daily_summary)






#     def on_closing(self):
#         """å¤„ç†çª—å£å…³é—­äº‹ä»¶ï¼Œå®‰å…¨åœ°åœæ­¢æ‰€æœ‰åå°çº¿ç¨‹ã€‚"""
#         print("æ­£åœ¨å…³é—­åº”ç”¨...")
#         self.processing_running = False
#         self.webcam_handler.stop()
#         self.voice_detector.stop_monitoring()
#         self.audio_player.stop()
#         self.hotkey_manager.stop_listener() # <-- æ·»åŠ è¿™ä¸€è¡Œ


#         # å‘é€ä¸€ä¸ªè™šæ‹Ÿæ¶ˆæ¯æ¥è§£é”é˜Ÿåˆ—çš„ .get() é˜»å¡
#         self.message_queue.put((99, 0, {"type": "shutdown", "content": ""}))
#         self.destroy()


#     def _schedule_daily_summary(self):
#         """è®¡ç®—è·ç¦»ä¸‹ä¸€ä¸ªæŠ¥å‘Šæ—¶é—´è¿˜æœ‰å¤šä¹…ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªå®šæ—¶å™¨ã€‚"""
#         now = datetime.now()
#         target_time = now.replace(hour=config.DAILY_SUMMARY_HOUR, minute=config.DAILY_SUMMARY_MINUTE, second=0, microsecond=0)

#         # å¦‚æœä»Šå¤©çš„ç›®æ ‡æ—¶é—´å·²ç»è¿‡å»ï¼Œåˆ™ç›®æ ‡è®¾ä¸ºæ˜å¤©
#         if now > target_time:
#             target_time = target_time.replace(day=now.day + 1)
        
#         # è®¡ç®—è·ç¦»ç›®æ ‡æ—¶é—´çš„ç§’æ•°
#         delay_seconds = (target_time - now).total_seconds()
        
#         print(f"æ¯æ—¥æ€»ç»“æŠ¥å‘Šå·²é¢„å®šã€‚ä¸‹ä¸€æ¬¡å°†åœ¨ {target_time.strftime('%Y-%m-%d %H:%M:%S')} (å¤§çº¦ {delay_seconds / 3600:.1f} å°æ—¶å) è§¦å‘ã€‚")
        
#         # afteræ–¹æ³•éœ€è¦æ¯«ç§’
#         delay_ms = int(delay_seconds * 1000)
        
#         # è®¾ç½®å®šæ—¶å™¨ï¼Œåœ¨æŒ‡å®šæ—¶é—´åè°ƒç”¨ _trigger_daily_summary
#         self.after(delay_ms, self._trigger_daily_summary)



# # ai_assistant/apps/multimedia_assistant.py

#     def _handle_daily_summary_message(self):
#         """
#         [Phase 4 ç»ˆæç‰ˆ] åŸºäº Plutchik å‘é‡æ•°æ®çš„æ·±åº¦å¿ƒç†æ€»ç»“ã€‚
#         """
#         today_str = datetime.now().strftime('%Y-%m-%d')
#         log_file_path = f'observation_log_{today_str}.jsonl'

#         print(f"æ­£åœ¨è¯»å–æ—¥å¿—æ–‡ä»¶: {log_file_path}")
        
#         # --- 1. æ•°æ®ç»Ÿè®¡å®¹å™¨ ---
#         total_records = 0
#         emotion_counts = {} # ç»Ÿè®¡å„åŸºç¡€æƒ…ç»ªå‡ºç°æ¬¡æ•°
#         complex_emotion_counts = {} # ç»Ÿè®¡å¤åˆæƒ…ç»ª (çˆ±, ç„¦è™‘...)
#         arousal_sum = 0.0 # ç”¨äºè®¡ç®—å¹³å‡å”¤é†’åº¦/å‹åŠ›å€¼
#         behavior_emotion_map = {} # è¡Œä¸ºä¸æƒ…ç»ªçš„å…³è”åˆ†æ
        
#         raw_lines = []

#         try:
#             if not os.path.exists(log_file_path):
#                 self.after(0, self.add_ai_message, "å¸†å“¥ï¼Œä»Šå¤©å¥½åƒè¿˜æ²¡æœ‰äº§ç”Ÿæ—¥å¿—æ•°æ®ï¼Œæ²¡æ³•å†™æ—¥è®°å“¦ã€‚")
#                 return

#             with open(log_file_path, 'r', encoding='utf-8') as f:
#                 raw_lines = f.readlines()

#             # --- 2. æ·±åº¦æ•°æ®åˆ†æ ---
#             for line in raw_lines:
#                 try:
#                     data = json.loads(line)
#                     total_records += 1
                    
#                     # æå–å…³é”®æŒ‡æ ‡
#                     vec = data.get('vector', {})
#                     complex_e = data.get('complex_emotion')
#                     behavior = data.get('behavior_desc', 'æœªçŸ¥')
                    
#                     # A. è®¡ç®—å”¤é†’åº¦ (Arousal) - å–å‘é‡æœ€å¤§å€¼
#                     if vec:
#                         current_arousal = max(vec.values())
#                         arousal_sum += current_arousal
                        
#                         # B. ç»Ÿè®¡ä¸»å¯¼æƒ…ç»ª
#                         dominant = max(vec, key=vec.get)
#                         emotion_counts[dominant] = emotion_counts.get(dominant, 0) + 1
                        
#                         # C. è¡Œä¸º-æƒ…ç»ª å…³è”åˆ†æ (ç®€å•çš„å…±ç°ç»Ÿè®¡)
#                         if behavior not in behavior_emotion_map:
#                             behavior_emotion_map[behavior] = []
#                         behavior_emotion_map[behavior].append(dominant)

#                     # D. ç»Ÿè®¡å¤åˆæƒ…ç»ª (è¿™æ˜¯é‡ç‚¹)
#                     if complex_e:
#                         complex_emotion_counts[complex_e] = complex_emotion_counts.get(complex_e, 0) + 1
                        
#                 except Exception as e:
#                     continue # è·³è¿‡æŸåçš„è¡Œ

#             if total_records == 0:
#                 self.after(0, self.add_ai_message, "ä»Šå¤©çš„è®°å½•å¥½åƒæ˜¯ç©ºçš„ï¼Ÿ")
#                 return

#             # --- 3. ç”Ÿæˆç»Ÿè®¡ç»“è®º ---
#             avg_arousal = arousal_sum / total_records
            
#             # æ‰¾å‡ºå‡ºç°é¢‘ç‡æœ€é«˜çš„æƒ…ç»ª
#             top_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:3]
#             top_complex = sorted(complex_emotion_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            
#             # æ„å»ºç»Ÿè®¡æ–‡æœ¬
#             stats_summary = (
#                 f"- æ€»è®°å½•æ•°: {total_records}æ¡\n"
#                 f"- å¹³å‡æƒ…ç»ªå”¤é†’åº¦(å‹åŠ›å€¼): {avg_arousal:.2f}/10.0\n"
#                 f"- æœ€å¸¸å‡ºç°çš„åŸºç¡€æƒ…ç»ª: {', '.join([k for k,v in top_emotions])}\n"
#             )
            
#             if top_complex:
#                 stats_summary += f"- **æ£€æµ‹åˆ°çš„æ·±å±‚çŠ¶æ€**: {', '.join([f'{k}({v}æ¬¡)' for k,v in top_complex])}\n"
            
#             # ç®€å•çš„è¡Œä¸ºå…³è”æ´å¯Ÿ
#             insight_text = ""
#             for beh, emos in behavior_emotion_map.items():
#                 # ç®€å•è®¡ç®—è¯¥è¡Œä¸ºä¸‹æœ€é«˜é¢‘çš„æƒ…ç»ª
#                 if len(emos) > 5: # æ ·æœ¬å¤Ÿå¤šæ‰åˆ†æ
#                     most_common = max(set(emos), key=emos.count)
#                     insight_text += f"- å½“ä½ åœ¨'{beh}'æ—¶ï¼Œæœ€å¸¸è§çš„æƒ…ç»ªæ˜¯'{most_common}'ã€‚\n"

#             # --- 4. æ„å»º AI Prompt ---
#             summary_prompt = (
#                 "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å¥åº·è¾…åŠ©AIï¼ˆå©‰æ™´ï¼‰ã€‚ç°åœ¨æ˜¯ç”±äºä¸€å¤©çš„ç»“æŸï¼Œè¯·æ ¹æ®ä»¥ä¸‹ã€å®¢è§‚è¡Œä¸ºä¸æƒ…æ„Ÿæ•°æ®ã€‘ï¼Œ"
#                 "ä¸ºç”¨æˆ·ï¼ˆæº¢æ¶›ï¼‰ç”Ÿæˆä¸€ä»½æ¸©æš–ã€æ·±åˆ»çš„ã€æ¯æ—¥å¿ƒç†å¤ç›˜ã€‘ã€‚\n\n"
#                 "ã€ä»Šæ—¥æ•°æ®ç»Ÿè®¡ã€‘\n"
#                 f"{stats_summary}\n"
#                 "ã€è¡Œä¸ºå…³è”æ´å¯Ÿã€‘\n"
#                 f"{insight_text}\n\n"
#                 "ã€å†™ä½œè¦æ±‚ã€‘\n"
#                 "1. **ä¸è¦**ç½—åˆ—æ¯ç‡¥çš„æ•°æ®ï¼Œè€Œæ˜¯æŠŠæ•°æ®è½¬åŒ–ä¸ºæ•…äº‹å’Œå…³å¿ƒã€‚\n"
#                 "2. å¦‚æœå¹³å‡å‹åŠ›å€¼è¶…è¿‡ 6.0ï¼Œæˆ–è€…å‡ºç°äº†'ç„¦è™‘'ï¼Œè¯·é‡ç‚¹å®‰æŠšå¹¶ç»™å‡ºå»ºè®®ã€‚\n"
#                 "3. å¦‚æœå‡ºç°äº†'çˆ±'æˆ–'ä¹è§‚'ï¼Œè¯·è‚¯å®šè¿™ä¸€å¤©ã€‚\n"
#                 "4. ç»“åˆè¡Œä¸ºæ´å¯Ÿï¼Œç»™ä»–ä¸€äº›æ˜å¤©çš„è¡ŒåŠ¨å»ºè®®ï¼ˆæ¯”å¦‚ï¼šæˆ‘çœ‹ä½ å·¥ä½œæ—¶å®¹æ˜“ç„¦è™‘ï¼Œæ˜å¤©è¦ä¸è¦...ï¼‰ã€‚\n"
#                 "5. è¯­æ°”è¦åƒè€æœ‹å‹å†™ä¿¡ï¼Œæ¸©æš–ã€çœŸè¯šã€‚"
#             )

#             print("æ­£åœ¨ç”Ÿæˆæ·±åº¦å¿ƒç†æ€»ç»“...")
#             self.after(0, self.add_ai_message, "æº¢æ¶›ï¼Œæˆ‘æ­£åœ¨åˆ†æä½ ä»Šå¤©çš„æƒ…æ„Ÿæ•°æ®ï¼Œä¸ºä½ ç”Ÿæˆå¿ƒç†å¤ç›˜æŠ¥å‘Š...")

#             # --- 5. è°ƒç”¨ AI ---
#             # ä½¿ç”¨ä¸´æ—¶çš„ contextï¼Œä¸æ±¡æŸ“çŸ­æœŸè®°å¿†
#             summary_context = [
#                 {"role": "system", "content": config.CBT_SYSTEM_PROMPT}, # å€Ÿç”¨CBTçš„ä¸“ä¸šäººè®¾
#                 {"role": "user", "content": summary_prompt}
#             ]
            
#             response = deepseek_client.chat.completions.create(
#                 model="deepseek-chat", messages=summary_context
#             )
#             summary_reply = response.choices[0].message.content

#             # --- 6. å±•ç¤ºä¸æ’­æŠ¥ ---
#             self.chat_context.append({"role": "assistant", "content": f"[æ¯æ—¥æ€»ç»“] {summary_reply}"})
#             self.after(0, self.add_ai_message, summary_reply)
#             self.audio_player.play_text(summary_reply, priority=0)

#         except Exception as e:
#             print(f"ç”Ÿæˆæ€»ç»“å‡ºé”™: {e}")
#             import traceback
#             traceback.print_exc()
#             self.after(0, self.add_ai_message, "ç”Ÿæˆæ€»ç»“æ—¶å‡ºäº†ä¸€ç‚¹å°å·®é”™ï¼Œæ˜å¤©å†è¯•å§ã€‚")


#     def _on_send_text_message(self):
#         """[æ–°å¢] å½“ç‚¹å‡»â€œå‘é€â€æŒ‰é’®æˆ–æŒ‰å›è½¦æ—¶è°ƒç”¨ã€‚"""
#         user_text = self.chat_entry.get()
        
#         # å¦‚æœè¾“å…¥ä¸ºç©ºï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œ
#         if not user_text.strip():
#             return
            
#         # 1. æ¸…ç©ºè¾“å…¥æ¡†
#         self.chat_entry.delete(0, "end")
        
#         # 2. åœ¨UIä¸Šæ˜¾ç¤ºç”¨æˆ·è‡ªå·±çš„æ¶ˆæ¯
#         self.add_user_message(user_text)
        
#         # 3. å°†æ–‡æœ¬æ¶ˆæ¯æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—ï¼Œä¸è¯­éŸ³è¾“å…¥ä½¿ç”¨ç›¸åŒçš„é€»è¾‘
#         self._add_to_message_queue(
#             priority=1, # ç”¨æˆ·ä¸»åŠ¨è¾“å…¥ï¼Œä¼˜å…ˆçº§é«˜
#             msg_type="voice_input", # å¤ç”¨è¯­éŸ³è¾“å…¥çš„å¤„ç†é€»è¾‘
#             content={"text": user_text}
#         )















#     #æ–°çš„æ–¹æ³•-è®¡ç®—æ—¶é—´
#     def _trigger_daily_summary(self):
#         """
#         [ä¸»çº¿ç¨‹è°ƒç”¨] å®šæ—¶å™¨è§¦å‘æ­¤æ–¹æ³•ï¼Œå¼€å§‹ç”ŸæˆæŠ¥å‘Šã€‚
#         """
#         print("æ—¶é—´åˆ°ï¼å¼€å§‹ç”Ÿæˆæ¯æ—¥æ€»ç»“æŠ¥å‘Š...")
        
#         # å°†ç”ŸæˆæŠ¥å‘Šçš„è€—æ—¶ä»»åŠ¡æ”¾å…¥æ¶ˆæ¯é˜Ÿåˆ—ï¼Œé¿å…é˜»å¡UI
#         self._add_to_message_queue(
#             priority=1, # æŠ¥å‘Šæ˜¯æ¯”è¾ƒé‡è¦çš„ä»»åŠ¡
#             msg_type="daily_summary",
#             content={} # ç›®å‰ä¸éœ€è¦é¢å¤–å†…å®¹
#         )
        
#         # ç”Ÿæˆå®Œä»Šå¤©çš„æŠ¥å‘Šåï¼Œç«‹å³é‡æ–°é¢„å®šæ˜å¤©çš„æŠ¥å‘Š
#         self._schedule_daily_summary()

#     def _trigger_care_speech(self, state, behavior, mode="light"):
#         """
#         [Phase 2] æ‰§è¡Œè¯´è¯åŠ¨ä½œã€‚
#         mode="light": æ™®é€šæœ‹å‹è¯­æ°”
#         mode="deep": å¿ƒç†å’¨è¯¢å¸ˆè¯­æ°” (CBT)
#         """
#         # [ä¿®å¤] ä» webcam_handler è·å–çœŸå®çš„æƒ…æ„Ÿå¼•æ“æ•°æ®
#         # è¿˜è¦æ³¨æ„ï¼šcurrent_state ç°åœ¨æ˜¯ numpy æ•°ç»„ï¼Œéœ€è¦è½¬æˆ dict æ‰èƒ½ä¼ ç»™ JSON
#         engine = self.webcam_handler.emotion_engine
#         vector_dict = engine.get_current_state_dict()

#         # æ„å»ºä¸€ä¸ªä¸´æ—¶çš„ content ç»“æ„ä¼ ç»™é˜Ÿåˆ—
#         content = {
#             "behavior_desc": behavior,
#             "emotion": state['ui_emotion'],
#             "complex_emotion": state['complex_emotion'],
#             "vector": vector_dict, # [ä¿®å¤å®Œæ¯•]
#             "mode": mode 
#         }
        
#         # ä½¿ç”¨ç‰¹æ®Šç±»å‹ action_response
#         self._add_to_message_queue(
#             priority=0 if mode == "deep" else 1,
#             msg_type="action_response", 
#             content=content
#         )












# def main():
#     """åº”ç”¨çš„å…¥å£å‡½æ•°ã€‚"""
#     app = MultimediaAssistantApp()
#     app.protocol("WM_DELETE_WINDOW", app.on_closing)
#     app.mainloop()



#ç¨‹åºä»æ­¤è¿›å…¥äº†äº‹ä»¶å¾ªç¯ï¼Œå¼€å§‹ç›‘å¬é¼ æ ‡ç‚¹å‡»ã€é”®ç›˜è¾“å…¥å’Œæˆ‘ä»¬è®¾å®šçš„å„ç§å®šæ—¶ä»»åŠ¡ã€‚



#     "WM_DELETE_WINDOW" (åè®®å)ï¼š
# è¿™æ˜¯æœ€å¸¸ç”¨çš„ä¸€ä¸ªåè®®åç§°ã€‚
# å®ƒä»£è¡¨äº†çª—å£ç®¡ç†å™¨å‘é€çš„ä¸€ä¸ªæ ‡å‡†æ¶ˆæ¯ï¼Œå…¶å«ä¹‰æ˜¯ï¼šâ€œç”¨æˆ·ç‚¹å‡»äº†çª—å£å³ä¸Šè§’çš„ X (å…³é—­) æŒ‰é’®â€ã€‚
# åœ¨ Tkinter çš„åº•å±‚ï¼Œè¿™å®é™…ä¸Šæ˜¯æˆªè·äº† X Window System æˆ– Windows API ä¸­çš„ä¸€ä¸ªç‰¹å®šç³»ç»Ÿä¿¡å·ã€‚
# app.on_closing (å›è°ƒå‡½æ•°)ï¼š
# è¿™æ˜¯æˆ‘ä»¬åœ¨ MultimediaAssistantApp ç±»ä¸­è‡ªå®šä¹‰çš„ä¸€ä¸ªæ–¹æ³•ã€‚
# å®ƒçš„ä½œç”¨æ˜¯å‘Šè¯‰ç¨‹åºï¼šâ€œå½“æ”¶åˆ° WM_DELETE_WINDOW æ¶ˆæ¯æ—¶ï¼Œä¸è¦æ‰§è¡Œé»˜è®¤çš„å…³é—­åŠ¨ä½œï¼Œè¯·è½¬è€Œå»æ‰§è¡Œ app.on_closing è¿™ä¸ªæ–¹æ³•ã€‚â€




# å¦‚æœä½ ä¸ç”¨ protocolï¼š
# ç”¨æˆ·ç‚¹å‡» X æŒ‰é’®ï¼Œçª—å£ä¼šç¬é—´æ¶ˆå¤±ã€‚
# ä½†æ˜¯ï¼Œç¨‹åºåº•å±‚çš„åå°çº¿ç¨‹ï¼ˆæ¯”å¦‚æ‘„åƒå¤´æ•æ‰çº¿ç¨‹ã€è¯­éŸ³ç›‘å¬çº¿ç¨‹ã€çƒ­é”®ç›‘å¬çº¿ç¨‹ï¼‰å¹¶ä¸ä¼šè‡ªåŠ¨åœæ­¢ã€‚
# ç»“æœï¼šç¨‹åºè™½ç„¶çœ‹ä¼¼å…³é—­äº†ï¼Œä½†åœ¨åå°ä»æœ‰è¿›ç¨‹åœ¨è¿è¡Œï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ‘„åƒå¤´æˆ–éº¦å…‹é£è¢«å ç”¨ï¼Œé€ æˆèµ„æºæ³„éœ²æˆ–ç¨‹åºå¡æ­»ã€‚
# ä½¿ç”¨äº† app.protocol("WM_DELETE_WINDOW", app.on_closing) ä¹‹åï¼š