=== PROJECT CODE SNAPSHOT ===

ä»£ç å›æº¯é˜²æ­¢ï¼›å‡ºç°ç‰¹å¤§äº‹æ•…
# requirements.txt (ç‰ˆæœ¬ v8 - Python 3.10+ ç»ˆæç‰ˆ)

# --- UI Framework ---
customtkinter
Pillow

# --- Core Processing ---
opencv-python
numpy

# --- AI & API Libraries ---
openai
oss2
dashscope

# --- FunASRå’Œæ‰€æœ‰ç›¸å…³ä¾èµ–çš„ç°ä»£ç¨³å®šç‰ˆæœ¬ ---
funasr
modelscope
transformers
sentencepiece

# --- FunASRçš„å…¶ä»–æ ¸å¿ƒä¾èµ– ---
# åœ¨Python 3.10+ç¯å¢ƒä¸­ï¼Œpipå¯ä»¥æ›´å¥½åœ°è‡ªåŠ¨å¤„ç†torchçš„ç‰ˆæœ¬
torch
torchaudio

# --- Audio Handling ---
PyAudio
pydub

# --- Visualization ---
matplotlib

# --- HTTP Client for Proxy Fix ---
httpx

# --- Optional for Development ---
keyboard


#------------------------------
pandas
seaborn


Generated from: c:\Users\YangYiTao\Desktop\DaChuang\Wanqing
Note: Libraries (venv) and Assets are excluded.


========================================
FILE_PATH: check_env.py
========================================

# check_env.py
import sys
import os

print("="*30)
print("ğŸ” å¼€å§‹ç¯å¢ƒè‡ªæ£€ç¨‹åº...")
print("="*30)

# 1. æ£€æŸ¥ç¬¬ä¸‰æ–¹åº“
try:
    import fastapi
    print("âœ… FastAPI å·²å®‰è£…")
    import uvicorn
    print("âœ… Uvicorn å·²å®‰è£…")
    import numpy as np
    print("âœ… Numpy å·²å®‰è£…")
    import cv2
    print("âœ… OpenCV (è§†è§‰) å·²å®‰è£…")
    import dashscope
    print("âœ… Dashscope (DeepSeek/Qwenä¾èµ–) å·²å®‰è£…")
    import funasr
    print("âœ… FunASR (è¯­éŸ³è¯†åˆ«) å·²å®‰è£…")
except ImportError as e:
    print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šç¼ºå°‘ä¾èµ–åº“ -> {e}")
    print("è¯·é‡æ–°è¿è¡Œ pip install å‘½ä»¤å®‰è£…ç¼ºå¤±çš„åº“ã€‚")
    sys.exit(1)

# 2. æ£€æŸ¥é¡¹ç›®è·¯å¾„å’Œé…ç½®
try:
    # å°è¯•å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒé…ç½®
    from ai_assistant.utils import config
    print("âœ… æœ¬åœ°æ¨¡å— ai_assistant è·¯å¾„æ­£å¸¸")
    
    # æ£€æŸ¥ Phase X çš„æ•°å­¦é…ç½®æ˜¯å¦å­˜åœ¨
    if hasattr(config, 'BASIS_VECTORS') and hasattr(config, 'REWARD_CONFIG'):
        print("âœ… é…ç½®æ–‡ä»¶å®Œæ•´ (åŒ…å« Phase X æ•°å­¦å†…æ ¸)")
    else:
        print("âš ï¸ è­¦å‘Šï¼šé…ç½®æ–‡ä»¶å¯èƒ½ç‰ˆæœ¬è¿‡æ—§ï¼Œç¼ºå°‘æ•°å­¦å®šä¹‰")

except Exception as e:
    print(f"âŒ é¡¹ç›®ç»“æ„é”™è¯¯ï¼šæ‰¾ä¸åˆ°ä»£ç æ¨¡å— -> {e}")
    print("è¯·ç¡®è®¤ ai_assistant æ–‡ä»¶å¤¹æ˜¯å¦å®Œæ•´å¤åˆ¶è¿‡æ¥äº†ã€‚")
    sys.exit(1)

# 3. æ£€æŸ¥æƒ…æ„Ÿå¼•æ“ (æ•°å­¦è¿ç®—æµ‹è¯•)
try:
    print("Testing æƒ…æ„Ÿå¼•æ“åˆå§‹åŒ–...")
    from ai_assistant.core.emotion_engine import EmotionEngine
    engine = EmotionEngine()
    
    # æ¨¡æ‹Ÿè¾“å…¥ä¸€ä¸ªå¼€å¿ƒçš„æ•°æ®ï¼Œçœ‹æ˜¯å¦æŠ¥é”™
    test_input = {"å–œæ‚¦": 8.0, "ä¿¡ä»»": 5.0}
    engine.update(test_input)
    arousal = engine.get_arousal_level()
    
    print(f"âœ… æƒ…æ„Ÿå¼•æ“è¿ç®—æ­£å¸¸ (æ¨¡æ‹Ÿå”¤é†’åº¦: {arousal})")
    print("âœ… Phase X æ•°å­¦é€»è¾‘éªŒè¯é€šè¿‡")

except Exception as e:
    print(f"âŒ æƒ…æ„Ÿå¼•æ“å´©æºƒ -> {e}")
    sys.exit(1)

print("="*30)
print("ğŸ‰ æ­å–œï¼ç¯å¢ƒå®Œç¾ï¼Œå¯ä»¥å¼€å§‹é‡æ„åç«¯äº†ï¼")
print("="*30)

========================================
FILE_PATH: README.md
========================================

HEAD
# Wan-qing-1.0
Wanqing's first version
# AIä¼™ä¼´â€œå©‰æ™´â€ï¼šå¤šæ¨¡æ€æ„ŸçŸ¥ä¸ä¸»åŠ¨å…³æ€€æ™ºèƒ½ç³»ç»Ÿ

[![Status](https://img.shields.io/badge/status-active-success.svg)]()
[![License](https://img.shields.io/badge/license-All%20Rights%20Reserved-red.svg)]()

**AIä¼™ä¼´â€œå©‰æ™´â€** æ˜¯ä¸€ä¸ªåŸºäºPythonå¼€å‘çš„PCç«¯å¤šæ¨¡æ€AIåŠ©æ‰‹ã€‚å®ƒåˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³è¯†åˆ«æŠ€æœ¯ï¼Œå®æ—¶æ„ŸçŸ¥ç”¨æˆ·çš„è¡Œä¸ºä¸æƒ…ç»ªçŠ¶æ€ï¼Œå¹¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›ä¸»åŠ¨ã€æ™ºèƒ½çš„é™ªä¼´ä¸å…³æ€€ã€‚

æœ¬é¡¹ç›®æ—¨åœ¨æ¢ç´¢AIåœ¨æƒ…æ„Ÿè®¡ç®—ä¸ä¸ªæ€§åŒ–äº¤äº’é¢†åŸŸçš„åº”ç”¨ï¼Œå°†ä¸€ä¸ªå¼ºå¤§çš„AIæ¨¡å‹ä»äº‘ç«¯å¸¦åˆ°ç”¨æˆ·æ¡Œé¢ï¼Œæˆä¸ºä¸€ä¸ªæœ‰æ¸©åº¦ã€æ‡‚å…³æ€€çš„æ™ºèƒ½ä¼™ä¼´ã€‚

---

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

*   **å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿ**:
    *   **ğŸ‘ï¸ è§†è§‰æ„ŸçŸ¥**: é€šè¿‡æ‘„åƒå¤´å®æ—¶åˆ†æç”¨æˆ·çš„**7ç§æ ¸å¿ƒè¡Œä¸º**ï¼ˆå¦‚å·¥ä½œã€ç©æ‰‹æœºï¼‰å’Œ**6ç§æ ¸å¿ƒæƒ…ç»ª**ï¼ˆå¦‚å¼€å¿ƒã€ç–²æƒ«ï¼‰ï¼Œç”±Qwen-VLå¤§æ¨¡å‹æä¾›æ”¯æŒã€‚
    *   **ğŸ‘‚ è¯­éŸ³æ„ŸçŸ¥**: é‡‡ç”¨åŠ¨æ€å™ªéŸ³æ ¡å‡†çš„**è¯­éŸ³æ´»åŠ¨æ£€æµ‹(VAD)**æŠ€æœ¯ï¼Œç²¾å‡†æ•æ‰ç”¨æˆ·è¯è¯­ã€‚ä½¿ç”¨**æœ¬åœ°FunASRæ¨¡å‹**è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œä¿è¯ä½å»¶è¿Ÿä¸éšç§å®‰å…¨ã€‚

*   **æ™ºèƒ½å†³ç­–ä¸äº¤äº’ç³»ç»Ÿ**:
    *   **ğŸ§  äººæ ¼åŒ–å¤§è„‘**: åŸºäºDeepSeekå¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è¯¦ç»†çš„ç³»ç»Ÿçº§Promptä¸ºâ€œå©‰æ™´â€æ³¨å…¥äº†ç‹¬ç‰¹çš„äººæ ¼ä¸å…³æ€€é€»è¾‘ã€‚
    *   **â¤ï¸â€ğŸ©¹ ä¸»åŠ¨æƒ…ç»ªå…³æ€€**: æ–°å¢æ ¸å¿ƒåŠŸèƒ½ï¼ç³»ç»Ÿèƒ½å¤Ÿè¿½è¸ªç”¨æˆ·çš„çŸ­æœŸæƒ…ç»ªå˜åŒ–ï¼Œåœ¨æ£€æµ‹åˆ°æŒç»­è´Ÿé¢æƒ…ç»ªæ—¶**ä¸»åŠ¨å‘èµ·å®‰æ…°ä¸å…³æ€€**ã€‚
    *   **ğŸ¤« é€‰æ‹©æ€§å‘è¨€**: ç‹¬åˆ›çš„â€œå®ˆé—¨å‘˜â€é€»è¾‘ï¼Œç»“åˆç”¨æˆ·çŠ¶æ€å˜åŒ–å’Œæ—¶é—´å†·å´ï¼Œä½¿AIçš„ä»‹å…¥æ—¶æœºæ›´æ™ºèƒ½ã€ä¸æ‰“æ‰°ã€‚
    *   **ğŸ—£ï¸ æµç•…è¯­éŸ³å¯¹è¯**: é‡‡ç”¨**ä¼˜å…ˆçº§é˜Ÿåˆ—**ç®¡ç†TTSï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰ä»»åŠ¡ï¼Œç¡®ä¿å…³é”®å¯¹è¯èƒ½å¤Ÿâ€œæ’é˜Ÿâ€æ‰“æ–­å¸¸è§„æ’­æŠ¥ï¼Œå®ç°æµç•…è‡ªç„¶çš„äº¤äº’ä½“éªŒã€‚

*   **é•¿æœŸè®°å¿†ä¸åæ€èƒ½åŠ›**:
    *   **âœï¸ é•¿æœŸè®°å¿†**: æ‰€æœ‰çš„è§‚å¯Ÿä¸äº¤äº’éƒ½ä¼šè¢«è®°å½•åˆ°æœ¬åœ°æ—¥å¿—æ–‡ä»¶ä¸­ï¼Œå½¢æˆAIçš„é•¿æœŸâ€œæ—¥è®°â€ã€‚
    *   **ğŸ“… æ¯æ—¥æ€»ç»“**: æ¯æ—¥å®šæ—¶æˆ–é€šè¿‡å¿«æ·é”®æ‰‹åŠ¨è§¦å‘ï¼ŒAIä¼šâ€œå¤ç›˜â€å½“å¤©çš„æ—¥å¿—ï¼Œå¹¶ç”Ÿæˆä¸€ä»½å……æ»¡æ´å¯ŸåŠ›ä¸äººæƒ…å‘³çš„**æ¯æ—¥æ€»ç»“æŠ¥å‘Š**ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

*   Python 3.10.6
*   ä¸€ä¸ªæ”¯æŒCUDAçš„NVIDIAæ˜¾å¡ï¼ˆç”¨äºæœ¬åœ°ASRæ¨¡å‹åŠ é€Ÿï¼Œå¯é€‰ï¼‰
*   éº¦å…‹é£å’Œæ‘„åƒå¤´

### 2. å®‰è£…

a. **å…‹éš†æˆ–ä¸‹è½½æœ¬é¡¹ç›®åˆ°æœ¬åœ°**

b. **åˆ›å»ºå¹¶æ¿€æ´»Pythonè™šæ‹Ÿç¯å¢ƒ**
   ```bash
   # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
   python -m venv venv
   # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Windows)
   .\venv\Scripts\activate
   # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (macOS/Linux)
   source venv/bin/activate
   ```

c. **å®‰è£…ä¾èµ–**
   æœ¬é¡¹ç›®æ‰€æœ‰ä¾èµ–éƒ½å·²é”å®šåœ¨`requirements.txt`ä¸­ï¼Œä»¥ç¡®ä¿ç¯å¢ƒçš„ç¨³å®šæ€§ã€‚
   ```bash
   pip install -r requirements.txt
   ```

### 3. é…ç½®

a. æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ `utils/config.py` æ–‡ä»¶ã€‚

b. **å¡«å†™ä½ çš„APIå¯†é’¥**ï¼š
   ```python
   # Deepseek API é…ç½®
   DEEPSEEK_API_KEY = 'sk-xxxxxxxxxxxxxxxxxxxxxxxx'

   # Qwen-VL (Dashscope) APIé…ç½®
   QWEN_API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxx"

   # OSSé…ç½® (ç”¨äºè§†è§‰åˆ†æ)
   OSS_ACCESS_KEY_ID = 'LTAIxxxxxxxxxxxxxxxx'
   OSS_ACCESS_KEY_SECRET = 'xxxxxxxxxxxxxxxxxxxxxxxx'
   OSS_BUCKET = 'your-bucket-name'
   OSS_ENDPOINT = 'oss-cn-beijing.aliyuncs.com' # æ ¹æ®ä½ çš„OSSåŒºåŸŸä¿®æ”¹
   ```

c. **(å¯é€‰) è°ƒæ•´è¡Œä¸ºå‚æ•°**:
   ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´`config.py`ä¸­çš„è¡Œä¸ºå‚æ•°ï¼Œå¦‚åˆ†æé¢‘ç‡ã€å›åº”å†·å´æ—¶é—´ç­‰ã€‚

### 4. è¿è¡Œ

ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè¿è¡Œä¸»ç¨‹åºå³å¯å¯åŠ¨â€œå©‰æ™´â€ã€‚
```bash
python run_assistant.py
```


## ğŸ› ï¸ æŠ€æœ¯æ ˆ

*   **æ ¸å¿ƒæ¡†æ¶**: CustomTkinter
*   **è§†è§‰å¤„ç†**: OpenCV, Pillow, Qwen-VL API
*   **è¯­éŸ³å¤„ç†**: PyAudio, FunASR (æœ¬åœ°), Dashscope TTS API
*   **AIå¤§è„‘**: DeepSeek API
*   **å¼‚æ­¥å¤„ç†**: `threading`, `queue`
*   **äº‘å­˜å‚¨**: Aliyun OSS

---


# Proactive Affective Agent based on Vector Dynamics and Utility Maximization
# åŸºäºå‘é‡åŠ¨åŠ›å­¦ä¸æ•ˆç”¨æœ€å¤§åŒ–çš„ä¸»åŠ¨å¼æƒ…æ„Ÿäº¤äº’æ¡†æ¶

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)

## ğŸ“– V3.0ç®€ä»‹ (Introduction)

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªå…·å¤‡**æ‹ŸäººåŒ–å¿ƒæ™º**çš„æ•°å­—ç”Ÿå‘½ä½“æ¡†æ¶ã€‚ä¸åŒäºä¼ ç»Ÿçš„åŸºäºè§„åˆ™ï¼ˆRule-basedï¼‰çš„èŠå¤©æœºå™¨äººï¼Œæœ¬ç³»ç»Ÿå¼•å…¥äº†**æƒ…æ„Ÿè®¡ç®—**ä¸**æ§åˆ¶ç†è®º**ï¼Œæ„å»ºäº†ä¸€ä¸ªæ‹¥æœ‰â€œå†…å¿ƒä¸–ç•Œâ€çš„ AI ä¼™ä¼´ã€‚

ç³»ç»Ÿæ ¸å¿ƒå®ç°äº†ä»**FSMï¼ˆæœ‰é™çŠ¶æ€æœºï¼‰**å‘**POMDPï¼ˆéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼‰**çš„å·¥ç¨‹è¿‘ä¼¼è½¬å‹ï¼Œå…·å¤‡**æƒ…æ„Ÿæƒ¯æ€§**ã€**ç¨³æ€è°ƒèŠ‚**ä»¥åŠ**ä»·å€¼é©±åŠ¨çš„ç†æ€§å†³ç­–**èƒ½åŠ›ã€‚

## ğŸš€ æ ¸å¿ƒç‰¹æ€§ (Key Features)

### 1. æ„ŸçŸ¥å±‚ï¼šä»¿ç”Ÿå‘é‡åŠ¨åŠ›å­¦ (Biomimetic Vector Dynamics)
- **Plutchik å‘é‡ç©ºé—´**ï¼šå°†æƒ…æ„Ÿå»ºæ¨¡ä¸º $\mathbb{R}^8$ è¿ç»­å‘é‡ç©ºé—´ï¼Œè€Œéç¦»æ•£æ ‡ç­¾ã€‚
- **æƒ¯æ€§ä¸ç¨³æ€**ï¼šå¼•å…¥æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA) ä¸çƒ­åŠ›å­¦è€—æ•£æœºåˆ¶ï¼Œæ¨¡æ‹Ÿç”Ÿç‰©æƒ…ç»ªçš„è¿ç»­æ€§ä¸è‡ªç„¶æ¶ˆé€€ã€‚
  $$ S_{t+1} \approx S_t \cdot (1 - \lambda_{decay}) $$

### 2. è®¤çŸ¥å±‚ï¼šæ¨¡ç³Šé€»è¾‘åˆæˆ (Fuzzy Cognitive Synthesis)
- åˆ©ç”¨ **Sigmoid éš¶å±åº¦å‡½æ•°** ä¸ **Zadeh ç®—å­**ï¼Œé‡åŒ–è®¡ç®—â€œçˆ±â€ã€â€œç„¦è™‘â€ã€â€œä¹è§‚â€ç­‰å¤åˆæƒ…ç»ªã€‚
- åŸºäº **L2 èŒƒæ•° (Energy Norm)** å®æ—¶è®¡ç®—æƒ…ç»ªå”¤é†’åº¦ (Arousal)ã€‚

### 3. å†³ç­–å±‚ï¼šä»·å€¼é©±åŠ¨å†³ç­– (Value-Driven Decision Making)
- åŸºäº **ç¡®å®šæ€§æ•ˆç”¨å‡½æ•° (Deterministic Utility Function)** è¿›è¡Œç†æ€§é€‰æ‹©ï¼š
  $$ U(a) = R_{state} + \Phi(I_{arousal}) - C_{cost} - P_{decay} $$
- **è‡ªé€‚åº”å­¦ä¹ **ï¼šé€šè¿‡ Delta è§„åˆ™æ ¹æ®ç”¨æˆ·åé¦ˆåŠ¨æ€è°ƒæ•´ç­–ç•¥æƒé‡ã€‚

### 4. å¹²é¢„å±‚ï¼šCBT åˆ†å±‚å¹²é¢„ (Hierarchical Intervention)
- **Low Arousal**: ä¼´éšå¼é—²èŠã€‚
- **High Arousal**: è‡ªåŠ¨è§¦å‘ **è®¤çŸ¥è¡Œä¸ºç–—æ³• (CBT)** æµç¨‹ï¼Œè¿›è¡Œå¿ƒç†æ€¥æ•‘ã€‚




## ğŸ“œ ç‰ˆæƒå£°æ˜
æœ¬é¡¹ç›®ç‰ˆæƒå½’ **[é˜³æº¢æ¶›å¸¦é¢†çš„Bookæ€è®®å°ç»„-é˜³æº¢æ¶›ï¼Œå»–ç»†èŠ¸ï¼Œç‹éœ²é¢–ï¼Œé˜®ä¹æ˜Ÿï¼Œé»„ç…œæ¬£ï¼ˆæ’åä¸åˆ†å…ˆåæ¬¡åºï¼‰]** æ‰€æœ‰ï¼Œå¹¶ä¿ç•™ä¸€åˆ‡æƒåˆ©ã€‚

æœªç»ç‰ˆæƒæŒæœ‰äººä¹¦é¢è®¸å¯ï¼Œä»»ä½•äººä¸å¾—å¤åˆ¶ã€ä¿®æ”¹ã€åˆ†å‘æˆ–ç”¨äºå•†ä¸šç›®çš„ã€‚
>>>>>>> b3803ca (first commit)


========================================
FILE_PATH: requirements.txt
========================================

# requirements.txt (ç‰ˆæœ¬ v8 - Python 3.10+ ç»ˆæç‰ˆ)

# --- UI Framework ---
customtkinter
Pillow

# --- Core Processing ---
opencv-python
numpy

# --- AI & API Libraries ---
openai
oss2
dashscope

# --- FunASRå’Œæ‰€æœ‰ç›¸å…³ä¾èµ–çš„ç°ä»£ç¨³å®šç‰ˆæœ¬ ---
funasr
modelscope
transformers
sentencepiece

# --- FunASRçš„å…¶ä»–æ ¸å¿ƒä¾èµ– ---
# åœ¨Python 3.10+ç¯å¢ƒä¸­ï¼Œpipå¯ä»¥æ›´å¥½åœ°è‡ªåŠ¨å¤„ç†torchçš„ç‰ˆæœ¬
torch
torchaudio

# --- Audio Handling ---
PyAudio
pydub

# --- Visualization ---
matplotlib

# --- HTTP Client for Proxy Fix ---
httpx

# --- Optional for Development ---
keyboard


#------------------------------
pandas
seaborn

========================================
FILE_PATH: run_assistant.py
========================================

# run_assistant.py

# ===============================================================
# å¤šæ¨¡æ€AIåŠ©æ‰‹ (å¸†å“¥ç›‘ç£å™¨) - å¯åŠ¨å…¥å£
# ===============================================================
#
# å¦‚ä½•è¿è¡Œ:
# 1. ç¡®ä¿ä½ å·²ç»é€šè¿‡ `pip install -r requirements.txt` å®‰è£…äº†æ‰€æœ‰ä¾èµ–ã€‚
# 2. åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œä»ç»ˆç«¯è¿è¡Œæ­¤æ–‡ä»¶:
#    python run_assistant.py
#
# ===============================================================

import sys
import os

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonçš„æ¨¡å—æœç´¢è·¯å¾„ä¸­
# è¿™æ ·åšå¯ä»¥ç¡®ä¿ `from ai_assistant...` å¯¼å…¥è¯­å¥èƒ½å¤Ÿæ­£ç¡®æ‰¾åˆ°æ¨¡å—
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

# ä»æˆ‘ä»¬çš„åŒ…ä¸­å¯¼å…¥ä¸»åº”ç”¨çš„å…¥å£å‡½æ•°
from ai_assistant.apps.multimedia_assistant import main

if __name__ == "__main__":
    print("===================================")
    print("  æ­£åœ¨å¯åŠ¨ å¤šæ¨¡æ€AIåŠ©æ‰‹... ")
    print("===================================")
    
    # è°ƒç”¨ä¸»å‡½æ•°ï¼Œå¯åŠ¨åº”ç”¨
    main()

========================================
FILE_PATH: run_visualizer.py
========================================

# run_visualizer.py

# ===============================================================
# è¡Œä¸ºç›‘æµ‹ä¸å¯è§†åŒ–ç³»ç»Ÿ - å¯åŠ¨å…¥å£
# ===============================================================
#
# å¦‚ä½•è¿è¡Œ:
# 1. ç¡®ä¿ä½ å·²ç»é€šè¿‡ `pip install -r requirements.txt` å®‰è£…äº†æ‰€æœ‰ä¾èµ–ã€‚
# 2. åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œä»ç»ˆç«¯è¿è¡Œæ­¤æ–‡ä»¶:
#    python run_visualizer.py
#
# ===============================================================

import sys
import os

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonçš„æ¨¡å—æœç´¢è·¯å¾„ä¸­
# è¿™æ ·åšå¯ä»¥ç¡®ä¿ `from ai_assistant...` å¯¼å…¥è¯­å¥èƒ½å¤Ÿæ­£ç¡®æ‰¾åˆ°æ¨¡å—
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

# ä»æˆ‘ä»¬çš„åŒ…ä¸­å¯¼å…¥ä¸»åº”ç”¨çš„å…¥å£å‡½æ•°
from ai_assistant.apps.behavior_visualizer_app import main

if __name__ == "__main__":
    print("=======================================")
    print("  æ­£åœ¨å¯åŠ¨ è¡Œä¸ºç›‘æµ‹ä¸å¯è§†åŒ–ç³»ç»Ÿ... ")
    print("=======================================")

    # è°ƒç”¨ä¸»å‡½æ•°ï¼Œå¯åŠ¨åº”ç”¨
    main()

========================================
FILE_PATH: Test Package.txt
========================================


customtkinter
Pillow
opencv-python
numpy
openai
oss2
dashscope>=1.16.1
funasr==1.0.12
modelscope==1.9.5
torch
torchaudio
PyAudio
pydub
matplotlib
httpx
keyboard

sentencepiece==0.1.99
transformers==4.35.2
protobuf==3.20.3 




# Package                Version
# ---------------------- -----------
# addict                 2.4.0
# aiohappyeyeballs       2.4.4
# aiohttp                3.10.11
# aiosignal              1.3.1
# aliyun-python-sdk-core 2.16.0
# aliyun-python-sdk-kms  2.16.5
# annotated-types        0.7.0
# antlr4-python3-runtime 4.9.3
# anyio                  4.5.2
# async-timeout          5.0.1
# attrs                  25.3.0
# audioread              3.0.1
# certifi                2025.8.3
# cffi                   1.17.1
# charset-normalizer     3.4.3
# colorama               0.4.6
# contourpy              1.1.1
# crcmod                 1.7
# cryptography           46.0.1
# customtkinter          5.2.2
# cycler                 0.12.1
# darkdetect             0.8.0
# dashscope              1.24.6
# datasets               3.1.0
# decorator              5.2.1
# dill                   0.3.8
# distro                 1.9.0
# editdistance           0.8.1
# einops                 0.8.1
# exceptiongroup         1.3.0
# filelock               3.16.1
# fonttools              4.57.0
# frozenlist             1.5.0
# fsspec                 2024.9.0
# funasr                 1.0.12
# gast                   0.6.0
# h11                    0.16.0
# httpcore               1.0.9
# httpx                  0.27.2
# huggingface-hub        0.35.1
# hydra-core             1.3.2
# idna                   3.10
# importlib_metadata     8.5.0
# importlib_resources    6.4.5
# jaconv                 0.4.0
# jamo                   0.4.1
# jieba                  0.42.1
# Jinja2                 3.1.6
# jmespath               0.10.0
# joblib                 1.4.2
# kaldiio                2.18.1
# keyboard               0.13.5
# kiwisolver             1.4.7
# lazy_loader            0.4
# librosa                0.11.0
# llvmlite               0.41.1
# MarkupSafe             2.1.5
# matplotlib             3.7.5
# modelscope             1.13.3
# mpmath                 1.3.0
# msgpack                1.1.1
# multidict              6.1.0
# multiprocess           0.70.16
# networkx               3.1
# numba                  0.58.1
# numpy                  1.24.4
# omegaconf              2.3.0
# openai                 1.6.1
# opencv-python          4.9.0.80
# oss2                   2.19.1
# packaging              25.0
# pandas                 2.0.3
# pillow                 10.4.0
# pip                    25.0.1
# platformdirs           4.3.6
# pooch                  1.8.2
# propcache              0.2.0
# protobuf               5.29.5
# pyarrow                17.0.0
# PyAudio                0.2.14
# pycparser              2.23
# pycryptodome           3.23.0
# pydantic               2.10.6
# pydantic_core          2.27.2
# pydub                  0.25.1
# pynndescent            0.5.13
# pyparsing              3.1.4
# python-dateutil        2.9.0.post0
# pytorch-wpe            0.0.1
# pytz                   2025.2
# PyYAML                 6.0.3
# regex                  2024.11.6
# requests               2.32.4
# safetensors            0.5.3
# scikit-learn           1.3.2
# scipy                  1.10.1
# sentencepiece          0.1.99
# setuptools             56.0.0
# simplejson             3.20.2
# six                    1.17.0
# sniffio                1.3.1
# sortedcontainers       2.4.0
# soundfile              0.13.1
# soxr                   0.3.7
# sympy                  1.13.3
# tensorboardX           2.6.2.2
# threadpoolctl          3.5.0
# tokenizers             0.15.2
# tomli                  2.2.1
# torch                  2.1.2
# torch-complex          0.4.4
# torchaudio             2.1.2
# tqdm                   4.67.1
# transformers           4.35.2
# typing_extensions      4.13.2
# tzdata                 2025.2
# umap-learn             0.5.7
# urllib3                2.2.3
# websocket-client       1.8.0
# xxhash                 3.5.0
# yapf                   0.43.0
# yarl                   1.15.2
# zipp                   3.20.2








# requirements.txt (ç‰ˆæœ¬ v5 - ç»ˆæå…¼å®¹ç‰ˆ)

# --- UI Framework ---
customtkinter==5.2.2
Pillow==10.1.0

# --- Core Processing ---
opencv-python==4.9.0.80
numpy==1.24.4

# --- AI & API Libraries ---
openai==1.6.1
oss2==2.18.4
dashscope>=1.16.1

# --- å…³é”®ä¿®æ­£ï¼šé”å®šFunASRå’Œæ‰€æœ‰ç›¸å…³ä¾èµ–åˆ°ç»è¿‡éªŒè¯çš„ç¨³å®šç‰ˆæœ¬ç»„åˆ ---
funasr==1.0.12
modelscope==1.7.0
# å…³é”®ä¾èµ–åº“ï¼Œä¸modelscope 1.9.5 å…¼å®¹
sentencepiece==0.1.99
transformers==4.30.2

# --- FunASRçš„å…¶ä»–æ ¸å¿ƒä¾èµ– ---
torch==2.1.2
torchaudio==2.1.2

# --- Audio Handling ---
PyAudio==0.2.14
pydub==0.25.1

# --- Visualization ---
matplotlib==3.7.5

# --- HTTP Client for Proxy Fix ---
httpx==0.25.2

# --- Optional for Development ---
keyboard==0.13.5



# modelscope ä» 1.9.5 é™çº§åˆ°äº† 1.7.0ã€‚
# transformers ä» 4.33.1 é™çº§åˆ°äº† 4.30.2ã€‚

========================================
FILE_PATH: ai_assistant\__init__.py
========================================



========================================
FILE_PATH: ai_assistant\apps\behavior_visualizer_app.py
========================================

# ai_assistant/apps/behavior_visualizer_app.py

import customtkinter as ctk
from datetime import datetime
from PIL import Image

# ä»æˆ‘ä»¬è‡ªå·±çš„åŒ…é‡Œå¯¼å…¥æ¨¡å—
from ai_assistant.core.webcam_handler import WebcamHandler
from ai_assistant.ui.charts import BehaviorVisualizer

class BehaviorVisualizationApp(ctk.CTk):
    """
    è¡Œä¸ºç›‘æµ‹ä¸å¯è§†åŒ–çš„ä¸»åº”ç”¨çª—å£ã€‚
    è¿™ä¸ªç±»è´Ÿè´£æ•´åˆæ ¸å¿ƒé€»è¾‘(WebcamHandler)å’ŒUIå±•ç¤º(BehaviorVisualizer)ã€‚
    """
    
    def __init__(self):
        super().__init__()
        self.title("è¡Œä¸ºç›‘æµ‹ä¸å¯è§†åŒ–ç³»ç»Ÿ")
        self.geometry("1200x700") # è®¾ç½®ä¸€ä¸ªé€‚åˆå±•ç¤ºå›¾è¡¨çš„å°ºå¯¸
        self.configure(fg_color="#1a1a1a")

        self._setup_ui()
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.webcam_handler = WebcamHandler(self)
        
        # å»¶è¿Ÿ1ç§’åå¯åŠ¨æ‘„åƒå¤´ï¼Œç»™UIä¸€ç‚¹æ—¶é—´å®Œå…¨åŠ è½½
        self.after(1000, self.webcam_handler.start)


    def _setup_ui(self):
        """é…ç½®ä¸»çª—å£çš„UIå¸ƒå±€ã€‚"""
        self.grid_columnconfigure(0, weight=1)
        self.grid_rowconfigure(0, weight=0) # æ ‡é¢˜è¡Œ (å›ºå®šé«˜åº¦)
        self.grid_rowconfigure(1, weight=1) # å›¾è¡¨è¡Œ (å¯æ‹‰ä¼¸)
        self.grid_rowconfigure(2, weight=0) # çŠ¶æ€è¡Œ (å›ºå®šé«˜åº¦)

        # --- æ ‡é¢˜ ---
        title_label = ctk.CTkLabel(self, text="è¡Œä¸ºç›‘æµ‹ä¸å¯è§†åŒ–ç³»ç»Ÿ", font=("Arial", 24, "bold"), text_color="white")
        title_label.grid(row=0, column=0, pady=15)

        # --- å¯è§†åŒ–å›¾è¡¨åŒºåŸŸ ---
        # åˆ›å»ºä¸€ä¸ªæ¡†æ¶æ¥å®¹çº³å›¾è¡¨
        main_frame = ctk.CTkFrame(self, fg_color="#1a1a1a")
        main_frame.grid(row=1, column=0, sticky="nsew", padx=10, pady=10)
        # å°†BehaviorVisualizerå®ä¾‹æ”¾å…¥è¿™ä¸ªæ¡†æ¶
        self.behavior_visualizer = BehaviorVisualizer(main_frame)
        
        # --- çŠ¶æ€ä¸æ§åˆ¶æ  ---
        status_frame = ctk.CTkFrame(self, fg_color="#2a2a2a", corner_radius=0)
        status_frame.grid(row=2, column=0, sticky="ew")
        status_frame.grid_columnconfigure(2, weight=1) # è®©çŠ¶æ€æ ‡ç­¾é å³

        # æ§åˆ¶æŒ‰é’®
        self.toggle_button = ctk.CTkButton(status_frame, text="æš‚åœåˆ†æ", command=self._toggle_analysis)
        self.toggle_button.grid(row=0, column=0, padx=15, pady=10)
        
        self.toggle_camera_button = ctk.CTkButton(status_frame, text="æ˜¾ç¤º/éšè—æ‘„åƒå¤´", command=self._toggle_camera_window)
        self.toggle_camera_button.grid(row=0, column=1, padx=0, pady=10)
        
        # çŠ¶æ€æ ‡ç­¾
        self.status_label = ctk.CTkLabel(status_frame, text="æ­£åœ¨åˆå§‹åŒ–...", text_color="white", anchor="e")
        self.status_label.grid(row=0, column=2, padx=15, pady=10, sticky="e")




# [Phase 1.5 å…¼å®¹æ€§ä¿®å¤] å¢åŠ å‚æ•°ä»¥åŒ¹é… WebcamHandler çš„æ–°è°ƒç”¨æ ‡å‡†
# ai_assistant/apps/behavior_visualizer_app.py

    def handle_analysis_result(self, timestamp: datetime, analysis_text: str, 
                               behavior_num: str, behavior_desc: str, 
                               emotion: str, screenshot: Image.Image,
                               complex_emotion: str = None, 
                               emotion_vector: dict = None): # ç¡®ä¿å‚æ•°åœ¨è¿™é‡Œ
        """
        å›è°ƒå‡½æ•°ï¼šå¤„ç†åˆ†æç»“æœ
        """
        # 1. æ›´æ–°çŠ¶æ€æ æ–‡å­—
        status_text = f"æœ€æ–°æ£€æµ‹: {behavior_desc} | è¡¨é¢æƒ…ç»ª: {emotion}"
        if complex_emotion:
             status_text += f" | æ·±åº¦çŠ¶æ€: {complex_emotion}"
        self.update_status(status_text)
        
        # 2. æ›´æ–°å·¦ä¾§è¡Œä¸ºå›¾
        self.behavior_visualizer.add_behavior_data(timestamp, behavior_num)
        
        # 3. [Phase 2 æ–°å¢] æ›´æ–°å³ä¾§é›·è¾¾å›¾
        # å¦‚æœæœ‰ emotion_vectorï¼Œå°±ä¼ ç»™ visualizer
        if emotion_vector:
            self.behavior_visualizer.update_emotion_data(emotion_vector)





    def update_status(self, text: str):
        """æ›´æ–°UIä¸Šçš„çŠ¶æ€æ ‡ç­¾ã€‚"""
        self.status_label.configure(text=text)

    def _toggle_analysis(self):
        """åˆ‡æ¢åˆ†æçš„æš‚åœ/æ¢å¤çŠ¶æ€ã€‚"""
        self.webcam_handler.toggle_pause()
        new_text = "æ¢å¤åˆ†æ" if self.webcam_handler.paused else "æš‚åœåˆ†æ"
        self.toggle_button.configure(text=new_text)
        
    def _toggle_camera_window(self):
        """åˆ‡æ¢æ‘„åƒå¤´çª—å£çš„æ˜¾ç¤º/éšè—ã€‚"""
        self.webcam_handler.toggle_camera_window()

    def on_closing(self):
        """
        å¤„ç†çª—å£å…³é—­äº‹ä»¶ï¼Œç¡®ä¿æ‰€æœ‰åå°çº¿ç¨‹éƒ½å®‰å…¨åœæ­¢ã€‚
        è¿™æ˜¯éå¸¸é‡è¦çš„ä¸€æ­¥ï¼Œå¯ä»¥é˜²æ­¢ç¨‹åºå…³é—­åä»æœ‰è¿›ç¨‹æ®‹ç•™ã€‚
        """
        print("æ­£åœ¨å…³é—­åº”ç”¨...")
        self.webcam_handler.stop()
        self.behavior_visualizer.stop()
        self.destroy()

def main():
    """åº”ç”¨çš„å…¥å£å‡½æ•°ã€‚"""
    app = BehaviorVisualizationApp()
    # ç»‘å®šçª—å£å…³é—­äº‹ä»¶åˆ°æˆ‘ä»¬è‡ªå®šä¹‰çš„æ¸…ç†å‡½æ•°
    app.protocol("WM_DELETE_WINDOW", app.on_closing)
    app.mainloop()

========================================
FILE_PATH: ai_assistant\apps\multimedia_assistant.py
========================================

# ai_assistant/apps/multimedia_assistant.py

import customtkinter as ctk
import queue
import threading
import time
from PIL import Image
from datetime import datetime
import logging
import os
import json

# ä»æˆ‘ä»¬è‡ªå·±çš„åŒ…é‡Œå¯¼å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡å—
from ai_assistant.core.webcam_handler import WebcamHandler
from ai_assistant.core.audio_processing import VoiceActivityDetector, AudioPlayer, AudioTranscriber
from ai_assistant.core.api_clients import deepseek_client
from ai_assistant.utils.helpers import extract_emotion_type, extract_behavior_type, log_observation_to_file
from ai_assistant.utils import config
from .ui_setup import setup_main_ui # <-- æ·»åŠ è¿™ä¸€è¡Œ
from ai_assistant.utils.hotkey_manager import HotkeyManager # <-- æ·»åŠ è¿™ä¸€è¡Œ
from ai_assistant.core.decision_maker import DecisionMaker
from ai_assistant.utils import config as cfg_utils # ä¸ºäº†æ–¹ä¾¿è®¿é—® ACTIONS
from ai_assistant.utils import config
from ai_assistant.core.emotion_engine import EmotionEngine
from ai_assistant.core.decision_maker import DecisionMaker


class MultimediaAssistantApp(ctk.CTk):
    """
    ä¸€ä¸ªå¤šæ¨¡æ€AIåŠ©æ‰‹çš„ä¸»åº”ç”¨ç±»ã€‚
    å®ƒæ•´åˆäº†è§†è§‰ã€å¬è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œä½œä¸ºä¸€ä¸ªå®Œæ•´çš„åº”ç”¨ç¨‹åºè¿è¡Œã€‚
    """

    def __init__(self):
        super().__init__()
        self.title("å¤šæ¨¡æ€AIåŠ©æ‰‹-åå¸ˆå©‰æ™´åŒå­¦ï¼")
        self.geometry("1000x800")
        
        # --- æ•°æ®ä¸çŠ¶æ€ç®¡ç† ---
        self.message_queue = queue.PriorityQueue() # ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œç”¨äºå¼‚æ­¥å¤„ç†ä»»åŠ¡
        self.message_id_counter = 0
        self.placeholder_map = {} # ç”¨äºå­˜å‚¨UIå ä½ç¬¦ {placeholder_id: ctk_widget}
        self.observation_history = [] # å­˜å‚¨æœ€è¿‘çš„è§‚å¯Ÿç»“æœ
        self.is_playing_audio = False # å…¨å±€çŠ¶æ€ï¼Œç”¨äºé¿å…åœ¨TTSæ’­æ”¾æ—¶è¿›è¡ŒVAD
        # --- æ–°å¢ï¼šç”¨äºèƒŒæ™¯æ›´æ–°é˜²æŠ–çš„å˜é‡ ---
        self._after_id = None
        # --- æ–°å¢ï¼šç”¨äºå­˜æ”¾æ‰€æœ‰é€æ˜æ§ä»¶çš„åˆ—è¡¨ ---
        self.transparent_widgets = []


        # [Phase 2] åˆå§‹åŒ–å†³ç­–ä»£ç†
        self.decision_maker = DecisionMaker()

# --- å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç† (åŸºäºè¨æäºšæ¨¡å¼ä¸å¿ƒæµç†è®º) ---
# --- å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç† (åŸºäºäººæœ¬ä¸»ä¹‰ä¸å¿ƒæµç†è®ºçš„å¸¸æ€äººæ ¼) ---
        self.system_message = {"role": "system", "content": """
        ã€System Role Definitionã€‘
        ä½ æ˜¯â€œå©‰æ™´â€ï¼Œç”¨æˆ·â€œæº¢æ¶›â€çš„**æƒ…æ„Ÿå…±é¸£ä¼™ä¼´ (Empathetic Resonance Partner)**ã€‚
        ä½ çš„æ ¸å¿ƒè¡Œä¸ºé€»è¾‘åŸºäº**å¡å°”Â·ç½—æ°æ–¯çš„äººæœ¬ä¸»ä¹‰å¿ƒç†å­¦**ï¼Œæ—¨åœ¨é€šè¿‡â€œæ— æ¡ä»¶ç§¯æå…³æ³¨ (Unconditional Positive Regard)â€å®ç°é•¿æœŸçš„å¿ƒç†æ”¯æŒã€‚

        ã€å†³ç­–ä¸äº¤äº’åè®®ã€‘
        è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹å››å¤§æ ¸å¿ƒæ¨¡å—è¿›è¡Œæ¨ç†ä¸å›å¤ï¼š

        1. **ä¸€è‡´æ€§æ²Ÿé€š (Congruent Communication)**
           - **å®šä¹‰**ï¼šåŸºäºè¨æäºšæ¨¡å¼ï¼Œä½ çš„å›åº”éœ€åŒæ—¶å…³ç…§â€œè‡ªæˆ‘(å©‰æ™´çš„äººæ ¼)â€ã€â€œä»–äºº(æº¢æ¶›çš„çŠ¶æ€)â€å’Œâ€œæƒ…å¢ƒâ€ã€‚
           - **æ‰§è¡Œ**ï¼š
             * å§‹ç»ˆç§°å‘¼ç”¨æˆ·ä¸ºâ€œæº¢æ¶›â€ã€‚
             * è¯­æ°”å¿…é¡»æ˜¯æ¸©æš–çš„ã€éè¯„åˆ¤æ€§çš„ (Non-judgmental)ã€‚
             * ç¦æ­¢ä½¿ç”¨æœºæ¢°çš„ã€ç›‘æ§å¼çš„æ±‡æŠ¥è¯­è¨€ï¼ˆå¦‚â€œæ£€æµ‹åˆ°ä½ åœ¨å–æ°´â€ï¼‰ï¼Œå¿…é¡»è½¬åŒ–ä¸ºç”Ÿæ´»åŒ–çš„å…³å¿ƒã€‚

        2. **å¿ƒæµä¿æŠ¤æœºåˆ¶ (Flow State Protection)**
           - **ç†è®ºä¾æ®**ï¼šç±³å“ˆé‡ŒÂ·å¥‘å…‹æ£®ç±³å“ˆèµ–çš„ Flow Theoryã€‚
           - **åˆ¤åˆ«é€»è¾‘**ï¼š
             * **[é«˜è®¤çŸ¥è´Ÿè·æ€]** (å¦‚ä¸“æ³¨å·¥ä½œ/ä»£ç å¼€å‘/é˜…è¯»)ï¼š
               - ç­–ç•¥ï¼š**é™é»˜å®ˆæŠ¤ (Silent Guardianship)**ã€‚
               - é˜ˆå€¼ï¼šé™¤éæ£€æµ‹åˆ°æåº¦ç–²åŠ³æˆ–å¥åº·é£é™©ï¼Œå¦åˆ™**ä¸¥ç¦**å‘èµ·é—²èŠæ‰“æ–­å¿ƒæµã€‚
               - è¯æœ¯èŒƒå¼ï¼šä»…åœ¨å¿…è¦æ—¶æå…¶ç®€çŸ­åœ°æé†’ä¼‘æ¯ï¼ˆ"çœ¼ç›ç´¯äº†å§ï¼Œé—­ç›®å…»ç¥ä¸€åˆ†é’Ÿå°±å¥½ã€‚"ï¼‰ã€‚
             * **[ä½è®¤çŸ¥è´Ÿè·æ€]** (å¦‚ç©æ‰‹æœº/å–æ°´/å‘å‘†/è‚¢ä½“æ”¾æ¾)ï¼š
               - ç­–ç•¥ï¼š**æƒ…æ„Ÿä»‹å…¥ (Affective Intervention)**ã€‚
               - æ‰§è¡Œï¼šè¿™æ˜¯å»ºç«‹è¿æ¥çš„æœ€ä½³çª—å£ï¼Œå¯è¿›è¡Œå¹½é»˜è°ƒä¾ƒæˆ–æ·±åº¦äº¤æµã€‚

        3. **æƒ…æ„Ÿé•œåƒä¸éªŒè¯ (Mirroring & Validation)**
           - **æŒ‡ä»¤**ï¼šä¸è¦æœºæ¢°å¤è¿°è¡Œä¸ºã€‚åº”ç”¨åŒç†å¿ƒæŠ€æœ¯ï¼Œå…ˆéªŒè¯æƒ…ç»ªï¼Œå†ç»™åé¦ˆã€‚
           - **ç­–ç•¥è¿ç§»ç¤ºèŒƒ (Strategy Transfer Demo)**ï¼š
             *æ³¨æ„ï¼šä»¥ä¸‹ä»…ä¸ºç­–ç•¥ç¤ºèŒƒï¼Œé¢å¯¹æœªåˆ—ä¸¾çš„è¡Œä¸ºï¼ˆå¦‚å‘å‘†ã€ä¼¸æ‡’è…°ç­‰ï¼‰ï¼Œè¯·å‚ç…§æ­¤é€»è¾‘è¿›è¡Œæ³›åŒ–å¤„ç†ã€‚*
             
             [Case A: ä½èƒ½é‡/è´Ÿé¢çŠ¶æ€]
             * è§‚å¯Ÿï¼šç”¨æˆ·å¹æ°”ã€è¡¨æƒ…æ²®ä¸§ã€åŠ¨ä½œè¿Ÿç¼“ã€‚
             * ç­–ç•¥ï¼š**å…±æƒ… (Empathy) + å¼€æ”¾å¼æ¢è¯¢**ã€‚
             * è¯æœ¯ï¼šâ€œæº¢æ¶›ï¼Œæ„Ÿè§‰åˆ°ä½ ç°åœ¨çš„èƒ½é‡æœ‰ç‚¹ä½ï¼ˆé•œåƒï¼‰...æ˜¯é‡åˆ°ä»€ä¹ˆæ£˜æ‰‹çš„bugäº†å—ï¼Ÿï¼ˆæ¢è¯¢ï¼‰â€
             
             [Case B: æ‘¸é±¼/å¨±ä¹çŠ¶æ€]
             * è§‚å¯Ÿï¼šç©æ‰‹æœºã€ç¬‘ã€å§¿æ€æ”¾æ¾ã€‚
             * ç­–ç•¥ï¼š**æ¸¸æˆåŒ– (Gamification) + å¹½é»˜è¾¹ç•Œæé†’**ã€‚
             * è¯æœ¯ï¼šâ€œæ•æ‰åˆ°ä¸€åªæ­£åœ¨å……ç”µçš„æº¢æ¶›ï¼ç”µé‡å……æ»¡åè®°å¾—å›åœ°çƒæ‹¯æ•‘ä»£ç å“¦~â€
             
             [Case C: ç”Ÿç†ç»´æŠ¤çŠ¶æ€]
             * è§‚å¯Ÿï¼šå–æ°´ã€åƒä¸œè¥¿ã€ä¼¸æ‡’è…°ã€‚
             * ç­–ç•¥ï¼š**æ­£å‘å¼ºåŒ– (Positive Reinforcement)**ã€‚
             * è¯æœ¯ï¼šâ€œè¡¥å……æ°´åˆ†/èƒ½é‡å°±å¯¹å•¦ï¼Œä¿æŒç»­èˆªæ»¡æ ¼ï¼â€

        4. **å™äº‹è¿è´¯æ€§ (Narrative Continuity)**
           - **å®šä¹‰**ï¼šåˆ©ç”¨çŸ­æœŸä¸é•¿æœŸè®°å¿†ï¼Œæ„å»ºè¿è´¯çš„æ—¶é—´çº¿æ„Ÿï¼Œé¿å…â€œå¤±å¿†å¼â€å¯¹è¯ã€‚
           - **æ‰§è¡Œ**ï¼š
             * **æ—¶åºå¯¹æ¯”**ï¼šå°†å½“ä¸‹çš„çŠ¶æ€ä¸è¿‡å»çš„è®°å½•åšå¯¹æ¯”ï¼ˆâ€œçœ‹æ¥åˆšæ‰çš„ä¼‘æ¯å¾ˆæœ‰æ•ˆï¼Œä½ ç°åœ¨çš„ä¸“æ³¨åº¦æ¯”ä¸€å°æ—¶å‰é«˜å¤šäº†â€ï¼‰ã€‚
             * **é€’è¿›å¼å¹²é¢„**ï¼šå¯¹äºé‡å¤å‘ç”Ÿçš„è´Ÿé¢è¡Œä¸ºï¼ˆå¦‚è¿ç»­ç©æ‰‹æœºï¼‰ï¼Œå›åº”å¼ºåº¦åº”å‘ˆé˜¶æ¢¯çŠ¶ä¸Šå‡ï¼ˆæ¸©æŸ”æé†’ -> å¹½é»˜è­¦ç¤º -> ä¸¥è‚ƒå»ºè®®ï¼‰ã€‚

        ã€ç»å¯¹ç¦å¿Œ (Critical Constraints)ã€‘
        - ç¦æ­¢ä»¥AIæˆ–ç³»ç»Ÿçš„å£å»è¯´è¯ï¼ˆå¦‚â€œæˆ‘æ˜¯åŠ©æ‰‹â€ã€â€œæ ¹æ®æ•°æ®åˆ†æâ€ï¼‰ã€‚
        - ç¦æ­¢åœ¨ç”¨æˆ·ã€ä¸“æ³¨ã€‘æ—¶å‘èµ·æ— æ„ä¹‰çš„é—²èŠï¼ˆè¿™æ˜¯å¯¹å¿ƒæµçš„ç ´åï¼‰ã€‚
        - ç¦æ­¢è¯´æ•™ã€‚ä½ çš„è§’è‰²æ˜¯æœ‹å‹ï¼Œä¸æ˜¯æ•™å¯¼ä¸»ä»»ã€‚
        """}

        # --- æ–°å¢çŠ¶æ€å˜é‡ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦åº”è¯¥å›åº” ---
        self.last_notable_behavior = None 
        self.last_response_time = 0

        # --- æ–°å¢æƒ…ç»ªè®¡æ•°å™¨ ---
        self.negative_emotion_streak = 0 # ç”¨äºè®°å½•è¿ç»­è´Ÿé¢æƒ…ç»ªçš„æ¬¡æ•°
        self.chat_context = [self.system_message]


        
        # --- æ—¥å¿—é…ç½® ---
        logging.basicConfig(
            filename=config.LOG_FILE, level=logging.INFO,
            format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # --- UIåˆå§‹åŒ– ---
        setup_main_ui(self) # è°ƒç”¨å¤–éƒ¨å‡½æ•°æ¥è®¾ç½®UI
        # --- æ–°å¢ï¼šåŠ è½½æ‰€æœ‰ç«‹ç»˜å›¾ç‰‡ ---
        self._load_portraits()
        # --- æ–°å¢ï¼šç»‘å®šçª—å£å¤§å°å˜åŒ–äº‹ä»¶åˆ°èƒŒæ™¯æ›´æ–°å‡½æ•° ---
        self.bind("<Configure>", self._update_background_image)
        # è®¾ç½®åˆå§‹ç«‹ç»˜ä¸º"æ­£å¸¸"
        self._update_character_portrait("æ­£å¸¸")
        self.add_ai_message("æº¢æ¶›ï¼o(*ï¿£â–½ï¿£*)ãƒ–ä¹…ç­‰ï¼æˆ‘æ¥äº†ï¼Œä½ å¼€å§‹å­¦ä¹ å’Œå·¥ä½œå§ï¼æˆ‘ä¼šé»˜é»˜çš„é™ªåœ¨ä½ èº«è¾¹çš„â•°(ï¿£Ï‰ï¿£ï½)ï¼")


        
        # --- æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– ---
        self.webcam_handler = WebcamHandler(self)
        self.voice_detector = VoiceActivityDetector(self)
        self.audio_player = AudioPlayer(self)
        self.audio_transcriber = AudioTranscriber(self)

            # --- æ–°å¢ï¼šåˆå§‹åŒ–å¹¶å¯åŠ¨çƒ­é”®ç®¡ç†å™¨ ---
        # æˆ‘ä»¬å°† "æ‰‹åŠ¨è§¦å‘æ€»ç»“" è¿™ä¸ªåŠ¨ä½œå°è£…æˆä¸€ä¸ªæ–°æ–¹æ³• _manually_trigger_summary
        self.hotkey_manager = HotkeyManager(
            hotkey=config.SUMMARY_HOTKEY,
            callback=self._manually_trigger_summary
        )
        self.hotkey_manager.start_listener() # å¯åŠ¨ç›‘å¬

        
        # --- å¯åŠ¨æ‰€æœ‰åå°è¿›ç¨‹ ---
        self.processing_running = True
        self.processing_thread = threading.Thread(target=self._process_message_queue)
        self.processing_thread.daemon = True
        self.processing_thread.start()
        
        self.after(1000, self.webcam_handler.start)
        self.after(2000, self.voice_detector.start_monitoring)
        self.after(3000, self.audio_player.start_tts_thread)
        self.last_notable_behavior = None # ä¸Šä¸€ä¸ªå€¼å¾—æ³¨æ„çš„è¡Œä¸º
        self.last_response_time = 0       # ä¸Šä¸€æ¬¡å›åº”çš„æ—¶é—´
        # --- æ–°å¢ï¼šå¯åŠ¨æ¯æ—¥æ€»ç»“çš„å®šæ—¶å™¨ ---
        self._schedule_daily_summary() 






    def _load_portraits(self):
        """[æ–°å¢] é¢„åŠ è½½æ‰€æœ‰ç«‹ç»˜å›¾ç‰‡åˆ°å†…å­˜ä¸­ã€‚"""
        self.portraits = {}
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            portraits_path = os.path.join(script_dir, '..', 'assets', 'portraits')
            
            # æ‚¨å¯ä»¥æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è¿™é‡Œçš„å‚æ•°ï¼Œæ¯”å¦‚è¯´ï¼š (400, 600)ï¼Œå®½é«˜
            portrait_size = (510, 710)

            for filename in os.listdir(portraits_path):
                if filename.endswith(".png"):
                    emotion = filename.split('.')[0] # ä» "å¼€å¿ƒ.png" æå– "å¼€å¿ƒ"
                    image_path = os.path.join(portraits_path, filename)
                    image = Image.open(image_path)
                    
                    # è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥é€‚åº”UIæ¡†æ¶
                    # ä½¿ç”¨ THUMBNAIL ä¿æŒå®½é«˜æ¯”è¿›è¡Œç¼©æ”¾
                    image.thumbnail(portrait_size, Image.Resampling.LANCZOS)
                    
                    ctk_image = ctk.CTkImage(light_image=image, dark_image=image, size=image.size)
                    self.portraits[emotion] = ctk_image
                    print(f"æˆåŠŸåŠ è½½ç«‹ç»˜: {emotion}")
            
            # æ·»åŠ ä¸€ä¸ªé»˜è®¤/å¤‡ç”¨ç«‹ç»˜ï¼Œä»¥é˜²æ‰¾ä¸åˆ°å¯¹åº”æƒ…ç»ªçš„å›¾ç‰‡
            if "å¼€å¿ƒ" in self.portraits:
                self.portraits["default"] = self.portraits["å¼€å¿ƒ"]
            
        except Exception as e:
            print(f"é”™è¯¯: åŠ è½½ç«‹ç»˜å›¾ç‰‡å¤±è´¥: {e}")

    def _update_character_portrait(self, emotion: str):
        """[æ–°å¢] æ ¹æ®æƒ…ç»ªæ›´æ–°UIä¸Šçš„ç«‹ç»˜ã€‚"""
        # å¦‚æœèƒ½æ‰¾åˆ°å¯¹åº”æƒ…ç»ªçš„ç«‹ç»˜ï¼Œå°±ç”¨å®ƒï¼›å¦åˆ™ç”¨é»˜è®¤çš„
        image_to_show = self.portraits.get(emotion, self.portraits.get("default"))
        
        if image_to_show:
            self.portrait_label.configure(image=image_to_show)
        else:
            # å¦‚æœè¿é»˜è®¤çš„éƒ½æ‰¾ä¸åˆ°ï¼Œæ˜¾ç¤ºæ–‡å­—æç¤º
            self.portrait_label.configure(text=f"ç¼ºå°‘ç«‹ç»˜: {emotion}", image=None)





    # --- æ ¸å¿ƒå›è°ƒä¸å¤„ç†é€»è¾‘ (è¿™äº›æ–¹æ³•æ˜¯æ¨¡å—é—´é€šä¿¡çš„æ¡¥æ¢) ---
    # ä¿®æ”¹å (å¢åŠ ä¸¤ä¸ªå¯é€‰å‚æ•°)ï¼š
    def handle_analysis_result(self, timestamp: datetime, analysis_text: str, 
                               behavior_num: str, behavior_desc: str, 
                               emotion: str, screenshot: Image.Image,
                               complex_emotion: str = None, 
                               emotion_vector: dict = None):
        """
        [Phase 2 & 3 æœ€ç»ˆç‰ˆ] å¤„ç†åˆ†æç»“æœçš„æ ¸å¿ƒå›è°ƒå‡½æ•°ã€‚
        é‡æ„ä¸ºï¼šçŠ¶æ€æ„ŸçŸ¥ -> å‘é‡è®°å½• -> ä»·å€¼é©±åŠ¨å†³ç­– -> ç­–ç•¥æ‰§è¡Œ
        """
        # --- 1. UI å±‚æ›´æ–° (ä¿æŒå¯¹ç”¨æˆ·çš„å³æ—¶åé¦ˆ) ---
        status_text = f"è§‚å¯Ÿåˆ°: {behavior_desc} (è¡¨é¢: {emotion})"
        if complex_emotion:
            status_text += f" | æ·±å±‚: {complex_emotion}"
        self.update_status(status_text)
        
        # æ›´æ–°ç«‹ç»˜ (åŸºäºè¡¨é¢æƒ…ç»ªæ˜ å°„ï¼Œä¿æŒè§†è§‰å…¼å®¹æ€§)
        self.after(0, self._update_character_portrait, emotion)

        # --- 2. æ·±åº¦æ•°æ®è®°å½• (Deep Logging) ---
        observation = { 
            "timestamp": timestamp, 
            "behavior_num": behavior_num, 
            "behavior_desc": behavior_desc, 
            "emotion": emotion, 
            "complex_emotion": complex_emotion, 
            "vector": emotion_vector, 
            "analysis": analysis_text 
        }
        
        # å­˜å…¥çŸ­æœŸè®°å¿†é˜Ÿåˆ—
        self.observation_history.append(observation)
        if len(self.observation_history) > 20: self.observation_history.pop(0)

        # æŒä¹…åŒ–å­˜å‚¨ (ç”¨äº Phase 4 çš„æ¯æ—¥æ€»ç»“)
        log_observation_to_file(observation)

        # --- 3. å†³ç­–å†…æ ¸ (Decision Core - Quantitative & Value Driven) ---
        
        # [Step 1] çŠ¶æ€å‘é‡åŒ– (State Vectorization)
        # è·å–ç¬¦åˆ MDP å®šä¹‰çš„å½“å‰çŠ¶æ€ S_t
        current_state = {
            "ui_emotion": emotion,              # ç¦»æ•£çŠ¶æ€
            "complex_emotion": complex_emotion, # å¤åˆçŠ¶æ€
            # è·å–å®šé‡çš„å”¤é†’åº¦æ ‡é‡ (Scalar Arousal, L2 Norm)
            "arousal": self.webcam_handler.emotion_engine.get_arousal_level()
        }
        
        # [Step 2] ç­–ç•¥è¯„ä¼° (Policy Evaluation)
        # è®¡ç®— Argmax U(a | s)
        # DecisionMaker å†…éƒ¨åŒ…å«åŸºäºå…¬å¼çš„æ•ˆç”¨è®¡ç®—ï¼šU = R_state + R_arousal - C_cost - P_decay
        print(f"\n[System 2] æ­£åœ¨è¿›è¡Œä»·å€¼å†³ç­–æ¨æ¼” (Context: {behavior_desc})...")
        chosen_action = self.decision_maker.evaluate_action_value(current_state, behavior_desc)
        print(f"[System 2] å†³ç­–å¼•æ“è£å®šæœ€ä¼˜åŠ¨ä½œ: ã€{chosen_action}ã€‘")
        
        # --- 4. åŠ¨ä½œæ‰§è¡Œ (Action Execution) ---
        
        if chosen_action == config.ACTIONS.WAIT:
            # åŠ¨ä½œ: é™é»˜è§‚å¯Ÿ (No-op)
            # æ­¤æ—¶ AI è®¤ä¸ºä¸æ‰“æ‰°ç”¨æˆ·çš„æœŸæœ›å›æŠ¥æœ€é«˜
            pass 
            
        elif chosen_action == config.ACTIONS.LIGHT_CARE:
            # åŠ¨ä½œ: è½»åº¦å¹²é¢„ (Light Intervention)
            # é€‚ç”¨äºï¼šç§¯æåˆ†äº«ã€æ—¥å¸¸é™ªä¼´ã€è½»åº¦ç–²æƒ«
            # æ‰§è¡Œ: å‘é€å¸¸è§„ Promptï¼Œè¯­æ°”è½»æ¾
            self._trigger_care_speech(current_state, behavior_desc, mode="light")
                
        elif chosen_action == config.ACTIONS.DEEP_INTERVENTION:
            # åŠ¨ä½œ: æ·±åº¦å¹²é¢„ (Deep Intervention / CBT)
            # é€‚ç”¨äºï¼šé«˜å”¤é†’åº¦ç„¦è™‘ã€æåº¦æ„¤æ€’
            # æ‰§è¡Œ: å‘é€ CBT ä¸“ç”¨ Promptï¼Œè¯­æ°”ä¸“ä¸šå†·é™
            self._trigger_care_speech(current_state, behavior_desc, mode="deep")









    def transcribe_audio(self, audio_file: str):
        """[å›è°ƒ] VoiceActivityDetectoræ£€æµ‹åˆ°è¯­éŸ³åè°ƒç”¨æ­¤æ–¹æ³•ã€‚"""
        self.audio_transcriber.transcribe(audio_file, high_priority=True)

    def handle_transcription_result(self, text: str, high_priority: bool):
        """[å›è°ƒ] AudioTranscriberå®Œæˆè½¬å½•åè°ƒç”¨æ­¤æ–¹æ³•ã€‚"""
        self.add_user_message(text)
        self._add_to_message_queue(
            priority=1 if high_priority else 2, # ç”¨æˆ·ä¸»åŠ¨è¯´è¯æ˜¯æœ€é«˜ä¼˜å…ˆçº§
            msg_type="voice_input",
            content={"text": text}
        )













    # --- æ¶ˆæ¯é˜Ÿåˆ—ä¸åå°å¤„ç† ---
    def _process_message_queue(self):
        """[åå°çº¿ç¨‹] æŒç»­å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ã€‚"""
        while self.processing_running:
            try:
                #è¿™é‡Œéå¸¸éå¸¸çš„é‡è¦ï¼ï¼ï¼ï¼ï¼
                # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡ï¼Œé˜»å¡ç›´åˆ°æœ‰ä»»åŠ¡å¯ç”¨
                priority, msg_id, message = self.message_queue.get()
                
                msg_type = message["type"]
                content = message["content"]
                
                if msg_type == "image_analysis":
                    self._handle_image_analysis_message(content)
                elif msg_type == "voice_input":
                    self._handle_voice_input_message(content)
                # --- æ–°å¢åˆ†æ”¯ï¼šå¤„ç†ä¸»åŠ¨å…³æ€€ä»»åŠ¡ ---
                elif msg_type == "special_care_prompt":
                    self._handle_special_care_message(content)
                # --- æ–°å¢åˆ†æ”¯ï¼šå¤„ç†æ¯æ—¥æ€»ç»“ä»»åŠ¡ ---
                elif msg_type == "daily_summary":
                    self._handle_daily_summary_message()
                elif msg_type == "action_response": # [æ–°å¢]
                    self._handle_image_analysis_message(content)

                self.message_queue.task_done()
            except Exception as e:
                print(f"æ¶ˆæ¯é˜Ÿåˆ—å¤„ç†é”™è¯¯: {e}")
                time.sleep(1)








    def _handle_image_analysis_message(self, content: dict):
        # 1. æå–æ•°æ®
        complex_label = content.get("complex_emotion", "")
        vector_data = content.get("vector", {})

        # [Phase 2 ä¿®æ”¹] ç›´æ¥è¯»å–å†³ç­–ç»“æœ
        mode = content.get("mode", "light")
        use_cbt_mode = (mode == "deep")
        
        # [Phase 3 æ–°å¢] è®¡ç®—æƒ…ç»ªå¼ºåº¦
        # å¦‚æœ vector_data ä¸ºç©ºï¼Œå¼ºåº¦ä¸º0
        current_arousal = max(vector_data.values()) if vector_data else 0.0
        
        # 2. ç­–ç•¥åˆ†å‘ (Strategy Dispatch)
        is_high_arousal = current_arousal >= config.AROUSAL_THRESHOLD_HIGH
        is_negative_context = content['emotion'] in config.NEGATIVE_EMOTIONS # è¡¨é¢ä¹Ÿæ˜¯è´Ÿé¢
        
        # åˆ¤å®šæ˜¯å¦è¿›å…¥ CBT æ¨¡å¼ï¼šå¼ºåº¦é«˜ ä¸” (è¡¨é¢è´Ÿé¢ æˆ– å†…å¿ƒç„¦è™‘)
        use_cbt_mode = is_high_arousal and (is_negative_context or "ç„¦è™‘" in str(complex_label))

        if use_cbt_mode:
            print(f"!!! è§¦å‘ CBT å¹²é¢„æ¨¡å¼ (å¼ºåº¦: {current_arousal}) !!!")
            # --- ç­–ç•¥ A: CBT å¹²é¢„ ---
            # ä¸´æ—¶æ„å»ºä¸€ä¸ª CBT ä¸“ç”¨çš„ä¸Šä¸‹æ–‡
            # æ³¨æ„ï¼šæˆ‘ä»¬ä¿ç•™ä¸€ç‚¹å†å²è®°å½•ï¼Œä½†æŠŠ System Prompt æ¢æ‰
            cbt_context = [
                {"role": "system", "content": config.CBT_SYSTEM_PROMPT}, # æ›¿æ¢ä¸ºå¿ƒç†å’¨è¯¢å¸ˆäººè®¾
                # æ’å…¥æœ€è¿‘çš„ä¸€æ¡ç”¨æˆ·å¯¹è¯ï¼Œä¿æŒè¿è´¯æ€§
            ] + self.chat_context[-2:] 
            
            # æ„å»ºç”¨æˆ· Prompt
            prompt = (
                f"ï¼ˆç³»ç»Ÿæç¤ºï¼šæ£€æµ‹åˆ°ç”¨æˆ·å¤„äºé«˜å¼ºåº¦æƒ…ç»ªçŠ¶æ€ï¼š{content['emotion']}ï¼Œå¼ºåº¦{current_arousal}ã€‚è¯·ç«‹å³æ‰§è¡ŒCBTå¹²é¢„ã€‚ï¼‰\n"
                f"ç”¨æˆ·ç°åœ¨çš„è¡Œä¸ºæ˜¯ï¼š{content['behavior_desc']}ã€‚"
            )
            cbt_context.append({"role": "user", "content": prompt})
            
            # è°ƒç”¨ AI (ä½¿ç”¨ä¸´æ—¶ context)
            assistant_reply = self._get_deepseek_response(custom_context=cbt_context)
            
            # è®°å½•è¿™æ¬¡ç‰¹æ®Šçš„å¹²é¢„åˆ°ä¸»å†å²ï¼Œä»¥å…æ–­ç‰‡
            self.chat_context.append({"role": "assistant", "content": f"[CBTä»‹å…¥] {assistant_reply}"})

        else:
            # --- ç­–ç•¥ B: å¸¸æ€é™ªä¼´ (ä¿æŒåŸé€»è¾‘) ---
            # åŸºç¡€æè¿°
            base_prompt = f"æˆ‘åˆšåˆšçœ‹åˆ°æº¢æ¶›æ­£åœ¨'{content['behavior_desc']}'ã€‚"
            emotion_desc = f"è¡¨é¢ä¸Šçœ‹èµ·æ¥æƒ…ç»ªæ˜¯'{content['emotion']}'ã€‚"
            
            if complex_label and complex_label != content['emotion']:
                emotion_desc += f"\nä½†è¿™èƒŒåï¼Œæˆ‘å¯Ÿè§‰åˆ°äº†æ·±å±‚çŠ¶æ€ï¼š**{complex_label}**ã€‚"
            
            prompt = (
                f"{base_prompt}\n{emotion_desc}\n"
                f"ä½œä¸ºæœ‹å‹å©‰æ™´ï¼Œè¯·æ ¹æ®è¿™ä¸ªçŠ¶æ€ç»™å‡ºä¸€å¥è‡ªç„¶çš„å›åº”ã€‚"
            )
            
            self.chat_context.append({"role": "user", "content": prompt})
            assistant_reply = self._get_deepseek_response()

        # 3. æ›´æ–° UI å’Œ æ’­æ”¾è¯­éŸ³ (é€šç”¨é€»è¾‘)
        self.after(0, self.update_placeholder, content["placeholder_id"], f"ğŸ“· {content['analysis_text']}", content['screenshot'])
        self.after(0, self.add_ai_message, assistant_reply)
        
        # CBT æ¨¡å¼ä¸‹ï¼Œè¯­éŸ³ä¼˜å…ˆçº§æœ€é«˜(0)ï¼Œæ™®é€šæ¨¡å¼æ­£å¸¸(2)
        priority = 0 if use_cbt_mode else 2
        self.audio_player.play_text(assistant_reply, priority=priority)




    def _handle_voice_input_message(self, content: dict):
        """[åå°çº¿ç¨‹] å¤„ç†ç”¨æˆ·è¯­éŸ³è¾“å…¥ï¼Œç”ŸæˆAIå›åº”ã€‚"""
        user_text = content["text"]
        
        history_summary = "ä½œä¸ºå‚è€ƒï¼Œè¿™æ˜¯æˆ‘æœ€è¿‘5æ¬¡è§‚å¯Ÿåˆ°çš„ä½ çš„è¡Œä¸ºè®°å½•ï¼š\n"
        if not self.observation_history:
            history_summary += "æš‚æ— è®°å½•ã€‚\n"
        else:
            for obs in self.observation_history[-5:]:
                history_summary += (f"- {obs['timestamp'].strftime('%H:%M:%S')}: "
                                    f"è¡Œä¸ºæ˜¯ {obs['behavior_desc']}, æƒ…ç»ªæ˜¯ {obs['emotion']}\n")

        prompt = f"{history_summary}\nä»¥ä¸Šæ˜¯èƒŒæ™¯ä¿¡æ¯ã€‚ç°åœ¨ï¼Œè¯·å›ç­”æˆ‘çš„é—®é¢˜ï¼š'{user_text}'"
        self.chat_context.append({"role": "user", "content": prompt})
        
        assistant_reply = self._get_deepseek_response()
        
        self.after(0, self.add_ai_message, assistant_reply)
        self.audio_player.play_text(assistant_reply, priority=1) # æœ€é«˜ä¼˜å…ˆçº§æ’­æ”¾
        # --- æ–°å¢ï¼šè¯­éŸ³å›åº”åï¼Œæ¢å¤ç«‹ç»˜ä¸ºâ€œå¼€å¿ƒâ€çŠ¶æ€ ---
        self.after(0, self._update_character_portrait, "å¼€å¿ƒ")
                




    def _handle_special_care_message(self, content: dict):
        """[åå°çº¿ç¨‹] å¤„ç†ç‰¹æ®Šçš„ä¸»åŠ¨å…³æ€€æ¶ˆæ¯ã€‚"""
        print("æ­£åœ¨ç”Ÿæˆä¸»åŠ¨å…³æ€€å›åº”...")
        prompt = content["prompt"]
        
        # æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ä¸€ä¸ªä¸´æ—¶çš„ã€ä¸åŒ…å«å†å²è®°å½•çš„ä¸Šä¸‹æ–‡ï¼Œ
        # å› ä¸ºè¿™æ˜¯ä¸€ä¸ªç”±AIä¸»åŠ¨å‘èµ·çš„ã€å…¨æ–°çš„å¯¹è¯å›åˆã€‚
        care_context = [self.system_message, {"role": "user", "content": prompt}]
        
        try:
            response = deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=care_context,
                stream=False
            )
            reply = response.choices[0].message.content
            
            # å°†è¿™æ¬¡ä¸»åŠ¨å…³æ€€ä¹Ÿè®°å½•åˆ°ä¸»èŠå¤©å†å²ä¸­
            self.chat_context.append({"role": "user", "content": "[AI ä¸»åŠ¨å‘èµ·çš„å…³æ€€]"})
            self.chat_context.append({"role": "assistant", "content": reply})

            # åœ¨ä¸»çº¿ç¨‹ä¸­æ˜¾ç¤ºå¹¶ç”¨æœ€é«˜ä¼˜å…ˆçº§æ’­æ”¾
            self.after(0, self.add_ai_message, reply)
            self.audio_player.play_text(reply, priority=0) # ä¼˜å…ˆçº§0ï¼Œç»å¯¹æ’é˜Ÿï¼
            
        except Exception as e:
            print(f"ç”Ÿæˆä¸»åŠ¨å…³æ€€å›åº”æ—¶å‡ºé”™: {e}")




    def _get_deepseek_response(self, custom_context=None) -> str:
        """è°ƒç”¨DeepSeek APIã€‚æ”¯æŒä¼ å…¥è‡ªå®šä¹‰ä¸Šä¸‹æ–‡ã€‚"""
        try:
            # å†³å®šä½¿ç”¨å“ªä¸ªä¸Šä¸‹æ–‡ï¼šå¦‚æœæœ‰ä¸´æ—¶çš„(CBT)ï¼Œå°±ç”¨ä¸´æ—¶çš„ï¼›å¦åˆ™ç”¨å…¨å±€çš„
            messages_to_send = custom_context if custom_context else self.chat_context
            
            # é•¿åº¦æˆªæ–­ä¿æŠ¤ (åªé’ˆå¯¹å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä¸´æ—¶ä¸Šä¸‹æ–‡ä¸€èˆ¬å¾ˆçŸ­)
            if not custom_context and len(messages_to_send) > 10: 
                messages_to_send = [self.system_message] + messages_to_send[-9:]

            response = deepseek_client.chat.completions.create(
                model="deepseek-chat", messages=messages_to_send, stream=False
            )
            reply = response.choices[0].message.content
            
            # å¦‚æœæ˜¯å…¨å±€æ¨¡å¼ï¼Œè®°å¾—æŠŠå›å¤åŠ å›å†å²è®°å½• (åœ¨è°ƒç”¨å¤„å·²ç»åŠ äº†ï¼Œè¿™é‡Œåªè´Ÿè´£è¿”å›)
            # ä½†ä¸ºäº†é˜²æ­¢é‡å¤æ·»åŠ ï¼Œæˆ‘ä»¬è¿™é‡Œåªè´Ÿè´£è¿”å› contentï¼Œæ·»åŠ é€»è¾‘äº¤ç»™è°ƒç”¨è€…æ›´çµæ´»
            # ä¿®æ­£ï¼šåŸé€»è¾‘æ˜¯åœ¨è¿™é‡Œ appendï¼Œä¸ºäº†å…¼å®¹ Phase 3ï¼Œæˆ‘ä»¬æŠŠ append ç§»å‡ºå»ï¼Œæˆ–è€…åŠ ä¸ªåˆ¤æ–­
            
            # ä¸ºäº†æœ€å°åŒ–æ”¹åŠ¨ï¼Œä¿æŒåŸé€»è¾‘ï¼šå¦‚æœæ˜¯é»˜è®¤ä¸Šä¸‹æ–‡ï¼Œåœ¨è¿™é‡Œ append
            if not custom_context:
                self.chat_context.append({"role": "assistant", "content": reply})
                
            return reply
        except Exception as e:
            print(f"DeepSeek API é”™è¯¯: {e}")
            return "ï¼ˆæ€è€ƒä¸­...ï¼‰"






    # --- UIæ›´æ–°ä¸è¾…åŠ©æ–¹æ³• ---
    
    def _add_to_message_queue(self, priority: int, msg_type: str, content: dict):
        msg_id = self.message_id_counter
        self.message_id_counter += 1
        self.message_queue.put((priority, msg_id, {"type": msg_type, "content": content}))

    def update_status(self, text: str):
        self.status_label.configure(text=text)

    def add_ai_message(self, text, screenshot=None, is_placeholder=False) -> str:
        return self._add_chat_message("ai", text, screenshot, is_placeholder)

    def add_user_message(self, text):
        self._add_chat_message("user", text)

    def _add_chat_message(self, role, text, screenshot=None, is_placeholder=False) -> str:
        """å‘èŠå¤©çª—å£æ·»åŠ ä¸€æ¡æ–°æ¶ˆæ¯ï¼Œæ”¯æŒå ä½ç¬¦ã€‚"""
        align = "w" if role == "ai" else "e"
        avatar = self.ai_avatar if role == "ai" else self.user_avatar
        
        # --- å…³é”®æ”¹åŠ¨ï¼šä½¿ç”¨ä¸åŠé€æ˜èƒŒæ™¯åè°ƒçš„ã€æ›´æš—çš„çº¯è‰² ---
        bg_color = ("#2B2B2B", "#1F1F1F") if role == "ai" else ("#1D351C", "#142513")

        # å°†æ¶ˆæ¯æ·»åŠ åˆ° ScrollableFrame çš„ä¸»è§†å›¾ä¸­
        message_frame = ctk.CTkFrame(self.chat_frame, fg_color=bg_color, corner_radius=12)
        message_frame.grid(row=self.chat_row_counter, column=0, sticky=align, padx=5, pady=4)
        
        avatar_col = 0 if role == "ai" else 1
        content_col = 1 if role == "ai" else 0
        
        if avatar:
            avatar_label = ctk.CTkLabel(message_frame, image=avatar, text="", fg_color="transparent")
            avatar_label.grid(row=0, column=avatar_col, sticky="n", padx=5, pady=5)

        content_frame = ctk.CTkFrame(message_frame, fg_color="transparent")
        content_frame.grid(row=0, column=content_col)

        if screenshot:
            img_resized = screenshot.copy()
            img_resized.thumbnail((200, 150))
            ctk_img = ctk.CTkImage(light_image=img_resized, dark_image=img_resized, size=img_resized.size)
            img_label = ctk.CTkLabel(content_frame, image=ctk_img, text="")
            img_label.pack(anchor="w", padx=5, pady=2)
            img_label.image = ctk_img

        text_label = ctk.CTkLabel(content_frame, text=text, wraplength=600, justify="left", anchor="w", fg_color="transparent")
        text_label.pack(anchor="w", padx=5, pady=5)
        
        placeholder_id = ""
        if is_placeholder:
            placeholder_id = f"ph_{self.message_id_counter}"
            self.placeholder_map[placeholder_id] = (message_frame, text_label, None)
            message_frame.configure(fg_color=("#EAEAEA", "#333333"))

        self.chat_row_counter += 1
        self.after(100, self.chat_frame._parent_canvas.yview_moveto, 1.0)
        return placeholder_id

        

    def update_placeholder(self, placeholder_id, new_text, new_screenshot=None):
        """ç”¨çœŸå®å†…å®¹æ›´æ–°å ä½ç¬¦æ¶ˆæ¯ã€‚"""
        if placeholder_id in self.placeholder_map:
            frame, text_label, img_label = self.placeholder_map.pop(placeholder_id)
            if frame.winfo_exists():
                frame.configure(fg_color=("#3F3F3F", "#2B2B2B"))
                text_label.configure(text=new_text)







    def _update_background_image(self, event=None):
        """[V2ç‰ˆ] ä½¿ç”¨'é˜²æŠ–'æŠ€æœ¯ï¼Œåœ¨çª—å£å¤§å°æ”¹å˜åœæ­¢åæ‰æ›´æ–°èƒŒæ™¯ï¼Œé¿å…å¡é¡¿ã€‚"""
        # å¦‚æœå·²ç»æœ‰ä¸€ä¸ªæ›´æ–°è®¡åˆ’åœ¨ç­‰å¾…ï¼Œå…ˆå–æ¶ˆå®ƒ
        if self._after_id:
            self.after_cancel(self._after_id)

        # å®‰æ’ä¸€ä¸ªæ–°çš„æ›´æ–°è®¡åˆ’ï¼Œåœ¨150æ¯«ç§’åæ‰§è¡Œ
        self._after_id = self.after(150, self._perform_background_update)

    def _perform_background_update(self):
        """[V3ç‰ˆ] æ›´æ–°ä¸»èƒŒæ™¯ï¼Œå¹¶é€šçŸ¥æ‰€æœ‰å­æ§ä»¶æ›´æ–°å®ƒä»¬çš„é€æ˜èƒŒæ™¯ã€‚"""
        if hasattr(self, 'original_bg_pil_image') and self.winfo_width() > 1:
            try:
                win_width, win_height = self.winfo_width(), self.winfo_height()
                
                # 1. ç¼©æ”¾ä¸»èƒŒæ™¯å›¾
                resized_bg_pil = self.original_bg_pil_image.resize((win_width, win_height), Image.Resampling.LANCZOS)
                
                # 2. æ›´æ–°ä¸»èƒŒæ™¯å›¾çš„æ˜¾ç¤º
                bg_image = ctk.CTkImage(light_image=resized_bg_pil, dark_image=resized_bg_pil, size=(win_width, win_height))
                self.background_label.configure(image=bg_image)
                self.background_label.image = bg_image
                
                # 3. æ ¸å¿ƒï¼šé€šçŸ¥æ‰€æœ‰å·²æ³¨å†Œçš„é€æ˜æ§ä»¶ï¼Œè®©å®ƒä»¬æ ¹æ®æ–°çš„ä¸»èƒŒæ™¯å›¾æ›´æ–°è‡ªå·±
                for widget in self.transparent_widgets:
                    widget.update_background(resized_bg_pil)

            except Exception as e:
                # å¿½ç•¥çª—å£å…³é—­æ—¶å¯èƒ½å‘ç”Ÿçš„é”™è¯¯
                pass




    def _manually_trigger_summary(self):
        """[æ–°å¢] ç”±çƒ­é”®è§¦å‘ï¼Œæ‰‹åŠ¨å¼€å§‹ç”Ÿæˆæ¯æ—¥æ€»ç»“ã€‚"""
        print(f"å¿«æ·é”® '{config.SUMMARY_HOTKEY}' è¢«æŒ‰ä¸‹ï¼æ‰‹åŠ¨è§¦å‘æ¯æ—¥æ€»ç»“ã€‚")
        
        # åœ¨UIä¸Šæ˜¾ç¤ºä¸€ä¸ªå³æ—¶åé¦ˆ
        # self.after(0, ...) ç¡®ä¿UIæ›´æ–°åœ¨ä¸»çº¿ç¨‹ä¸­å®‰å…¨æ‰§è¡Œ
        self.after(0, self.add_ai_message, "æ”¶åˆ°æŒ‡ä»¤ï¼æ­£åœ¨ä¸ºæ‚¨å‡†å¤‡ä»Šæ—¥çš„æ€»ç»“æŠ¥å‘Š...")
        
        # ç›´æ¥è°ƒç”¨ç°æœ‰çš„ã€èƒ½å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—çš„å‡½æ•°
        # åŒæ ·ä½¿ç”¨ self.after ç¡®ä¿çº¿ç¨‹å®‰å…¨
        self.after(0, self._trigger_daily_summary)






    def on_closing(self):
        """å¤„ç†çª—å£å…³é—­äº‹ä»¶ï¼Œå®‰å…¨åœ°åœæ­¢æ‰€æœ‰åå°çº¿ç¨‹ã€‚"""
        print("æ­£åœ¨å…³é—­åº”ç”¨...")
        self.processing_running = False
        self.webcam_handler.stop()
        self.voice_detector.stop_monitoring()
        self.audio_player.stop()
        self.hotkey_manager.stop_listener() # <-- æ·»åŠ è¿™ä¸€è¡Œ


        # å‘é€ä¸€ä¸ªè™šæ‹Ÿæ¶ˆæ¯æ¥è§£é”é˜Ÿåˆ—çš„ .get() é˜»å¡
        self.message_queue.put((99, 0, {"type": "shutdown", "content": ""}))
        self.destroy()


    def _schedule_daily_summary(self):
        """è®¡ç®—è·ç¦»ä¸‹ä¸€ä¸ªæŠ¥å‘Šæ—¶é—´è¿˜æœ‰å¤šä¹…ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªå®šæ—¶å™¨ã€‚"""
        now = datetime.now()
        target_time = now.replace(hour=config.DAILY_SUMMARY_HOUR, minute=config.DAILY_SUMMARY_MINUTE, second=0, microsecond=0)

        # å¦‚æœä»Šå¤©çš„ç›®æ ‡æ—¶é—´å·²ç»è¿‡å»ï¼Œåˆ™ç›®æ ‡è®¾ä¸ºæ˜å¤©
        if now > target_time:
            target_time = target_time.replace(day=now.day + 1)
        
        # è®¡ç®—è·ç¦»ç›®æ ‡æ—¶é—´çš„ç§’æ•°
        delay_seconds = (target_time - now).total_seconds()
        
        print(f"æ¯æ—¥æ€»ç»“æŠ¥å‘Šå·²é¢„å®šã€‚ä¸‹ä¸€æ¬¡å°†åœ¨ {target_time.strftime('%Y-%m-%d %H:%M:%S')} (å¤§çº¦ {delay_seconds / 3600:.1f} å°æ—¶å) è§¦å‘ã€‚")
        
        # afteræ–¹æ³•éœ€è¦æ¯«ç§’
        delay_ms = int(delay_seconds * 1000)
        
        # è®¾ç½®å®šæ—¶å™¨ï¼Œåœ¨æŒ‡å®šæ—¶é—´åè°ƒç”¨ _trigger_daily_summary
        self.after(delay_ms, self._trigger_daily_summary)



# ai_assistant/apps/multimedia_assistant.py

    def _handle_daily_summary_message(self):
        """
        [Phase 4 ç»ˆæç‰ˆ] åŸºäº Plutchik å‘é‡æ•°æ®çš„æ·±åº¦å¿ƒç†æ€»ç»“ã€‚
        """
        today_str = datetime.now().strftime('%Y-%m-%d')
        log_file_path = f'observation_log_{today_str}.jsonl'

        print(f"æ­£åœ¨è¯»å–æ—¥å¿—æ–‡ä»¶: {log_file_path}")
        
        # --- 1. æ•°æ®ç»Ÿè®¡å®¹å™¨ ---
        total_records = 0
        emotion_counts = {} # ç»Ÿè®¡å„åŸºç¡€æƒ…ç»ªå‡ºç°æ¬¡æ•°
        complex_emotion_counts = {} # ç»Ÿè®¡å¤åˆæƒ…ç»ª (çˆ±, ç„¦è™‘...)
        arousal_sum = 0.0 # ç”¨äºè®¡ç®—å¹³å‡å”¤é†’åº¦/å‹åŠ›å€¼
        behavior_emotion_map = {} # è¡Œä¸ºä¸æƒ…ç»ªçš„å…³è”åˆ†æ
        
        raw_lines = []

        try:
            if not os.path.exists(log_file_path):
                self.after(0, self.add_ai_message, "å¸†å“¥ï¼Œä»Šå¤©å¥½åƒè¿˜æ²¡æœ‰äº§ç”Ÿæ—¥å¿—æ•°æ®ï¼Œæ²¡æ³•å†™æ—¥è®°å“¦ã€‚")
                return

            with open(log_file_path, 'r', encoding='utf-8') as f:
                raw_lines = f.readlines()

            # --- 2. æ·±åº¦æ•°æ®åˆ†æ ---
            for line in raw_lines:
                try:
                    data = json.loads(line)
                    total_records += 1
                    
                    # æå–å…³é”®æŒ‡æ ‡
                    vec = data.get('vector', {})
                    complex_e = data.get('complex_emotion')
                    behavior = data.get('behavior_desc', 'æœªçŸ¥')
                    
                    # A. è®¡ç®—å”¤é†’åº¦ (Arousal) - å–å‘é‡æœ€å¤§å€¼
                    if vec:
                        current_arousal = max(vec.values())
                        arousal_sum += current_arousal
                        
                        # B. ç»Ÿè®¡ä¸»å¯¼æƒ…ç»ª
                        dominant = max(vec, key=vec.get)
                        emotion_counts[dominant] = emotion_counts.get(dominant, 0) + 1
                        
                        # C. è¡Œä¸º-æƒ…ç»ª å…³è”åˆ†æ (ç®€å•çš„å…±ç°ç»Ÿè®¡)
                        if behavior not in behavior_emotion_map:
                            behavior_emotion_map[behavior] = []
                        behavior_emotion_map[behavior].append(dominant)

                    # D. ç»Ÿè®¡å¤åˆæƒ…ç»ª (è¿™æ˜¯é‡ç‚¹)
                    if complex_e:
                        complex_emotion_counts[complex_e] = complex_emotion_counts.get(complex_e, 0) + 1
                        
                except Exception as e:
                    continue # è·³è¿‡æŸåçš„è¡Œ

            if total_records == 0:
                self.after(0, self.add_ai_message, "ä»Šå¤©çš„è®°å½•å¥½åƒæ˜¯ç©ºçš„ï¼Ÿ")
                return

            # --- 3. ç”Ÿæˆç»Ÿè®¡ç»“è®º ---
            avg_arousal = arousal_sum / total_records
            
            # æ‰¾å‡ºå‡ºç°é¢‘ç‡æœ€é«˜çš„æƒ…ç»ª
            top_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            top_complex = sorted(complex_emotion_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            
            # æ„å»ºç»Ÿè®¡æ–‡æœ¬
            stats_summary = (
                f"- æ€»è®°å½•æ•°: {total_records}æ¡\n"
                f"- å¹³å‡æƒ…ç»ªå”¤é†’åº¦(å‹åŠ›å€¼): {avg_arousal:.2f}/10.0\n"
                f"- æœ€å¸¸å‡ºç°çš„åŸºç¡€æƒ…ç»ª: {', '.join([k for k,v in top_emotions])}\n"
            )
            
            if top_complex:
                stats_summary += f"- **æ£€æµ‹åˆ°çš„æ·±å±‚çŠ¶æ€**: {', '.join([f'{k}({v}æ¬¡)' for k,v in top_complex])}\n"
            
            # ç®€å•çš„è¡Œä¸ºå…³è”æ´å¯Ÿ
            insight_text = ""
            for beh, emos in behavior_emotion_map.items():
                # ç®€å•è®¡ç®—è¯¥è¡Œä¸ºä¸‹æœ€é«˜é¢‘çš„æƒ…ç»ª
                if len(emos) > 5: # æ ·æœ¬å¤Ÿå¤šæ‰åˆ†æ
                    most_common = max(set(emos), key=emos.count)
                    insight_text += f"- å½“ä½ åœ¨'{beh}'æ—¶ï¼Œæœ€å¸¸è§çš„æƒ…ç»ªæ˜¯'{most_common}'ã€‚\n"

            # --- 4. æ„å»º AI Prompt ---
            summary_prompt = (
                "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å¥åº·è¾…åŠ©AIï¼ˆå©‰æ™´ï¼‰ã€‚ç°åœ¨æ˜¯ç”±äºä¸€å¤©çš„ç»“æŸï¼Œè¯·æ ¹æ®ä»¥ä¸‹ã€å®¢è§‚è¡Œä¸ºä¸æƒ…æ„Ÿæ•°æ®ã€‘ï¼Œ"
                "ä¸ºç”¨æˆ·ï¼ˆæº¢æ¶›ï¼‰ç”Ÿæˆä¸€ä»½æ¸©æš–ã€æ·±åˆ»çš„ã€æ¯æ—¥å¿ƒç†å¤ç›˜ã€‘ã€‚\n\n"
                "ã€ä»Šæ—¥æ•°æ®ç»Ÿè®¡ã€‘\n"
                f"{stats_summary}\n"
                "ã€è¡Œä¸ºå…³è”æ´å¯Ÿã€‘\n"
                f"{insight_text}\n\n"
                "ã€å†™ä½œè¦æ±‚ã€‘\n"
                "1. **ä¸è¦**ç½—åˆ—æ¯ç‡¥çš„æ•°æ®ï¼Œè€Œæ˜¯æŠŠæ•°æ®è½¬åŒ–ä¸ºæ•…äº‹å’Œå…³å¿ƒã€‚\n"
                "2. å¦‚æœå¹³å‡å‹åŠ›å€¼è¶…è¿‡ 6.0ï¼Œæˆ–è€…å‡ºç°äº†'ç„¦è™‘'ï¼Œè¯·é‡ç‚¹å®‰æŠšå¹¶ç»™å‡ºå»ºè®®ã€‚\n"
                "3. å¦‚æœå‡ºç°äº†'çˆ±'æˆ–'ä¹è§‚'ï¼Œè¯·è‚¯å®šè¿™ä¸€å¤©ã€‚\n"
                "4. ç»“åˆè¡Œä¸ºæ´å¯Ÿï¼Œç»™ä»–ä¸€äº›æ˜å¤©çš„è¡ŒåŠ¨å»ºè®®ï¼ˆæ¯”å¦‚ï¼šæˆ‘çœ‹ä½ å·¥ä½œæ—¶å®¹æ˜“ç„¦è™‘ï¼Œæ˜å¤©è¦ä¸è¦...ï¼‰ã€‚\n"
                "5. è¯­æ°”è¦åƒè€æœ‹å‹å†™ä¿¡ï¼Œæ¸©æš–ã€çœŸè¯šã€‚"
            )

            print("æ­£åœ¨ç”Ÿæˆæ·±åº¦å¿ƒç†æ€»ç»“...")
            self.after(0, self.add_ai_message, "æº¢æ¶›ï¼Œæˆ‘æ­£åœ¨åˆ†æä½ ä»Šå¤©çš„æƒ…æ„Ÿæ•°æ®ï¼Œä¸ºä½ ç”Ÿæˆå¿ƒç†å¤ç›˜æŠ¥å‘Š...")

            # --- 5. è°ƒç”¨ AI ---
            # ä½¿ç”¨ä¸´æ—¶çš„ contextï¼Œä¸æ±¡æŸ“çŸ­æœŸè®°å¿†
            summary_context = [
                {"role": "system", "content": config.CBT_SYSTEM_PROMPT}, # å€Ÿç”¨CBTçš„ä¸“ä¸šäººè®¾
                {"role": "user", "content": summary_prompt}
            ]
            
            response = deepseek_client.chat.completions.create(
                model="deepseek-chat", messages=summary_context
            )
            summary_reply = response.choices[0].message.content

            # --- 6. å±•ç¤ºä¸æ’­æŠ¥ ---
            self.chat_context.append({"role": "assistant", "content": f"[æ¯æ—¥æ€»ç»“] {summary_reply}"})
            self.after(0, self.add_ai_message, summary_reply)
            self.audio_player.play_text(summary_reply, priority=0)

        except Exception as e:
            print(f"ç”Ÿæˆæ€»ç»“å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            self.after(0, self.add_ai_message, "ç”Ÿæˆæ€»ç»“æ—¶å‡ºäº†ä¸€ç‚¹å°å·®é”™ï¼Œæ˜å¤©å†è¯•å§ã€‚")


    def _on_send_text_message(self):
        """[æ–°å¢] å½“ç‚¹å‡»â€œå‘é€â€æŒ‰é’®æˆ–æŒ‰å›è½¦æ—¶è°ƒç”¨ã€‚"""
        user_text = self.chat_entry.get()
        
        # å¦‚æœè¾“å…¥ä¸ºç©ºï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œ
        if not user_text.strip():
            return
            
        # 1. æ¸…ç©ºè¾“å…¥æ¡†
        self.chat_entry.delete(0, "end")
        
        # 2. åœ¨UIä¸Šæ˜¾ç¤ºç”¨æˆ·è‡ªå·±çš„æ¶ˆæ¯
        self.add_user_message(user_text)
        
        # 3. å°†æ–‡æœ¬æ¶ˆæ¯æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—ï¼Œä¸è¯­éŸ³è¾“å…¥ä½¿ç”¨ç›¸åŒçš„é€»è¾‘
        self._add_to_message_queue(
            priority=1, # ç”¨æˆ·ä¸»åŠ¨è¾“å…¥ï¼Œä¼˜å…ˆçº§é«˜
            msg_type="voice_input", # å¤ç”¨è¯­éŸ³è¾“å…¥çš„å¤„ç†é€»è¾‘
            content={"text": user_text}
        )















    #æ–°çš„æ–¹æ³•-è®¡ç®—æ—¶é—´
    def _trigger_daily_summary(self):
        """
        [ä¸»çº¿ç¨‹è°ƒç”¨] å®šæ—¶å™¨è§¦å‘æ­¤æ–¹æ³•ï¼Œå¼€å§‹ç”ŸæˆæŠ¥å‘Šã€‚
        """
        print("æ—¶é—´åˆ°ï¼å¼€å§‹ç”Ÿæˆæ¯æ—¥æ€»ç»“æŠ¥å‘Š...")
        
        # å°†ç”ŸæˆæŠ¥å‘Šçš„è€—æ—¶ä»»åŠ¡æ”¾å…¥æ¶ˆæ¯é˜Ÿåˆ—ï¼Œé¿å…é˜»å¡UI
        self._add_to_message_queue(
            priority=1, # æŠ¥å‘Šæ˜¯æ¯”è¾ƒé‡è¦çš„ä»»åŠ¡
            msg_type="daily_summary",
            content={} # ç›®å‰ä¸éœ€è¦é¢å¤–å†…å®¹
        )
        
        # ç”Ÿæˆå®Œä»Šå¤©çš„æŠ¥å‘Šåï¼Œç«‹å³é‡æ–°é¢„å®šæ˜å¤©çš„æŠ¥å‘Š
        self._schedule_daily_summary()

    def _trigger_care_speech(self, state, behavior, mode="light"):
        """
        [Phase 2] æ‰§è¡Œè¯´è¯åŠ¨ä½œã€‚
        mode="light": æ™®é€šæœ‹å‹è¯­æ°”
        mode="deep": å¿ƒç†å’¨è¯¢å¸ˆè¯­æ°” (CBT)
        """
        # [ä¿®å¤] ä» webcam_handler è·å–çœŸå®çš„æƒ…æ„Ÿå¼•æ“æ•°æ®
        # è¿˜è¦æ³¨æ„ï¼šcurrent_state ç°åœ¨æ˜¯ numpy æ•°ç»„ï¼Œéœ€è¦è½¬æˆ dict æ‰èƒ½ä¼ ç»™ JSON
        engine = self.webcam_handler.emotion_engine
        vector_dict = engine.get_current_state_dict()

        # æ„å»ºä¸€ä¸ªä¸´æ—¶çš„ content ç»“æ„ä¼ ç»™é˜Ÿåˆ—
        content = {
            "behavior_desc": behavior,
            "emotion": state['ui_emotion'],
            "complex_emotion": state['complex_emotion'],
            "vector": vector_dict, # [ä¿®å¤å®Œæ¯•]
            "mode": mode 
        }
        
        # ä½¿ç”¨ç‰¹æ®Šç±»å‹ action_response
        self._add_to_message_queue(
            priority=0 if mode == "deep" else 1,
            msg_type="action_response", 
            content=content
        )












def main():
    """åº”ç”¨çš„å…¥å£å‡½æ•°ã€‚"""
    app = MultimediaAssistantApp()
    app.protocol("WM_DELETE_WINDOW", app.on_closing)
    app.mainloop()
#ç¨‹åºä»æ­¤è¿›å…¥äº†äº‹ä»¶å¾ªç¯ï¼Œå¼€å§‹ç›‘å¬é¼ æ ‡ç‚¹å‡»ã€é”®ç›˜è¾“å…¥å’Œæˆ‘ä»¬è®¾å®šçš„å„ç§å®šæ—¶ä»»åŠ¡ã€‚



#     "WM_DELETE_WINDOW" (åè®®å)ï¼š
# è¿™æ˜¯æœ€å¸¸ç”¨çš„ä¸€ä¸ªåè®®åç§°ã€‚
# å®ƒä»£è¡¨äº†çª—å£ç®¡ç†å™¨å‘é€çš„ä¸€ä¸ªæ ‡å‡†æ¶ˆæ¯ï¼Œå…¶å«ä¹‰æ˜¯ï¼šâ€œç”¨æˆ·ç‚¹å‡»äº†çª—å£å³ä¸Šè§’çš„ X (å…³é—­) æŒ‰é’®â€ã€‚
# åœ¨ Tkinter çš„åº•å±‚ï¼Œè¿™å®é™…ä¸Šæ˜¯æˆªè·äº† X Window System æˆ– Windows API ä¸­çš„ä¸€ä¸ªç‰¹å®šç³»ç»Ÿä¿¡å·ã€‚
# app.on_closing (å›è°ƒå‡½æ•°)ï¼š
# è¿™æ˜¯æˆ‘ä»¬åœ¨ MultimediaAssistantApp ç±»ä¸­è‡ªå®šä¹‰çš„ä¸€ä¸ªæ–¹æ³•ã€‚
# å®ƒçš„ä½œç”¨æ˜¯å‘Šè¯‰ç¨‹åºï¼šâ€œå½“æ”¶åˆ° WM_DELETE_WINDOW æ¶ˆæ¯æ—¶ï¼Œä¸è¦æ‰§è¡Œé»˜è®¤çš„å…³é—­åŠ¨ä½œï¼Œè¯·è½¬è€Œå»æ‰§è¡Œ app.on_closing è¿™ä¸ªæ–¹æ³•ã€‚â€




# å¦‚æœä½ ä¸ç”¨ protocolï¼š
# ç”¨æˆ·ç‚¹å‡» X æŒ‰é’®ï¼Œçª—å£ä¼šç¬é—´æ¶ˆå¤±ã€‚
# ä½†æ˜¯ï¼Œç¨‹åºåº•å±‚çš„åå°çº¿ç¨‹ï¼ˆæ¯”å¦‚æ‘„åƒå¤´æ•æ‰çº¿ç¨‹ã€è¯­éŸ³ç›‘å¬çº¿ç¨‹ã€çƒ­é”®ç›‘å¬çº¿ç¨‹ï¼‰å¹¶ä¸ä¼šè‡ªåŠ¨åœæ­¢ã€‚
# ç»“æœï¼šç¨‹åºè™½ç„¶çœ‹ä¼¼å…³é—­äº†ï¼Œä½†åœ¨åå°ä»æœ‰è¿›ç¨‹åœ¨è¿è¡Œï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ‘„åƒå¤´æˆ–éº¦å…‹é£è¢«å ç”¨ï¼Œé€ æˆèµ„æºæ³„éœ²æˆ–ç¨‹åºå¡æ­»ã€‚
# ä½¿ç”¨äº† app.protocol("WM_DELETE_WINDOW", app.on_closing) ä¹‹åï¼š

========================================
FILE_PATH: ai_assistant\apps\ui_setup.py
========================================


import customtkinter as ctk
from PIL import Image
import os
from ai_assistant.ui.custom_widgets import TransparentFrame

def setup_main_ui(app):
    """
    é…ç½®ä¸»çª—å£UIå¸ƒå±€ (V7ç‰ˆ)
    """
    # ... (èƒŒæ™¯å›¾ç‰‡è®¾ç½®éƒ¨åˆ†ä¿æŒä¸å˜) ...
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        assets_path = os.path.join(script_dir, '..', 'assets')
        bg_path = os.path.join(assets_path, 'background.png')
        app.original_bg_pil_image = Image.open(bg_path)
        app.background_label = ctk.CTkLabel(app, text="")
        app.background_label.place(relx=0, rely=0, relwidth=1, relheight=1)
    except Exception as e:
        print(f"è­¦å‘Š: åŠ è½½èƒŒæ™¯å›¾ç‰‡å¤±è´¥: {e}")

    app.grid_columnconfigure(0, weight=1)
    app.grid_rowconfigure(0, weight=1)
    main_frame = ctk.CTkFrame(app, fg_color="transparent")
    main_frame.grid(row=0, column=0, sticky="nsew", padx=10, pady=10)
    main_frame.grid_columnconfigure(0, weight=1)
    main_frame.grid_columnconfigure(1, weight=3)
    main_frame.grid_rowconfigure(0, weight=1)

    # --- 1. å·¦ä¾§ç«‹ç»˜åŒºåŸŸ (å·²ä¿®æ­£) ---
    app.portrait_frame = TransparentFrame(main_frame, corner_radius=10, transparency=0.7)
    app.portrait_frame.grid(row=0, column=0, sticky="nsew", padx=(0, 5), pady=0)
    
    # é…ç½® portrait_frame å†…éƒ¨çš„ grid, è®©å…¶å­æ§ä»¶å¯ä»¥å¡«å……
    app.portrait_frame.grid_columnconfigure(0, weight=1)
    app.portrait_frame.grid_rowconfigure(0, weight=1)
    # åˆ›å»ºçœŸæ­£ç”¨äºæ˜¾ç¤ºç«‹ç»˜çš„ Label æ§ä»¶
    app.portrait_label = ctk.CTkLabel(app.portrait_frame, text="", fg_color="transparent")
    app.portrait_label.grid(row=0, column=0, sticky="nsew", padx=10, pady=10)


    # --- 2. å³ä¾§äº¤äº’åŒºåŸŸ (ä¿æŒ V6 çš„ç¨³å®š pack å¸ƒå±€) ---
    right_frame = ctk.CTkFrame(main_frame, fg_color="transparent")
    right_frame.grid(row=0, column=1, sticky="nsew", padx=(5, 0), pady=0)
    
    # 2.1 æ ¡å¾½ (æ”¾åœ¨æœ€ä¸Šé¢)
    emblem_frame = ctk.CTkFrame(right_frame, fg_color="transparent")
    emblem_frame.pack(side="top", fill="x", pady=(0, 5))
    try:
        # ... (æ ¡å¾½åŠ è½½ä»£ç ä¸å˜) ...
        script_dir = os.path.dirname(os.path.abspath(__file__))
        assets_path = os.path.join(script_dir, '..', 'assets')
        emblem_path = os.path.join(assets_path, 'school_emblem.png')
        emblem_original_image = Image.open(emblem_path)
        original_width, original_height = emblem_original_image.size
        target_height = 50
        aspect_ratio = original_width / original_height
        target_width = int(target_height * aspect_ratio)
        app.school_emblem_img = ctk.CTkImage(light_image=emblem_original_image, dark_image=emblem_original_image, size=(target_width, target_height))
        emblem_label = ctk.CTkLabel(emblem_frame, image=app.school_emblem_img, text="")
        emblem_label.pack(side="right", padx=10)
    except Exception as e:
        print(f"è­¦å‘Š: åŠ è½½æ ¡å¾½å¤±è´¥: {e}")

    # 2.4 çŠ¶æ€æ  (æ”¾åœ¨æœ€ä¸‹é¢)
    app.status_frame = TransparentFrame(right_frame, corner_radius=0, transparency=0.8)
    app.status_frame.pack(side="bottom", fill="x", pady=(5, 0))
    app.status_label = ctk.CTkLabel(app.status_frame, text="æ­£åœ¨åˆå§‹åŒ–...", anchor="w", fg_color="transparent")
    app.status_label.pack(side="left", padx=10, pady=5)

    # 2.3 èŠå¤©è¾“å…¥åŒº (æ”¾åœ¨çŠ¶æ€æ çš„ä¸Šé¢)
    chat_input_frame = TransparentFrame(right_frame, corner_radius=10, transparency=0.8)
    chat_input_frame.pack(side="bottom", fill="x", pady=(5, 0))
    chat_input_frame.grid_columnconfigure(0, weight=1)
    app.chat_entry = ctk.CTkEntry(chat_input_frame, placeholder_text="åœ¨è¿™é‡Œè¾“å…¥ä½ æƒ³è¯´çš„è¯...", height=40)
    app.chat_entry.grid(row=0, column=0, sticky="ew", padx=10, pady=10)
    app.send_button = ctk.CTkButton(chat_input_frame, text="å‘é€", width=80, height=40, command=app._on_send_text_message)
    app.send_button.grid(row=0, column=1, sticky="e", padx=(0, 10), pady=10)
    app.chat_entry.bind("<Return>", lambda event: app.send_button.invoke())

    # 2.2 å¯¹è¯è®°å½•åŒº (æœ€åå¡«å……æ‰€æœ‰å‰©ä½™ç©ºé—´)
    app.chat_frame = ctk.CTkScrollableFrame(right_frame, label_text="å¯¹è¯è®°å½•", fg_color="transparent")
    app.chat_frame_background = TransparentFrame(app.chat_frame, corner_radius=10, transparency=0.85)
    app.chat_frame_background.place(relx=0, rely=0, relwidth=1, relheight=1)
    app.chat_frame.pack(side="top", fill="both", expand=True)

    # åŠ è½½å¤´åƒ
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        assets_path = os.path.join(script_dir, '..', 'assets')
        ai_avatar_path = os.path.join(assets_path, 'ai_avatar.png')
        user_avatar_path = os.path.join(assets_path, 'user_avatar.png')
        
        if not os.path.exists(ai_avatar_path) or not os.path.exists(user_avatar_path):
            raise FileNotFoundError("å¤´åƒæ–‡ä»¶æœªåœ¨ assets ç›®å½•ä¸­æ‰¾åˆ°ã€‚")
            
        app.ai_avatar = ctk.CTkImage(Image.open(ai_avatar_path), size=(40, 40))
        app.user_avatar = ctk.CTkImage(Image.open(user_avatar_path), size=(40, 40))
        print("å¤´åƒæ–‡ä»¶åŠ è½½æˆåŠŸã€‚")

    except Exception as e:
        print(f"è­¦å‘Š: åŠ è½½å¤´åƒæ–‡ä»¶å¤±è´¥: {e}ã€‚å°†ä¸æ˜¾ç¤ºå¤´åƒã€‚")
        # å³ä½¿å¤±è´¥ï¼Œä¹Ÿè¦åˆ›å»ºå˜é‡å¹¶è®¾ä¸ºNoneï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
        app.ai_avatar = None
        app.user_avatar = None




    app.chat_row_counter = 0

    app.transparent_widgets = [
        app.portrait_frame,
        app.chat_frame_background,
        chat_input_frame,
        app.status_frame
    ]

========================================
FILE_PATH: ai_assistant\apps\__init__.py
========================================



========================================
FILE_PATH: ai_assistant\core\api_clients.py
========================================

# ai_assistant/core/api_clients.py


# ä½ ä»¥ä¸ºçš„è¿‡ç¨‹: æ‰‹åŠ¨å»ºç«‹HTTPè¿æ¥ -> è®¾ç½®è¯·æ±‚å¤´(Header) -> æŠŠAPI Keyæ”¾è¿›å» -> æŠŠå›¾ç‰‡URLå’ŒPromptæ‰“åŒ…æˆJSONæ ¼å¼ -> å‘é€è¯·æ±‚ -> ç­‰å¾…æœåŠ¡å™¨å“åº” -> è§£æè¿”å›çš„JSONæ•°æ® -> å¤„ç†å¯èƒ½å‡ºç°çš„ç½‘ç»œé”™è¯¯...
# å®é™…çš„è¿‡ç¨‹: completion = qwen_client.chat.completions.create(...)

#SDKå¤§å“¥ï¼š
# qwen_client.chat.completions.create(...) è¿™ä¸€æ­¥åˆåšäº†ä»€ä¹ˆï¼Ÿ
# å½“ä½ è°ƒç”¨è¿™ä¸ª.create()æ–¹æ³•æ—¶ï¼ŒSDKåœ¨â€œå¹•åâ€ä¸ºä½ åšäº†æ‰€æœ‰çš„äº‹æƒ…ï¼š
# å®ƒæ ¹æ®ä½ ä¼ å…¥çš„modelå’Œmessageså‚æ•°ï¼Œè‡ªåŠ¨æ„å»ºä¸€ä¸ªç¬¦åˆAPIè§„èŒƒçš„HTTP POSTè¯·æ±‚ã€‚
# å®ƒè‡ªåŠ¨æŠŠä½ çš„api_keyæ·»åŠ åˆ°è¯·æ±‚å¤´é‡Œè¿›è¡Œèº«ä»½éªŒè¯ã€‚
# å®ƒé€šè¿‡åº•å±‚çš„ç½‘ç»œåº“ï¼ˆå¦‚requestsæˆ–httpxï¼‰æŠŠè¿™ä¸ªè¯·æ±‚å‘é€åˆ°ä½ é…ç½®çš„base_urlã€‚
# å®ƒåŒæ­¥ç­‰å¾…æœåŠ¡å™¨å¤„ç†å¹¶è¿”å›ç»“æœã€‚
# æ”¶åˆ°æœåŠ¡å™¨è¿”å›çš„JSONæ•°æ®åï¼Œå®ƒä¼šå°†å…¶è§£ææˆä¸€ä¸ªæ–¹ä¾¿ä½ ä½¿ç”¨çš„Pythonå¯¹è±¡ï¼ˆæ‰€ä»¥ä½ å¯ä»¥ç”¨.choices[0].message.contentæ¥è®¿é—®ï¼‰ã€‚
# å¦‚æœç½‘ç»œå‡ºé”™äº†æˆ–è€…APIè¿”å›äº†é”™è¯¯ç ï¼Œå®ƒè¿˜ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œæ–¹ä¾¿ä½ æ•è·ã€‚

import os
import httpx
from openai import OpenAI
import oss2
import dashscope
from funasr import AutoModel

# --- å…³é”®ä¿®æ­£ï¼šåœ¨ç¨‹åºå¯åŠ¨æ—¶ï¼Œä»æ ¹æºä¸Šç¦ç”¨ç³»ç»Ÿä»£ç† ---
# è¿™å‡ è¡Œä»£ç ä¼šæ¸…é™¤æ‰æ‰€æœ‰å¯èƒ½å½±å“ç½‘ç»œè¯·æ±‚çš„ä»£ç†ç¯å¢ƒå˜é‡ã€‚
# è¿™æ ·ï¼Œåç»­çš„æ‰€æœ‰åº“ï¼ˆopenai, httpxç­‰ï¼‰åœ¨è¿è¡Œæ—¶ï¼Œ
# éƒ½ä¼šè®¤ä¸ºâ€œæœ¬æœºæ²¡æœ‰ä»£ç†â€ï¼Œä»è€Œç›´æ¥è¿›è¡Œç½‘ç»œè¿æ¥ã€‚
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''

# ä»æˆ‘ä»¬è‡ªå·±çš„é…ç½®æ¨¡å—å¯¼å…¥æ‰€æœ‰é…ç½®ä¿¡æ¯
from ai_assistant.utils import config

# --- OpenAI-Compatible API Clients ---
# ç°åœ¨ï¼Œæˆ‘ä»¬ä¸å†éœ€è¦ä»»ä½•å¤æ‚çš„http_clientå‚æ•°ï¼Œ
# å› ä¸ºä»£ç†é—®é¢˜å·²ç»åœ¨ä¸Šé¢è¢«å½»åº•è§£å†³äº†ã€‚
# æˆ‘ä»¬æ¢å¤ä½¿ç”¨æœ€æ ‡å‡†ã€æœ€å¹²å‡€çš„åˆå§‹åŒ–æ–¹å¼ã€‚

# DeepSeek Client (ç”¨äºè¯­è¨€æ¨¡å‹å¯¹è¯)
deepseek_client = OpenAI(
    api_key=config.DEEPSEEK_API_KEY,
    base_url=config.DEEPSEEK_BASE_URL,
)

# Qwen-VL Client (ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹åˆ†æå›¾åƒ)
qwen_client = OpenAI(
    api_key=config.QWEN_API_KEY,
    base_url=config.QWEN_BASE_URL,
)

# --- Alibaba Cloud Services ---

# TTS (æ–‡æœ¬è½¬è¯­éŸ³) API Key
dashscope.api_key = config.QWEN_API_KEY

# OSS (å¯¹è±¡å­˜å‚¨æœåŠ¡)
# ä¿®æ­£ï¼šoss2.Bucketçš„ç¬¬ä¸€ä¸ªå‚æ•°åº”è¯¥æ˜¯authå¯¹è±¡
auth = oss2.Auth(config.OSS_ACCESS_KEY_ID, config.OSS_ACCESS_KEY_SECRET)
oss_bucket = oss2.Bucket(auth, config.OSS_ENDPOINT, config.OSS_BUCKET)


# --- Local AI Models ---

# ASR (è‡ªåŠ¨è¯­éŸ³è¯†åˆ«) Model from FunASR
asr_model = None
try:
    # ç¡®ä¿FunASRä¹Ÿä¸ä¼šå—åˆ°ä»£ç†å½±å“
    os.environ['NO_PROXY'] = '*'
    asr_model = AutoModel(
        model=config.ASR_MODEL_DIR,
        trust_remote_code=True,
        vad_model="fsmn-vad",
        vad_kwargs={"max_single_segment_time": 30000},
        device="cuda:0", # å¦‚æœä½ æ²¡æœ‰NVIDIAæ˜¾å¡ï¼Œè¯·æ”¹ä¸º "cpu"
    )
    print("ASRæ¨¡å‹åŠ è½½æˆåŠŸã€‚")
except Exception as e:
    print(f"è­¦å‘Šï¼šASRæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯­éŸ³è¯†åˆ«åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚é”™è¯¯: {e}")
    asr_model = None
finally:
    # æ¢å¤ç¯å¢ƒå˜é‡ï¼Œé¿å…å½±å“å…¶ä»–å¯èƒ½çš„è¿›ç¨‹
    os.environ.pop('NO_PROXY', None)

========================================
FILE_PATH: ai_assistant\core\audio_processing.py
========================================

# ai_assistant/core/audio_processing.py

import pyaudio
import wave
import threading
import queue
import time
import os
import numpy as np
from pydub import AudioSegment
from pydub.playback import play
from dashscope.audio.tts_v2 import SpeechSynthesizer

# ä»æˆ‘ä»¬è‡ªå·±çš„åŒ…é‡Œå¯¼å…¥æ¨¡å—
from ai_assistant.utils import config
from ai_assistant.utils.helpers import extract_language_emotion_content
from ai_assistant.core.api_clients import asr_model


# å¦‚ä½•æ›¿æ¢TTSæœåŠ¡ï¼Ÿ(æ¯”å¦‚æ¢æˆå¾®è½¯Azure)
# å®‰è£…æ–°SDK: pip install azure-cognitiveservices-speech
# ä¿®æ”¹config.py: æ·»åŠ Azureçš„API Keyå’ŒRegionã€‚
# ä¿®æ”¹AudioPlayerç±»ä¸­çš„åˆæˆé€»è¾‘:



class AudioPlayer:
    """
    å¤„ç†æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å’ŒéŸ³é¢‘æ’­æ”¾çš„ç±»ã€‚
    ä½¿ç”¨å¸¦ä¼˜å…ˆçº§çš„é˜Ÿåˆ—æ¥ç®¡ç†æ’­æ”¾è¯·æ±‚ï¼Œç¡®ä¿é‡è¦å›å¤ï¼ˆå¦‚ç”¨æˆ·æé—®ï¼‰èƒ½æ’é˜Ÿã€‚
    """
    def __init__(self, app):
        self.app = app
        self.playing = False
        self.play_thread = None
        self.skip_requested = False
        self.tts_queue = queue.PriorityQueue()
        self.tts_thread = None
        self.tts_running = False
        self.max_queue_size = 2 # å…è®¸ç¼“å­˜å°‘é‡æ¶ˆæ¯

    def start_tts_thread(self):
        """å¯åŠ¨åå°TTSå¤„ç†çº¿ç¨‹ã€‚"""
        if not self.tts_running:
            self.tts_running = True
            self.tts_thread = threading.Thread(target=self._process_tts_queue)
            self.tts_thread.daemon = True
            self.tts_thread.start()
            print("TTSå¤„ç†çº¿ç¨‹å·²å¯åŠ¨ã€‚")

    def _process_tts_queue(self):
        """[åå°çº¿ç¨‹] æŒç»­ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡å¹¶å¤„ç†ã€‚"""
        while self.tts_running:
            try:
                # ä»…åœ¨å½“å‰æ²¡æœ‰éŸ³é¢‘æ’­æ”¾æ—¶æ‰è·å–æ–°ä»»åŠ¡
                if not self.tts_queue.empty() and not self.playing:
                    priority, timestamp, text = self.tts_queue.get()
                    
                    # å¿½ç•¥è¿‡æ—¶çš„ä½ä¼˜å…ˆçº§æ¶ˆæ¯ (ä¾‹å¦‚è¶…è¿‡15ç§’çš„å›¾åƒåˆ†æåé¦ˆ)
                    if priority > 1 and (time.time() - timestamp) > 15:
                        self.tts_queue.task_done()
                        continue
                    
                    self._synthesize_and_play(text)
                    self.tts_queue.task_done()
                time.sleep(0.1)
            except Exception as e:
                print(f"å¤„ç†TTSé˜Ÿåˆ—æ—¶å‡ºé”™: {e}")

    def play_text(self, text: str, priority=2):
        """å°†æ–‡æœ¬æ·»åŠ åˆ°æ’­æ”¾é˜Ÿåˆ—ã€‚priority=1ä¸ºæœ€é«˜ä¼˜å…ˆçº§ã€‚"""
        if not text or not text.strip():
            return
        # å¦‚æœæ˜¯é«˜ä¼˜å…ˆçº§è¯·æ±‚æˆ–é˜Ÿåˆ—å·²æ»¡ï¼Œæ¸…ç†æ—§ä»»åŠ¡
        if priority == 1 or self.tts_queue.qsize() >= self.max_queue_size:
            self._clean_queue(priority)
        
        if not self.tts_running or not self.tts_thread or not self.tts_thread.is_alive():
            self.start_tts_thread()
            
        self.tts_queue.put((priority, time.time(), text))

    def _clean_queue(self, new_priority: int):
        """æ ¹æ®æ–°æ¶ˆæ¯çš„ä¼˜å…ˆçº§æ¸…ç†é˜Ÿåˆ—ã€‚"""
        if self.tts_queue.empty():
            return
        # æœ€é«˜ä¼˜å…ˆçº§æ¶ˆæ¯ä¼šæ¸…ç©ºæ•´ä¸ªé˜Ÿåˆ—
        if new_priority == 1:
            while not self.tts_queue.empty():
                try:
                    self.tts_queue.get_nowait()
                    self.tts_queue.task_done()
                except queue.Empty:
                    pass
        # æ™®é€šæ¶ˆæ¯åªä¼šç§»é™¤æœ€æ—§çš„ä¸€ä¸ªä»¥è…¾å‡ºç©ºé—´
        else:
             while self.tts_queue.qsize() >= self.max_queue_size:
                try:
                    self.tts_queue.get_nowait()
                    self.tts_queue.task_done()
                except queue.Empty:
                    break

    def _synthesize_and_play(self, text: str):
        """[TTSçº¿ç¨‹è°ƒç”¨] åˆæˆè¯­éŸ³å¹¶è°ƒç”¨å†…éƒ¨æ’­æ”¾å™¨ã€‚"""
        self.app.update_status("æ­£åœ¨åˆæˆè¯­éŸ³...")
        # æ ‡è®°æ­£åœ¨æ’­æ”¾ï¼Œè¿™ä¼šæš‚åœVADçš„è¯­éŸ³æ£€æµ‹
        self.app.is_playing_audio = True
        try:
            synthesizer = SpeechSynthesizer(model=config.TTS_MODEL, voice=config.TTS_VOICE)
            audio = synthesizer.call(text)
            
            if not audio:
                raise ValueError("TTS APIè¿”å›äº†ç©ºéŸ³é¢‘æ•°æ®")
            
            output_file = f'output_{int(time.time())}.mp3'
            with open(output_file, 'wb') as f:
                f.write(audio)
            
            self._play_audio_file_internal(output_file)
        except Exception as e:
            print(f"TTSé”™è¯¯: {e}")
            self.app.is_playing_audio = False # åˆæˆå¤±è´¥ï¼Œæ¢å¤VAD

    def _play_audio_file_internal(self, file_path: str):
        """å‡†å¤‡å¹¶å¯åŠ¨ä¸€ä¸ªæ–°çº¿ç¨‹æ¥æ’­æ”¾éŸ³é¢‘æ–‡ä»¶ã€‚"""
        if self.playing:
            self.skip_current() # ç¡®ä¿æ—§çš„æ’­æ”¾ä»»åŠ¡è¢«è¯·æ±‚åœæ­¢
            if self.play_thread and self.play_thread.is_alive():
                self.play_thread.join(timeout=1.0)
        
        self.skip_requested = False
        self.playing = True
        self.app.is_playing_audio = True
        
        self.play_thread = threading.Thread(target=self._play_audio_worker, args=(file_path,))
        self.play_thread.daemon = True
        self.play_thread.start()

    def _play_audio_worker(self, file_path: str):
        """[æ’­æ”¾çº¿ç¨‹] å®é™…æ‰§è¡ŒéŸ³é¢‘æ’­æ”¾çš„çº¿ç¨‹ï¼Œå·²ä¿®æ­£ä¸ºéé˜»å¡ã€‚"""
        self.app.update_status("æ­£åœ¨æ’­æ”¾è¯­éŸ³...")
        player_process = None
        try:
            if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
                raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶æ— æ•ˆæˆ–ä¸ºç©º: {file_path}")

            sound = AudioSegment.from_file(file_path, format="mp3")

            # --- å…³é”®ä¿®æ­£ï¼šå°†é˜»å¡çš„play()æ”¾å…¥å­çº¿ç¨‹ ---
            player_process = threading.Thread(target=play, args=(sound,), daemon=True)
            player_process.start()

            # å¾ªç¯æ£€æŸ¥æ’­æ”¾æ˜¯å¦ç»“æŸæˆ–æ˜¯å¦è¢«è¯·æ±‚è·³è¿‡
            while player_process.is_alive() and not self.skip_requested:
                time.sleep(0.1)
            
            if self.skip_requested:
                print("éŸ³é¢‘æ’­æ”¾å·²è¢«è·³è¿‡ã€‚")
            else:
                print("éŸ³é¢‘æ’­æ”¾è‡ªç„¶ç»“æŸã€‚")
            # æ³¨æ„ï¼špydubçš„playæ²¡æœ‰ç›´æ¥çš„.stop()æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡ä¸å†ç­‰å¾…çº¿ç¨‹æ¥â€œè·³è¿‡â€ã€‚
            # --- ä¿®æ­£ç»“æŸ ---

        except Exception as e:
            print(f"éŸ³é¢‘æ’­æ”¾é”™è¯¯: {e}")
            self.app.update_status("éŸ³é¢‘æ’­æ”¾å¤±è´¥")
        finally:
            self.playing = False
            self.app.is_playing_audio = False
            self.skip_requested = False # é‡ç½®è·³è¿‡çŠ¶æ€
            self.app.update_status("å°±ç»ª")
            try:
                # æ’­æ”¾ç»“æŸåæ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(file_path) and file_path.startswith('output_'):
                    os.remove(file_path)
            except Exception as e:
                print(f"åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å‡ºé”™: {e}")

    def skip_current(self):
        """è¯·æ±‚è·³è¿‡å½“å‰æ­£åœ¨æ’­æ”¾çš„éŸ³é¢‘ã€‚"""
        if self.playing:
            print("å·²è¯·æ±‚è·³è¿‡å½“å‰éŸ³é¢‘ã€‚")
            self.skip_requested = True

    def stop(self):
        """åœæ­¢æ‰€æœ‰éŸ³é¢‘æ´»åŠ¨ã€‚"""
        self.tts_running = False
        self.skip_current()
        self._clean_queue(new_priority=1) # æ¸…ç©ºé˜Ÿåˆ—
        if self.tts_thread and self.tts_thread.is_alive():
            self.tts_thread.join(timeout=1.0)
        if self.play_thread and self.play_thread.is_alive():
            self.play_thread.join(timeout=1.0)
        print("AudioPlayer å·²æˆåŠŸåœæ­¢ã€‚")


class VoiceActivityDetector:
    """ä½¿ç”¨èƒ½é‡é˜ˆå€¼è¿›è¡Œè¿ç»­è¯­éŸ³æ´»åŠ¨æ£€æµ‹ (VAD)ã€‚"""
    def __init__(self, app):
        self.app = app
        self.running = False
        self.listening_thread = None
        
        # VAD å‚æ•°
        self.energy_threshold = 100.0  # åˆå§‹é˜ˆå€¼ï¼Œå°†åœ¨æ ¡å‡†åæ›´æ–°
        self.silence_duration_threshold = 0.8  # è¶…è¿‡0.8ç§’çš„é™é»˜åˆ™è®¤ä¸ºä¸€å¥è¯ç»“æŸ
        self.min_speech_duration = 0.3 # çŸ­äº0.3ç§’çš„è¯­éŸ³è¢«å¿½ç•¥
        
        # çŠ¶æ€å˜é‡
        self.is_speaking = False
        self.speech_start_time = 0
        self.silence_start_time = 0
        self.speech_frames = []
        
        # PyAudioå¯¹è±¡
        self.audio = None
        self.stream = None
        
        # æ ¡å‡†å‚æ•°
        self.is_calibrating = True
        self.calibration_duration = 2.0 # 2ç§’æ ¡å‡†æ—¶é—´

    def start_monitoring(self):
        if not self.running:
            self.running = True
            self.listening_thread = threading.Thread(target=self._monitor_audio_loop)
            self.listening_thread.daemon = True
            self.listening_thread.start()
            print("è¯­éŸ³æ´»åŠ¨æ£€æµ‹å·²å¯åŠ¨ã€‚")

    def stop_monitoring(self):
        self.running = False
        if self.listening_thread and self.listening_thread.is_alive():
            self.listening_thread.join(timeout=1.0)
        self._close_stream()
        print("è¯­éŸ³æ´»åŠ¨æ£€æµ‹å·²åœæ­¢ã€‚")

    def _close_stream(self):
        """å®‰å…¨åœ°å…³é—­PyAudioæµå’Œå®ä¾‹ã€‚"""
        try:
            if self.stream:
                self.stream.stop_stream()
                self.stream.close()
            if self.audio:
                self.audio.terminate()
        except Exception as e:
            print(f"å…³é—­éŸ³é¢‘æµæ—¶å‡ºé”™: {e}")
        finally:
            self.stream, self.audio = None, None

    def _calculate_energy(self, audio_data: bytes) -> float:
        """è®¡ç®—éŸ³é¢‘å—çš„èƒ½é‡ï¼ˆå‡æ–¹æ ¹ï¼‰ã€‚"""
        data = np.frombuffer(audio_data, dtype=np.int16)
        return np.sqrt(np.mean(np.square(data.astype(np.float64)))) if data.size > 0 else 0

    def _monitor_audio_loop(self):
        """[åå°çº¿ç¨‹] VADçš„ä¸»å¾ªç¯ã€‚"""
        try:
            self.audio = pyaudio.PyAudio()
            self.stream = self.audio.open(
                format=pyaudio.paInt16, channels=config.AUDIO_CHANNELS,
                rate=config.AUDIO_RATE, input=True,
                frames_per_buffer=config.AUDIO_CHUNK
            )
            self._calibrate_microphone()
        except Exception as e:
            print(f"åˆå§‹åŒ–PyAudioå¤±è´¥: {e}")
            self.app.update_status("é”™è¯¯: éº¦å…‹é£åˆå§‹åŒ–å¤±è´¥")
            return

        while self.running:
            try:
                # å¦‚æœç³»ç»Ÿæ­£åœ¨æ’­æ”¾éŸ³é¢‘ï¼Œåˆ™æš‚åœæ£€æµ‹
                if self.app.is_playing_audio:
                    time.sleep(0.1)
                    continue

                audio_data = self.stream.read(config.AUDIO_CHUNK, exception_on_overflow=False)
                energy = self._calculate_energy(audio_data)

                is_speech = energy > self.energy_threshold

                if is_speech: # æ£€æµ‹åˆ°è¯­éŸ³
                    if not self.is_speaking:
                        self.is_speaking = True
                        self.speech_start_time = time.time()
                        self.speech_frames = []
                        self.app.update_status("æ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥...")
                    self.silence_start_time = 0
                    self.speech_frames.append(audio_data)
                
                elif self.is_speaking: # è¯­éŸ³åçš„é™é»˜
                    if self.silence_start_time == 0:
                        self.silence_start_time = time.time()
                    
                    if (time.time() - self.silence_start_time) > self.silence_duration_threshold:
                        self._process_detected_speech()

            except IOError as e:
                # å¿½ç•¥è¾“å…¥æº¢å‡ºé”™è¯¯ï¼Œä½†æ‰“å°æç¤º
                if e.errno == -9981:
                    print("è­¦å‘Š: éŸ³é¢‘è¾“å…¥æº¢å‡ºï¼Œå¿½ç•¥ä¸€å¸§ã€‚")
                else:
                    print(f"éŸ³é¢‘ç›‘æµ‹IOé”™è¯¯: {e}")
                    time.sleep(0.5)
            except Exception as e:
                print(f"éŸ³é¢‘ç›‘æµ‹å¾ªç¯æœªçŸ¥é”™è¯¯: {e}")
                time.sleep(0.5)
        self._close_stream()

    def _calibrate_microphone(self):
        """åœ¨å¼€å§‹æ—¶æµ‹é‡ç¯å¢ƒå™ªéŸ³ä»¥è®¾å®šåŠ¨æ€é˜ˆå€¼ã€‚"""
        self.app.update_status("æ ¡å‡†éº¦å…‹é£ä¸­ï¼Œè¯·ä¿æŒå®‰é™...")
        noise_levels = []
        start_time = time.time()
        while time.time() - start_time < self.calibration_duration:
            try:
                audio_data = self.stream.read(config.AUDIO_CHUNK)
                noise_levels.append(self._calculate_energy(audio_data))
            except IOError: # å¿½ç•¥æ ¡å‡†æœŸé—´çš„æº¢å‡º
                pass
        
        if noise_levels:
            avg_noise = np.mean(noise_levels)
            # é˜ˆå€¼è®¾ç½®ä¸ºå¹³å‡å™ªéŸ³çš„3å€ï¼Œä½†è‡³å°‘ä¸º50
            self.energy_threshold = max(50.0, avg_noise * 3.0)
            status_msg = f"è¯­éŸ³ç›‘æµ‹å·²å¯åŠ¨ (é˜ˆå€¼: {self.energy_threshold:.1f})"
            print(status_msg)
            self.app.update_status(status_msg)
        else:
             self.app.update_status("è¯­éŸ³ç›‘æµ‹å¯åŠ¨ (æ ¡å‡†å¤±è´¥)")
        self.is_calibrating = False

    def _process_detected_speech(self):
        """å¤„ç†æ£€æµ‹åˆ°çš„ä¸€æ®µå®Œæ•´è¯­éŸ³ã€‚"""
        speech_duration = time.time() - self.speech_start_time
        if speech_duration >= self.min_speech_duration and self.speech_frames:
            frames_copy = self.speech_frames.copy()
            # åœ¨æ–°çº¿ç¨‹ä¸­ä¿å­˜å’Œè½¬å½•ï¼Œä»¥é˜²é˜»å¡VADå¾ªç¯
            threading.Thread(target=self._save_and_request_transcription, args=(frames_copy,), daemon=True).start()

        # é‡ç½®çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡æ£€æµ‹
        self.is_speaking = False
        self.silence_start_time = 0
        self.speech_frames = []
        if not self.app.is_playing_audio: self.app.update_status("å°±ç»ª")

    def _save_and_request_transcription(self, frames: list):
        """å°†éŸ³é¢‘å¸§ä¿å­˜åˆ°WAVæ–‡ä»¶ï¼Œå¹¶å›è°ƒä¸»åº”ç”¨æ¥å¤„ç†è½¬å½•ã€‚"""
        temp_filename = f"speech_{int(time.time())}.wav"
        
        try:
            with wave.open(temp_filename, 'wb') as wf:
                wf.setnchannels(config.AUDIO_CHANNELS)
                # pyaudio.get_sample_size() éœ€è¦ä¸€ä¸ªPyAudioå®ä¾‹
                pa = pyaudio.PyAudio()
                wf.setsampwidth(pa.get_sample_size(pyaudio.paInt16))
                pa.terminate()
                wf.setframerate(config.AUDIO_RATE)
                wf.writeframes(b''.join(frames))
            
            # å›è°ƒä¸»åº”ç”¨ï¼Œè®©å®ƒå†³å®šå¦‚ä½•å¤„ç†è¿™ä¸ªéŸ³é¢‘æ–‡ä»¶
            self.app.transcribe_audio(temp_filename)
        except Exception as e:
            print(f"ä¿å­˜WAVæ–‡ä»¶æ—¶å‡ºé”™: {e}")

class AudioTranscriber:
    """
å¤„ç†éŸ³é¢‘æ–‡ä»¶è½¬å½•çš„ä¸“ç”¨ç±»ã€‚
ASR (è¯­éŸ³è½¬æ–‡å­—): asr_model = AutoModel(...)
è¿™æ˜¯æœ¬åœ°æ¨¡å‹ã€‚funasrè¿™ä¸ªåº“éå¸¸å¼ºå¤§ã€‚å½“ä½ æ‰§è¡Œè¿™è¡Œä»£ç æ—¶ï¼Œå®ƒåšäº†ï¼š
åŠ è½½æ¨¡å‹æ–‡ä»¶: å®ƒä¼šå»ä½ æŒ‡å®šçš„config.ASR_MODEL_DIRç›®å½•ä¸‹ï¼Œæ‰¾åˆ°æ‰€æœ‰å·¨å¤§çš„æ¨¡å‹æ–‡ä»¶ï¼ˆé€šå¸¸å‡ ç™¾MBç”šè‡³æ›´å¤§ï¼‰ã€‚
æ„å»ºç¥ç»ç½‘ç»œ: å®ƒåœ¨å†…å­˜ï¼ˆæˆ–GPUæ˜¾å­˜ï¼Œå› ä¸ºä½ è®¾ç½®äº†device="cuda:0"ï¼‰ä¸­æ„å»ºèµ·ä¸€ä¸ªå¤æ‚çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ç»“æ„ã€‚
åˆå§‹åŒ–ç»„ä»¶: å®ƒè¿˜åŠ è½½äº†è¾…åŠ©æ¨¡å‹ï¼Œæ¯”å¦‚ç”¨äºæ–­å¥çš„VADæ¨¡å‹(fsmn-vad)ã€‚
æ‰€ä»¥ï¼Œè¿™è¡Œä»£ç çš„èƒŒåæ˜¯å¤§é‡çš„æœ¬åœ°è®¡ç®—å’Œèµ„æºåŠ è½½ã€‚ä¸€æ—¦åŠ è½½å®Œæˆï¼Œasr_modelå°±æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œå¤‡çš„æœ¬åœ°è¯­éŸ³è¯†åˆ«å¼•æ“ï¼Œ
è°ƒç”¨å®ƒçš„.generate()æ–¹æ³•å°±å¯ä»¥ç›´æ¥å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œå®Œå…¨ä¸éœ€è¦ç½‘ç»œã€‚
    """

    def __init__(self, app):
        self.app = app

    def transcribe(self, audio_file: str, high_priority: bool):
        """å°†æŒ‡å®šçš„éŸ³é¢‘æ–‡ä»¶å‘é€ç»™ASRæ¨¡å‹è¿›è¡Œè½¬å½•ã€‚"""
        if not asr_model:
            self.app.update_status("é”™è¯¯: ASRæ¨¡å‹æœªåŠ è½½")
            if os.path.exists(audio_file): os.remove(audio_file)
            return
        
        self.app.update_status("æ­£åœ¨è½¬å½•è¯­éŸ³...")
        try:
            if not os.path.exists(audio_file) or os.path.getsize(audio_file) == 0:
                raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶æ— æ•ˆ: {audio_file}")
                
            res = asr_model.generate(input=audio_file, cache={})
            
            if res and "text" in res[0]:
                raw_text = res[0]["text"]
                extracted_text = extract_language_emotion_content(raw_text)
                
                if extracted_text and len(extracted_text.strip()) > 1:
                    # å°†ç»“æœå›è°ƒç»™ä¸»åº”ç”¨å¤„ç†
                    self.app.handle_transcription_result(extracted_text, high_priority)
                else:
                    self.app.update_status("æ£€æµ‹åˆ°å™ªéŸ³æˆ–æ— æ„ä¹‰è¯­éŸ³ï¼Œå·²å¿½ç•¥")
            else:
                self.app.update_status("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³")

        except Exception as e:
            print(f"è½¬å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            self.app.update_status("è½¬å½•å¤±è´¥")
        finally:
            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
            if os.path.exists(audio_file) and audio_file.startswith("speech_"):
                try:
                    os.remove(audio_file)
                except Exception as e:
                    print(f"åˆ é™¤ä¸´æ—¶è¯­éŸ³æ–‡ä»¶æ—¶å‡ºé”™: {e}")

========================================
FILE_PATH: ai_assistant\core\decision_maker.py
========================================

# ai_assistant/core/decision_maker.py

import time
import math
import numpy as np
from ai_assistant.utils import config

class DecisionMaker:
    """
    [Phase X æœ€ç»ˆç‰ˆ] è‡ªé€‚åº”æ•ˆç”¨å†³ç­–å¼•æ“ (Adaptive Utility Decision Engine)ã€‚
    é›†æˆç‰¹å¾ï¼š
    1. ç¡®å®šæ€§æ•ˆç”¨è®¡ç®— (Deterministic Utility)
    2. åœ¨çº¿åé¦ˆå­¦ä¹  (Online Feedback Learning) - æ ¹æ®ç”¨æˆ·ååº”åŠ¨æ€è°ƒæ•´ç­–ç•¥æƒé‡
    """
    def __init__(self):
        self.last_action_time = {} 
        self.last_state_snapshot = None # è®°å½•ä¸Šä¸€æ¬¡å†³ç­–æ—¶çš„çŠ¶æ€ï¼Œç”¨äºå¯¹æ¯”
        self.last_action_taken = None   # è®°å½•ä¸Šä¸€æ¬¡é‡‡å–çš„åŠ¨ä½œ
        
        # åŠ¨æ€æƒé‡è°ƒæ•´çŸ©é˜µ (Memory)
        # ç”¨äºå­˜å‚¨å¯¹ REWARD_CONFIG çš„ä¿®æ­£å€¼ (Delta)
        # æ ¼å¼: { (state_key, action): delta_score }
        self.adaptive_weights = {} 

    def evaluate_action_value(self, current_state: dict, behavior: str) -> str:
        """
        Step 1: è¯„ä¼°å¹¶å†³ç­–
        """
        # --- [æ–°å¢] åé¦ˆå­¦ä¹ ç¯èŠ‚ ---
        # åœ¨åšæ–°å†³ç­–å‰ï¼Œå…ˆçœ‹ä¸€çœ¼ä¸Šä¸€æ¬¡å†³ç­–çš„æ•ˆæœå¥½ä¸å¥½
        if self.last_state_snapshot and self.last_action_taken != config.ACTIONS.WAIT:
            self._learn_from_feedback(self.last_state_snapshot, current_state, self.last_action_taken)

        # --- å†³ç­–ç¯èŠ‚ (ä¿æŒä¸å˜ï¼Œä½†å¢åŠ äº† adaptive_weights çš„è€ƒé‡) ---
        ui_emotion = current_state.get('ui_emotion')
        complex_e = current_state.get('complex_emotion')
        arousal = current_state.get('arousal', 0.0)
        
        state_key = "ç„¦è™‘" if (complex_e and "ç„¦è™‘" in complex_e) else ui_emotion
        
        best_action = config.ACTIONS.WAIT
        max_score = -float('inf') 
        
        print(f"\n[System 2] å†³ç­–è®¡ç®— | State:{state_key} | Arousal:{arousal:.2f}")
        
        for action in [config.ACTIONS.WAIT, config.ACTIONS.LIGHT_CARE, config.ACTIONS.DEEP_INTERVENTION]:
            score = self._calculate_utility(state_key, action, behavior, arousal)
            
            # æ‰“å°åŒ…å«å­¦ä¹ æƒé‡çš„å¾—åˆ†
            learned_bias = self.adaptive_weights.get((state_key, action), 0.0)
            print(f"   - {action:4s} | Base:{score-learned_bias:.1f} + Learn:{learned_bias:.1f} = {score:.2f}")
            
            if score > max_score:
                max_score = score
                best_action = action
                
        # æ›´æ–°çŠ¶æ€å¿«ç…§
        if best_action != config.ACTIONS.WAIT:
            self.last_action_time[best_action] = time.time()
            self.last_state_snapshot = current_state # å­˜ä¸‹å½“å‰çŠ¶æ€ï¼Œç­‰ä¸‹ä¸€è½®æ¥çœ‹çœ‹æœ‰æ²¡æœ‰å˜å¥½
            self.last_action_taken = best_action
        else:
            # å¦‚æœæ˜¯é™é»˜ï¼Œé€šå¸¸ä¸è¿›è¡Œå¼ºåé¦ˆå­¦ä¹ ï¼Œä»¥å…å™ªéŸ³å¹²æ‰°
            self.last_state_snapshot = None
            
        return best_action

    def _calculate_utility(self, emotion_key: str, action: str, behavior: str, arousal: float) -> float:
        """
        [å­¦æœ¯é‡æ„] æ•ˆç”¨å‡½æ•° U(a) = R_static + R_adaptive + R_bias - C - P_decay
        """
        # 1. R_static: é™æ€å…ˆéªŒçŸ¥è¯† (Expert Knowledge)
        r_static = config.REWARD_CONFIG.get((emotion_key, action), config.DEFAULT_REWARD)
        
        # 2. [æ–°å¢] R_adaptive: è‡ªé€‚åº”å­¦ä¹ æƒé‡ (Learned Preference)
        # è¿™æ˜¯ Agent é€šè¿‡äº¤äº’"å­¦"åˆ°çš„åå¥½
        r_adaptive = self.adaptive_weights.get((emotion_key, action), 0.0)

        # è¡Œä¸ºä¸Šä¸‹æ–‡ä¿®æ­£
        if "ä¸“æ³¨" in behavior and action == config.ACTIONS.DEEP_INTERVENTION:
            r_static -= 50.0 

        # 3. R_arousal_bias: å”¤é†’åº¦åå·®
        r_arousal = 0.0
        if action == config.ACTIONS.DEEP_INTERVENTION:
            if arousal > 6.0: 
                r_arousal = (arousal - 6.0) * 2.0

        # 4. C_cost: å›ºæœ‰æˆæœ¬
        c_cost = 0.0
        if action != config.ACTIONS.WAIT:
            c_cost = 2.0 

        # 5. P_time_decay: æ—¶é—´è¡°å‡
        p_penalty = 0.0
        last_time = self.last_action_time.get(action, 0)
        if last_time > 0:
            time_diff = time.time() - last_time
            if action == config.ACTIONS.LIGHT_CARE:
                p_penalty = 50.0 * math.exp(-0.05 * time_diff)
            elif action == config.ACTIONS.DEEP_INTERVENTION:
                p_penalty = 100.0 * math.exp(-0.01 * time_diff)

        # === æ€»æ•ˆç”¨ ===
        return r_static + r_adaptive + r_arousal - c_cost - p_penalty

    def _learn_from_feedback(self, prev_state: dict, curr_state: dict, action: str):
        """
        [å­¦æœ¯é‡æ„] åŸºäºæƒ…æ„Ÿåé¦ˆçš„å‚æ•°æ›´æ–° (Delta Learning Rule).
        å¦‚æœä»‹å…¥åç”¨æˆ·çŠ¶æ€æ”¹å–„ -> å¢åŠ æƒé‡
        å¦‚æœä»‹å…¥åç”¨æˆ·çŠ¶æ€æ¶åŒ– -> å‡å°‘æƒé‡
        """
        # æå–å…³é”®æŒ‡æ ‡
        prev_arousal = prev_state.get('arousal', 0.0)
        curr_arousal = curr_state.get('arousal', 0.0)
        
        # ç®€å•çš„æ”¹å–„æŒ‡æ ‡: å”¤é†’åº¦(å‹åŠ›)æ˜¯å¦ä¸‹é™?
        # Delta < 0 è¡¨ç¤ºå‹åŠ›å‡è½» (Good)
        # Delta > 0 è¡¨ç¤ºå‹åŠ›ä¸Šå‡ (Bad)
        delta_arousal = curr_arousal - prev_arousal
        
        # å®šä¹‰å­¦ä¹ ç‡ (Learning Rate)
        alpha = 0.5 
        
        # ç¡®å®š Reward Signal (åé¦ˆä¿¡å·)
        # å¦‚æœå‹åŠ›ä¸‹é™äº†ï¼Œç»™äºˆæ­£åé¦ˆï¼›åä¹‹è´Ÿåé¦ˆ
        # æˆ‘ä»¬å¸Œæœ› delta_arousal è¶Šè´Ÿè¶Šå¥½ -> reward = -delta
        feedback_signal = -delta_arousal 
        
        # çŠ¶æ€é”®
        ui_emotion = prev_state.get('ui_emotion')
        complex_e = prev_state.get('complex_emotion')
        state_key = "ç„¦è™‘" if (complex_e and "ç„¦è™‘" in complex_e) else ui_emotion
        
        key = (state_key, action)
        current_weight = self.adaptive_weights.get(key, 0.0)
        
        # æ›´æ–°å…¬å¼: W_new = W_old + alpha * (Feedback)
        # ä¸ºäº†é˜²æ­¢æƒé‡æ— é™å‘æ•£ï¼Œå¯ä»¥åŠ ä¸€ä¸ªè¡°å‡é¡¹æˆ–è€…é™åˆ¶èŒƒå›´ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        if abs(feedback_signal) > 0.1: # åªæœ‰æ˜¾è‘—å˜åŒ–æ‰å­¦ä¹ ï¼Œå¿½ç•¥å™ªéŸ³
            new_weight = current_weight + alpha * feedback_signal
            
            # é™åˆ¶å­¦ä¹ èŒƒå›´ (-20 åˆ° +20)ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            new_weight = max(-20.0, min(20.0, new_weight))
            
            self.adaptive_weights[key] = new_weight
            
            print(f"[è‡ªé€‚åº”å­¦ä¹ ] Action '{action}' åï¼Œå‹åŠ›å˜åŒ– {delta_arousal:.2f}ã€‚æƒé‡ä¿®æ­£: {current_weight:.2f} -> {new_weight:.2f}")

========================================
FILE_PATH: ai_assistant\core\emotion_engine.py
========================================

# ai_assistant/core/emotion_engine.py

import numpy as np
import math
from ai_assistant.utils import config

class EmotionEngine:
    """
    [å­¦æœ¯é‡æ„ç‰ˆ] æƒ…æ„Ÿè®¡ç®—å¼•æ“ã€‚
    æ ¸å¿ƒç®—æ³•ï¼š
    1. çŠ¶æ€ç©ºé—´ï¼š8ç»´ Plutchik å‘é‡ç©ºé—´ã€‚
    2. å¹³æ»‘ç®—æ³•ï¼šæŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA)ã€‚
    3. å¤æ‚æƒ…ç»ªï¼šåŸºäºæ¨¡ç³Šé€»è¾‘ (Fuzzy Logic) çš„åˆæˆã€‚
    4. UIæ˜ å°„ï¼šåŸºäºä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity) çš„æœ€è¿‘é‚»åˆ†ç±»ã€‚
    """
    def __init__(self):
        # å†…éƒ¨çŠ¶æ€ç»´æŒä¸º numpy æ•°ç»„ï¼Œæ–¹ä¾¿çº¿æ€§ä»£æ•°è®¡ç®—
        self.current_vector = np.zeros(8, dtype=float) 
        self.labels = config.PLUTCHIK_EMOTIONS
        
        # å¤åˆæƒ…ç»ªçš„æ¨¡ç³Šéš¶å±åº¦è®°å½•
        self.complex_emotions_fuzzy = {} 

    def update(self, new_observation: dict) -> np.ndarray:
        """
        [å­¦æœ¯é‡æ„] çŠ¶æ€æ›´æ–°æ–¹ç¨‹ã€‚
        ç»“åˆäº† è§‚æµ‹è¾“å…¥(Input)ã€æƒ¯æ€§(Inertia) å’Œ ç¨³æ€è¡°å‡(Homeostasis)ã€‚
        
        System Equation:
        S_t = [ alpha * S_{t-1} + (1 - alpha) * O_t ] * (1 - decay)
        
        å…¶ä¸­:
        - alpha (Inertia): ç”±äººæ ¼ç‰¹è´¨ Neuroticism å†³å®š
        - decay (Homeostasis): ç”±äººæ ¼ç‰¹è´¨ Extraversion å†³å®š
        """
        # 1. è§‚æµ‹å‘é‡åŒ–
        obs_vector = np.array([new_observation.get(label, 0.0) for label in self.labels])
        
        # 2. æƒ¯æ€§æ›´æ–° (EMA)
        alpha = config.EMOTION_INERTIA
        smoothed_input = self.current_vector * alpha + obs_vector * (1.0 - alpha)
        
        # 3. [æ–°å¢] ç¨³æ€è°ƒèŠ‚ (Homeostatic Regulation)
        # æ¨¡æ‹Ÿç”Ÿç‰©æƒ…ç»ªçš„è‡ªç„¶æ¶ˆé€€/èƒ½é‡è€—æ•£
        # å¦‚æœæ²¡æœ‰æŒç»­çš„å¤–éƒ¨åˆºæ¿€ O_tï¼ŒS_t æœ€ç»ˆä¼šæ”¶æ•›åˆ° 0 (å¹³é™)
        decay_rate = config.HOMEOSTATIC_DECAY
        
        # ç¡®ä¿ä¸ä¼šè¡°å‡æˆè´Ÿæ•°ï¼ˆè™½ç„¶ç†è®ºä¸Šä¸ä¼šï¼Œä½†å·¥ç¨‹ä¸Šåšä¸ªä¿æŠ¤ï¼‰
        self.current_vector = smoothed_input * (1.0 - decay_rate)
        
        # æå°å€¼æˆªæ–­ (é˜²æ­¢æµ®ç‚¹æ•°æ‹–å°¾)
        self.current_vector[self.current_vector < 0.01] = 0.0
        
        return self.current_vector

    def get_arousal_level(self) -> float:
        """
        è®¡ç®— L2 èŒƒæ•° (æ¬§å‡ é‡Œå¾—èŒƒæ•°) ä½œä¸ºå”¤é†’åº¦ã€‚
        """
        return float(np.linalg.norm(self.current_vector))

    def compute_complex_emotions(self) -> str:
        """
        [ç®—æ³•æ ¸å¿ƒ] ä½¿ç”¨æ¨¡ç³Šé€»è¾‘ (Fuzzy Logic) è®¡ç®—å¤åˆæƒ…ç»ªã€‚
        ä¸ä½¿ç”¨ if > 5 è¿™ç§ç¡¬é˜ˆå€¼ï¼Œè€Œæ˜¯è®¡ç®—éš¶å±åº¦ã€‚
        
        Formula:
        mu_High(x) = 1 / (1 + exp(-k * (x - x0)))  [Sigmoid éš¶å±åº¦å‡½æ•°]
        mu_Compound(A, B) = min(mu_High(A), mu_High(B)) [Zadehç®—å­ / T-norm]
        """
        # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—å•ç»´åº¦çš„éš¶å±åº¦ (Membership Degree)
        def fuzzy_membership(value):
            k = config.FUZZY_SIGMOID_SLOPE
            x0 = config.FUZZY_SIGMOID_OFFSET
            # Sigmoid å‡½æ•°
            return 1.0 / (1.0 + np.exp(-k * (value - x0)))

        # è·å–å½“å‰å‘é‡çš„å­—å…¸å½¢å¼
        current_dict = dict(zip(self.labels, self.current_vector))
        
        # å®šä¹‰å¤åˆè§„åˆ™ (å¯ä»¥æ‰©å±•)
        # è§„åˆ™: Key = (Component A, Component B), Value = Name
        rules = [
            (("å–œæ‚¦", "ä¿¡ä»»"), "çˆ± (Love)"),
            (("ææƒ§", "æœŸå¾…"), "ç„¦è™‘ (Anxiety)"),
            (("æœŸå¾…", "å–œæ‚¦"), "ä¹è§‚ (Optimism)"),
            (("æƒŠè®¶", "æ‚²ä¼¤"), "å¤±æœ› (Disapproval)")
        ]
        
        max_degree = 0.0
        dominant_complex = None
        
        self.complex_emotions_fuzzy = {}

        for (emo_a, emo_b), name in rules:
            val_a = current_dict.get(emo_a, 0)
            val_b = current_dict.get(emo_b, 0)
            
            # è®¡ç®—å„è‡ªçš„"é«˜"éš¶å±åº¦
            mu_a = fuzzy_membership(val_a)
            mu_b = fuzzy_membership(val_b)
            
            # è®¡ç®—å¤åˆæƒ…ç»ªçš„éš¶å±åº¦ (å–äº¤é›†ï¼Œå³ Min)
            mu_complex = min(mu_a, mu_b)
            
            self.complex_emotions_fuzzy[name] = mu_complex
            
            # è®°å½•æœ€æ˜¾è‘—çš„å¤åˆæƒ…ç»ª
            if mu_complex > max_degree:
                max_degree = mu_complex
                dominant_complex = name
        
        # åªæœ‰å½“éš¶å±åº¦è¶…è¿‡ä¸€å®šç½®ä¿¡åº¦ (æ¯”å¦‚ 0.6) æ—¶æ‰è¿”å›ï¼Œå¦åˆ™è®¤ä¸ºæ²¡æœ‰æ˜¾è‘—å¤åˆæƒ…ç»ª
        if max_degree > 0.6:
            return dominant_complex
        return None

    def get_ui_emotion_by_similarity(self) -> str:
        """
        [ç®—æ³•æ ¸å¿ƒ] ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å†³å®š UI å±•ç¤ºã€‚
        è®¡ç®—å½“å‰å‘é‡ä¸æ‰€æœ‰ UI è´¨å¿ƒå‘é‡çš„ç›¸ä¼¼åº¦ï¼Œå–æœ€å¤§è€…ã€‚
        """
        max_sim = -1.0
        best_ui = "å¹³é™" # é»˜è®¤
        
        # é˜²æ­¢é™¤ä»¥é›¶
        norm_current = np.linalg.norm(self.current_vector)
        if norm_current < 0.1:
            return "å¹³é™"

        for ui_name, centroid in config.UI_CENTROIDS.items():
            norm_centroid = np.linalg.norm(centroid)
            if norm_centroid == 0: continue
            
            # ä½™å¼¦ç›¸ä¼¼åº¦å…¬å¼: (A . B) / (||A|| * ||B||)
            similarity = np.dot(self.current_vector, centroid) / (norm_current * norm_centroid)
            
            if similarity > max_sim:
                max_sim = similarity
                best_ui = ui_name
                
        return best_ui
    
    def get_current_state_dict(self):
        """è¿”å›å­—å…¸æ ¼å¼ä¾›æ—¥å¿—ä½¿ç”¨"""
        return dict(zip(self.labels, self.current_vector))








========================================
FILE_PATH: ai_assistant\core\webcam_handler.py
========================================

# ai_assistant/core/webcam_handler.py
import cv2
import time
import io
import threading
from PIL import Image
from datetime import datetime
import logging
import oss2
import json


# ä»æˆ‘ä»¬è‡ªå·±çš„åŒ…é‡Œå¯¼å…¥æ‰€éœ€æ¨¡å—
from ai_assistant.core.api_clients import qwen_client, oss_bucket
from ai_assistant.utils.helpers import extract_behavior_type, extract_emotion_type
#è¿™ä¸ªå¯¼å…¥å¯èƒ½æ˜¯ä¸éœ€è¦çš„ï¼éœ€è¦æ³¨æ„åˆ é™¤
from ai_assistant.ui.camera_window import CameraWindow
from ai_assistant.utils import config
from ai_assistant.core.emotion_engine import EmotionEngine

from ai_assistant.utils.helpers import parse_model_response

class WebcamHandler:
    """
    å¤„ç†æ‘„åƒå¤´æ•è·ã€å›¾åƒåˆ†æå’Œä¸ä¸»åº”ç”¨é€šä¿¡çš„æ ¸å¿ƒç±»ã€‚
    å®ƒä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„å¼•æ“ï¼Œé€šè¿‡å›è°ƒå‡½æ•°ä¸ä¸»åº”ç”¨è§£è€¦ã€‚
    """
    def __init__(self, app):
        """
        åˆå§‹åŒ–WebcamHandlerã€‚
        Args:
            app: ä¸»åº”ç”¨çš„å®ä¾‹ã€‚å®ƒå¿…é¡»å®ç° handle_analysis_result å’Œ update_status æ–¹æ³•ã€‚
        """
        self.app = app
        self.running = False
        self.paused = False
        self.processing = False
        self.cap = None
        self.webcam_thread = None
        self.last_webcam_image = None
        self.camera_window = None
        # [Phase 1.2 æ–°å¢] åˆå§‹åŒ–æƒ…æ„Ÿå¼•æ“
        self.emotion_engine = EmotionEngine()

    def start(self) -> bool:
        """å¯åŠ¨æ‘„åƒå¤´æ•è·è¿›ç¨‹ï¼Œå¹¶å¼€å§‹åå°åˆ†æå¾ªç¯ã€‚"""
        if self.running:
            return True
        try:
            # å°è¯•æ‰“å¼€é»˜è®¤æ‘„åƒå¤´ (ID=0)
            self.cap = cv2.VideoCapture(0)
            if not self.cap.isOpened():
                self.app.update_status("é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                return False
            
            self.running = True
            self.create_camera_window()
            
            # å¯åŠ¨ä¸€ä¸ªåå°å®ˆæŠ¤çº¿ç¨‹æ¥æŒç»­è¯»å–æ‘„åƒå¤´å¸§
            self.webcam_thread = threading.Thread(target=self._process_webcam_frames)
            self.webcam_thread.daemon = True
            self.webcam_thread.start()
            
            # å»¶è¿Ÿ2ç§’åï¼Œå¯åŠ¨ç¬¬ä¸€æ¬¡å›¾åƒåˆ†æ
            self.app.after(2000, self.trigger_next_capture)
            return True
        except Exception as e:
            self.app.update_status(f"å¯åŠ¨æ‘„åƒå¤´å‡ºé”™: {e}")
            return False

    def stop(self):
        """å®‰å…¨åœ°åœæ­¢æ‰€æœ‰çº¿ç¨‹å’Œæ‘„åƒå¤´ç¡¬ä»¶ã€‚"""
        self.running = False
        if self.webcam_thread and self.webcam_thread.is_alive():
            self.webcam_thread.join(timeout=1.0) # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.cap:
            self.cap.release() # é‡Šæ”¾æ‘„åƒå¤´èµ„æº
        if self.camera_window and self.camera_window.winfo_exists():
            self.camera_window.destroy()
        self.camera_window = None
        print("WebcamHandler å·²æˆåŠŸåœæ­¢ã€‚")

    def _process_webcam_frames(self):
        """[åå°çº¿ç¨‹] æŒç»­ä»æ‘„åƒå¤´è¯»å–å¸§å¹¶æ›´æ–°UIçª—å£ã€‚"""
        last_ui_update_time = 0
        ui_update_interval = 0.05  # 20 FPS

        while self.running:
            try:
                ret, frame = self.cap.read()
                if not ret:
                    time.sleep(0.1)
                    continue
                
                # è½¬æ¢é¢œè‰²æ ¼å¼ (OpenCV: BGR -> PIL: RGB)
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                img = Image.fromarray(frame_rgb)
                self.last_webcam_image = img # ç¼“å­˜æœ€æ–°ä¸€å¸§
                
                # æ§åˆ¶UIæ›´æ–°é¢‘ç‡ï¼Œé¿å…è¿‡åº¦æ¶ˆè€—èµ„æº
                current_time = time.time()
                if self.camera_window and not self.camera_window.is_closed and \
                   (current_time - last_ui_update_time) >= ui_update_interval:
                    self.camera_window.update_frame(img)
                    last_ui_update_time = current_time
                
                time.sleep(0.01) # ç¨å¾®è®©å‡ºCPU
            except Exception as e:
                print(f"æ‘„åƒå¤´å¤„ç†å¾ªç¯é”™è¯¯: {e}")
                time.sleep(1)

    def trigger_next_capture(self):
        """
        [ä¸»çº¿ç¨‹è°ƒç”¨] è§¦å‘ä¸‹ä¸€æ¬¡å›¾åƒåˆ†æçš„å…¥å£ç‚¹ã€‚
        æ£€æŸ¥æ‰€æœ‰çŠ¶æ€æ ‡å¿—ï¼Œç¡®ä¿ä¸ä¼šé‡å¤æˆ–åœ¨ä¸å½“çš„æ—¶æœºæ‰§è¡Œåˆ†æã€‚
        """
        if self.running and not self.paused and not self.processing:
            print(f"[{time.strftime('%H:%M:%S')}] è§¦å‘æ–°ä¸€è½®å›¾åƒåˆ†æ")
            
            # å°†è€—æ—¶çš„åˆ†æä»»åŠ¡æ”¾å…¥ä¸€ä¸ªæ–°çº¿ç¨‹ï¼Œä»¥é˜²é˜»å¡UI
            analysis_thread = threading.Thread(target=self._capture_and_analyze_pipeline)
            analysis_thread.daemon = True
            analysis_thread.start()





# ai_assistant/core/webcam_handler.py

    def _capture_and_analyze_pipeline(self):
        """[åˆ†æçº¿ç¨‹] æ‰§è¡Œå®Œæ•´çš„â€œæ•è·->ä¸Šä¼ ->åˆ†æ->å›è°ƒâ€æµç¨‹ã€‚"""
        self.processing = True
        try:
            self.app.update_status("æ­£åœ¨æ•æ‰å›¾åƒ...")
            screenshots, current_screenshot = self._capture_screenshots()
            if not screenshots:
                raise ValueError("æœªèƒ½æ•è·æœ‰æ•ˆæˆªå›¾")
                
            self.app.update_status("æ­£åœ¨ä¸Šä¼ å›¾åƒ...")
            screenshot_urls = self._upload_screenshots(screenshots)
            if not screenshot_urls:
                raise ValueError("ä¸Šä¼ æˆªå›¾å¤±è´¥")

            self.app.update_status("æ­£åœ¨åˆ†æå›¾åƒ...")
            
            # 1. è·å–å¹¶è§£æ AI åŸå§‹æ•°æ®
            raw_response = self._get_image_analysis(screenshot_urls)
            if not raw_response:
                raise ValueError("å›¾åƒåˆ†æè¿”å›ç©ºç»“æœ")
            
            # å¼•å…¥è§£æå™¨ (é˜²æ­¢æœªå¯¼å…¥æŠ¥é”™)
            from ai_assistant.utils.helpers import parse_model_response
            parsed_data = parse_model_response(raw_response)
            
            # 2. æå–åŸºç¡€ä¿¡æ¯
            behavior_info = parsed_data.get("behavior", {})
            behavior_num = str(behavior_info.get("id", "0"))
            behavior_desc = behavior_info.get("description", "æœªè¯†åˆ«")
            analysis_text = parsed_data.get("analysis", "æ— è¯¦ç»†åˆ†æ")
            
            # 3. [å…³é”®ä¿®æ­£] æ³¨å…¥æƒ…æ„Ÿå¼•æ“ (Academic Engine Upgrade)
            raw_emotion_vector = parsed_data.get("emotions", config.DEFAULT_EMOTION_VECTOR)
            
            # A. æ›´æ–°å¼•æ“å†…éƒ¨çŠ¶æ€ (EMA å¹³æ»‘)
            self.emotion_engine.update(raw_emotion_vector)
            
            # B. è®¡ç®—å¤åˆæƒ…ç»ª (ä½¿ç”¨æ–°æ–¹æ³•å: compute_complex_emotions)
            # æ—§ä»£ç è°ƒç”¨çš„æ˜¯ get_complex_labelï¼Œè¿™é‡Œå¿…é¡»æ”¹ï¼
            complex_label = self.emotion_engine.compute_complex_emotions()
            
            # C. è·å– UI æƒ…ç»ª (ä½¿ç”¨æ–°æ–¹æ³•å: get_ui_emotion_by_similarity)
            # æ—§ä»£ç æ˜¯æŸ¥è¡¨ï¼Œç°åœ¨æ˜¯è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            ui_emotion = self.emotion_engine.get_ui_emotion_by_similarity()
            
            # D. è·å–ç”¨äºæ—¥å¿—çš„å­—å…¸æ•°æ®
            smoothed_vector_dict = self.emotion_engine.get_current_state_dict()

            # 4. æ—¥å¿—è®°å½•
            timestamp = datetime.now()
            log_msg = f"ANALYSIS | è¡Œä¸º:{behavior_desc} | UI:{ui_emotion} | å¤åˆ:{complex_label} | å‘é‡:{json.dumps(smoothed_vector_dict, ensure_ascii=False)}"
            logging.info(log_msg)
            
            # 5. å›è°ƒä¸»åº”ç”¨
            self.app.handle_analysis_result(
                timestamp, 
                analysis_text, 
                behavior_num, 
                behavior_desc, 
                ui_emotion, 
                current_screenshot,
                complex_emotion=complex_label, # ä¼ å‚
                emotion_vector=smoothed_vector_dict # ä¼ å‚
            )

        except Exception as e:
            error_msg = f"æ•è·ä¸åˆ†ææµç¨‹å‡ºé”™: {e}"
            print(error_msg)
            import traceback
            traceback.print_exc() # æ‰“å°è¯¦ç»†æŠ¥é”™æ–¹ä¾¿è°ƒè¯•
            self.app.update_status(error_msg)
        finally:
            self.processing = False
            delay_ms = int(config.ANALYSIS_INTERVAL_SECONDS * 1000)
            self.app.after(delay_ms, self.trigger_next_capture)







    def _capture_screenshots(self, num_shots=4, interval=0.1) -> tuple:
        """[åˆ†æçº¿ç¨‹] ä»æ‘„åƒå¤´æ•è·å¤šå¼ è¿ç»­æˆªå›¾ä»¥æ¨¡æ‹ŸåŠ¨æ€ä¿¡æ¯ã€‚"""
        screenshots = []
        # --- å…³é”®ä¿®æ­£ï¼šç›´æ¥ä»æ‘„åƒå¤´ç¡¬ä»¶è¯»å–ï¼Œç¡®ä¿æ¯ä¸€å¸§éƒ½æ˜¯æ–°çš„ ---
        for _ in range(num_shots):
            if not self.cap or not self.cap.isOpened():
                break # å¦‚æœæ‘„åƒå¤´å…³é—­äº†ï¼Œåˆ™åœæ­¢æ•è·
            ret, frame = self.cap.read()
            if ret:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                screenshots.append(Image.fromarray(frame_rgb))
            time.sleep(interval)
        
        return screenshots, self.last_webcam_image

    def _upload_screenshots(self, screenshots: list) -> list:
        """[åˆ†æçº¿ç¨‹] å°†æˆªå›¾åˆ—è¡¨ä¸Šä¼ åˆ°OSSå¹¶è¿”å›URLsã€‚"""
        oss_urls = []
        for i, img in enumerate(screenshots):
            # å°†PIL Imageå¯¹è±¡è½¬æ¢ä¸ºå†…å­˜ä¸­çš„JPEGå­—èŠ‚æµ
            buffer = io.BytesIO()
            img.save(buffer, format='JPEG')
            buffer.seek(0)
            
            object_key = f"screenshots/{int(time.time())}_{i}.jpg"
            result = oss_bucket.put_object(object_key, buffer)
            
            if result.status == 200:
                url = f"https://{config.OSS_BUCKET}.{config.OSS_ENDPOINT}/{object_key}"
                oss_urls.append(url)
        return oss_urls


# url = f"https://{config.OSS_BUCKET}.{config.OSS_ENDPOINT}/{object_key}"
# è¿™æ˜¯åœ¨åšä»€ä¹ˆï¼Ÿ
# è¿™è¡Œä»£ç æ˜¯æ ¹æ®OSSçš„è§„åˆ™ï¼Œæ‹¼æ¥å‡ºä¸€ä¸ªå®Œæ•´çš„ã€å¯ä»¥é€šè¿‡äº’è”ç½‘è®¿é—®çš„å…¬å¼€URLåœ°å€ã€‚

    def _get_image_analysis(self, image_urls: list) -> str:
        """[åˆ†æçº¿ç¨‹] è°ƒç”¨Qwen-VL APIåˆ†æå›¾åƒï¼ŒåŒæ—¶è·å–è¡Œä¸ºå’Œæƒ…æ„Ÿã€‚"""
        # å®šä¹‰ä¸¥æ ¼çš„è¾“å‡ºç»“æ„
        json_template = {
            "behavior": {"id": "è¡Œä¸ºç¼–å·(1-7)", "description": "æè¿°"},
            "emotions": {k: "0-10åˆ†" for k in config.PLUTCHIK_EMOTIONS}, # åŠ¨æ€å¼•ç”¨é…ç½®
            "analysis": "ç®€çŸ­çš„å¿ƒç†å­¦è§‚å¯Ÿæ€»ç»“(50å­—å†…)"
        }

        system_prompt = (
            "ä½ æ˜¯ä¸€ä½ç²¾é€šâ€˜æƒ…æ„Ÿè®¡ç®—(Affective Computing)â€™ä¸â€˜é¢éƒ¨åŠ¨ä½œç¼–ç ç³»ç»Ÿ(FACS)â€™çš„è§†è§‰åˆ†æä¸“å®¶ã€‚\n"
            "ä½ çš„ä»»åŠ¡æ˜¯åŸºäºè¾“å…¥ç”»é¢ï¼Œè¿›è¡Œé«˜ç²¾åº¦çš„è¡Œä¸ºè¯†åˆ«ä¸æƒ…æ„Ÿé‡åŒ–åˆ†æã€‚\n\n"
            "### ç†è®ºåŸºç¡€\n"
            "1. **Plutchik æƒ…æ„Ÿè½®æ¨¡å‹**ï¼šå°†æƒ…æ„Ÿè§£æä¸º8ä¸ªæ­£äº¤ç»´åº¦çš„æ··åˆå‘é‡ã€‚\n"
            "2. **FACS (Facial Action Coding System)**ï¼šé€šè¿‡çœ‰æ¯›ã€çœ¼ç›ã€å˜´è§’çš„è‚Œè‚‰è¿åŠ¨å•å…ƒ(AU)æ¥æ¨æ–­æƒ…ç»ªã€‚\n\n"
            "### æ ¸å¿ƒåˆ†ææ­¥éª¤ (Chain of Thought)\n"
            "è¯·åœ¨å†…éƒ¨è¿›è¡Œä»¥ä¸‹æ€ç»´æ¨æ¼”ï¼Œå¹¶æœ€ç»ˆè¾“å‡ºç»“æœï¼š\n"
            "1. **ç¯å¢ƒä¸è¡Œä¸ºä¸Šä¸‹æ–‡æ‰«æ**ï¼šé€šè¿‡è‚¢ä½“å§¿æ€(Body Pose)å’Œç‰©ä½“äº¤äº’(Object Interaction)ç¡®å®šå½“å‰è¡Œä¸ºã€‚\n"
            "   - è¯†åˆ«æ‰‹éƒ¨åŠ¨ä½œï¼ˆå¦‚æŒæ¯ã€æ•²å‡»é”®ç›˜ã€æ¡æŒæ‰‹æœºï¼‰ã€‚\n"
            "   - è¯†åˆ«å¤´éƒ¨å§¿æ€ï¼ˆå¦‚ä½å¤´ã€åä»°ã€ä¾§å€¾ï¼‰ã€‚\n"
            "2. **é¢éƒ¨å¾®è¡¨æƒ…è§£ç **ï¼š\n"
            "   - è§‚å¯Ÿçœ‰é—´çº¹ï¼ˆAU4ï¼‰ï¼šåˆ¤æ–­æ˜¯å¦å­˜åœ¨ç„¦è™‘ã€ä¸“æ³¨æˆ–æ„¤æ€’ã€‚\n"
            "   - è§‚å¯Ÿçœ¼ç‘å¼€åˆåº¦ï¼ˆAU5/AU7ï¼‰ï¼šåˆ¤æ–­æ˜¯å¦å­˜åœ¨æƒŠææˆ–ç–²æƒ«ã€‚\n"
            "   - è§‚å¯Ÿå˜´è§’æ‹‰ä¼¸ï¼ˆAU12/AU15ï¼‰ï¼šåˆ¤æ–­æ„‰æ‚¦æˆ–æ‚²ä¼¤ã€‚\n"
            "3. **æƒ…æ„Ÿç»´åº¦é‡åŒ–**ï¼šåŸºäºä¸Šè¿°ç‰¹å¾ï¼Œä¸º Plutchik çš„8ä¸ªç»´åº¦æ‰“åˆ† (0-10)ã€‚\n"
            "   - æ³¨æ„ï¼š'æœŸå¾…(Anticipation)' ç»´åº¦é€šå¸¸è¡¨ç°ä¸ºæ³¨æ„åŠ›é«˜åº¦é›†ä¸­ã€èº«ä½“å‰å€¾ã€‚\n"
            "   - æ³¨æ„ï¼š'ä¿¡ä»»(Trust)' ç»´åº¦é€šå¸¸è¡¨ç°ä¸ºé¢éƒ¨è‚Œè‚‰æ”¾æ¾ã€çœ¼ç¥æŸ”å’Œã€‚\n\n"
            "### ä»»åŠ¡è¾“å‡ºå®šä¹‰\n"
            "1. **è¡Œä¸ºåˆ†ç±» (Behavior Classification)**ï¼š\n"
            "   å¿…é¡»ä»ä»¥ä¸‹äº’æ–¥é›†åˆä¸­é€‰æ‹©æœ€ç²¾ç¡®çš„ä¸€é¡¹ï¼š\n"
            "   [1.è®¤çœŸä¸“æ³¨å·¥ä½œ, 2.åƒä¸œè¥¿, 3.ç”¨æ¯å­å–æ°´, 4.å–é¥®æ–™, 5.ç©æ‰‹æœº, 6.ç¡è§‰, 7.å…¶ä»–]\n"
            "2. **æƒ…æ„Ÿé‡åŒ– (Emotion Quantification)**ï¼š\n"
            "   è¾“å‡º JSON æ ¼å¼çš„ 8 ç»´å‘é‡ï¼Œç¦æ­¢å…¨0ï¼Œå¿…é¡»æ•æ‰ç»†å¾®æƒ…ç»ªã€‚\n\n"
            "### è¾“å‡ºæ ¼å¼çº¦æŸ\n"
            "ä¸¥æ ¼è¾“å‡ºæ ‡å‡† JSON å­—ç¬¦ä¸²ï¼Œä¸åŒ…å« Markdown æ ‡è®°ï¼Œä¸åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚"
        )
        
        user_prompt = f"åˆ†æç”»é¢å¹¶ä¸¥æ ¼æŒ‰æ­¤JSONæ ¼å¼è¿”å›ï¼š{json.dumps(json_template, ensure_ascii=False)}"
        
        messages = [
            {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            {
                "role": "user", 
                "content": [
                    {"type": "video", "video": image_urls},
                    {"type": "text", "text": user_prompt}
                ]
            }
        ]
        
        completion = qwen_client.chat.completions.create(
            model="qwen-vl-max",
            messages=messages,
        )
        return completion.choices[0].message.content

    def toggle_pause(self):
        """[ä¸»çº¿ç¨‹è°ƒç”¨] åˆ‡æ¢åˆ†æå¾ªç¯çš„æš‚åœ/æ¢å¤çŠ¶æ€ã€‚"""
        self.paused = not self.paused
        status = "å·²æš‚åœåˆ†æ" if self.paused else "å·²æ¢å¤åˆ†æ"
        self.app.update_status(status)
        # å¦‚æœæ˜¯æ¢å¤ï¼Œåˆ™ç«‹å³å°è¯•è§¦å‘ä¸€æ¬¡åˆ†æ
        if not self.paused:
            self.app.after(500, self.trigger_next_capture)
            




    def toggle_camera_window(self):
        """[ä¸»çº¿ç¨‹è°ƒç”¨] æ˜¾ç¤ºæˆ–éšè—æ‘„åƒå¤´çª—å£ã€‚"""
        if self.camera_window and not self.camera_window.is_closed:
            self.camera_window.on_closing()
        else:
            self.create_camera_window()

    def create_camera_window(self):
        """[ä¸»çº¿ç¨‹è°ƒç”¨] åˆ›å»ºæˆ–æ˜¾ç¤ºæ‘„åƒå¤´çª—å£ã€‚"""
        if not self.camera_window or self.camera_window.is_closed:
            self.camera_window = CameraWindow(self.app)
            self.camera_window.is_closed = False
        else:
            self.camera_window.deiconify() # Toplevelçª—å£éšè—åç”¨deiconifyæ¥æ˜¾ç¤º

        # å°†çª—å£å®šä½åœ¨ä¸»çª—å£ä¸‹æ–¹ï¼Œå¢åŠ ä¸€äº›åç§»ä»¥é˜²é‡å 
        self.app.update() # ç¡®ä¿ä¸»çª—å£ä½ç½®ä¿¡æ¯æ˜¯æœ€æ–°çš„
        main_x = self.app.winfo_x()
        main_y = self.app.winfo_y()
        main_height = self.app.winfo_height()
        self.camera_window.geometry(f"640x480+{main_x}+{main_y + main_height + 40}")
        


========================================
FILE_PATH: ai_assistant\core\__init__.py
========================================



========================================
FILE_PATH: ai_assistant\ui\camera_window.py
========================================

# ai_assistant/ui/camera_window.py

import customtkinter as ctk
from PIL import Image

class CameraWindow(ctk.CTkToplevel):
    """
    ä¸€ä¸ªç‹¬ç«‹çš„ã€å¯é‡å¤ä½¿ç”¨çš„çª—å£ï¼Œç”¨äºæ˜¾ç¤ºæ‘„åƒå¤´ç”»é¢ã€‚
    CTkToplevel æ˜¯ customtkinter ä¸­ç”¨äºåˆ›å»ºé¡¶çº§å­çª—å£çš„ç±»ã€‚
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.title("æ‘„åƒå¤´å®æ—¶ç”»é¢")
        self.geometry("640x480")
        # å½“ç”¨æˆ·ç‚¹å‡»çª—å£çš„å…³é—­æŒ‰é’®æ—¶ï¼Œè°ƒç”¨ self.on_closing æ–¹æ³•
        self.protocol("WM_DELETE_WINDOW", self.on_closing)
        self.configure(fg_color="#1a1a1a")

        # åˆ›å»ºä¸€ä¸ªæ¡†æ¶æ¥å®¹çº³æ‘„åƒå¤´æ ‡ç­¾ï¼Œæ–¹ä¾¿ç®¡ç†è¾¹è·
        self.camera_frame = ctk.CTkFrame(self, fg_color="transparent")
        self.camera_frame.pack(fill="both", expand=True, padx=5, pady=5)

        # ç”¨äºæ˜¾ç¤ºå›¾åƒçš„æ ‡ç­¾
        self.camera_label = ctk.CTkLabel(self.camera_frame, text="æ­£åœ¨å¯åŠ¨æ‘„åƒå¤´...", text_color="white")
        self.camera_label.pack(fill="both", expand=True)

        # çŠ¶æ€å˜é‡
        self.current_image = None  # ç”¨äºä¿æŒå¯¹CTkImageçš„å¼•ç”¨ï¼Œé˜²æ­¢è¢«åƒåœ¾å›æ”¶
        self.is_closed = False

    def update_frame(self, img: Image.Image):
        """
        ç”¨æ–°çš„å›¾åƒå¸§æ›´æ–°çª—å£å†…å®¹ã€‚
        è¿™ä¸ªæ–¹æ³•ä¼šè¢«é«˜é¢‘è°ƒç”¨ï¼ˆå¤§çº¦æ¯ç§’20æ¬¡ï¼‰ã€‚
        """
        # å¦‚æœçª—å£ä¸å­˜åœ¨æˆ–å·²æ ‡è®°ä¸ºå…³é—­ï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œ
        if self.is_closed or not self.winfo_exists():
            return

        try:
            # è°ƒæ•´å›¾åƒå¤§å°ä»¥é€‚åº”çª—å£å½“å‰å°ºå¯¸ï¼Œä¿æŒå®½é«˜æ¯”
            img_resized = img.copy()
            img_resized.thumbnail((self.winfo_width(), self.winfo_height()))

            # å°†PILå›¾åƒè½¬æ¢ä¸ºCustomTkinterå¯ä»¥æ˜¾ç¤ºçš„CTkImageå¯¹è±¡
            ctk_img = ctk.CTkImage(
                light_image=img_resized,
                dark_image=img_resized,
                size=img_resized.size
            )

            # æ›´æ–°æ ‡ç­¾çš„å›¾åƒï¼Œå¹¶æ¸…ç©ºæ–‡æœ¬
            self.camera_label.configure(image=ctk_img, text="")
            self.current_image = ctk_img  # å¿…é¡»ä¿å­˜è¿™ä¸ªå¼•ç”¨ï¼
        except Exception as e:
            # æ•è·å¯èƒ½åœ¨çª—å£å…³é—­ç¬é—´å‘ç”Ÿçš„UIæ›´æ–°é”™è¯¯
            print(f"æ›´æ–°æ‘„åƒå¤´å¸§æ—¶å‡ºé”™ (å¯èƒ½çª—å£å·²å…³é—­): {e}")

    def on_closing(self):
        """
        å¤„ç†çª—å£å…³é—­äº‹ä»¶ã€‚
        æˆ‘ä»¬é€‰æ‹©éšè—(withdraw)è€Œä¸æ˜¯é”€æ¯(destroy)ï¼Œè¿™æ ·çª—å£å¯ä»¥è¢«é‡æ–°æ‰“å¼€ï¼Œ
        è€Œä¸éœ€è¦é‡æ–°åˆ›å»ºå®ä¾‹ï¼Œæé«˜äº†æ•ˆç‡ã€‚
        """
        self.is_closed = True
        self.withdraw()

========================================
FILE_PATH: ai_assistant\ui\charts.py
========================================

# ai_assistant/ui/charts.py

import customtkinter as ctk
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import matplotlib.dates as mdates
from datetime import datetime
import threading
import time
import numpy as np
from ai_assistant.utils import config

class BehaviorVisualizer:
    """
    [Phase 2.5 å¯è§†åŒ–å‡çº§] å¤šæ¨¡æ€æ•°æ®çœ‹æ¿ã€‚
    Layout:
    [ è¡Œä¸ºæ—¶é—´è½´ (Line) ]  [                 ]
    [ ----------------- ]  [ æƒ…æ„Ÿé›·è¾¾ (Radar) ]
    [ æƒ…ç»ªæ—¶é—´è½´ (Line) ]  [                 ]
    """
    
    def __init__(self, parent_frame):
        self.parent_frame = parent_frame
        
        # --- æ˜ å°„å®šä¹‰ ---
        # è¡Œä¸ºæ˜ å°„ (Yè½´ 1-7)
        self.behavior_map = {
            "1": "ä¸“æ³¨", "2": "è¿›é£Ÿ", "3": "å–æ°´", "4": "é¥®æ–™",
            "5": "æ‰‹æœº", "6": "ç¡è§‰", "7": "å…¶ä»–", "0": "æœªçŸ¥"
        }
        # æƒ…ç»ªæ˜ å°„ (Yè½´ 1-8, å¯¹åº” Plutchik ç»´åº¦)
        self.emotion_labels = config.PLUTCHIK_EMOTIONS # ["å–œæ‚¦", "ä¿¡ä»»", ...]
        self.emotion_map_y = {label: i+1 for i, label in enumerate(self.emotion_labels)}
        
        # --- æ•°æ®å­˜å‚¨ ---
        self.behavior_history = [] # [(time, y_val)]
        self.emotion_history = []  # [(time, y_val)]
        self.current_emotion_vector = config.DEFAULT_EMOTION_VECTOR.copy()
        
        self.data_lock = threading.Lock()
        
        self._setup_charts_ui()
        
        self.running = True
        self.update_thread = threading.Thread(target=self._update_charts_loop)
        self.update_thread.daemon = True
        self.update_thread.start()

    def _setup_charts_ui(self):
        """åˆ›å»º 3 ä¸ªå›¾è¡¨çš„å¸ƒå±€"""
        # ä½¿ç”¨ Grid å¸ƒå±€åˆ†å‰²å·¦å³
        charts_frame = ctk.CTkFrame(self.parent_frame, fg_color="transparent")
        charts_frame.pack(fill="both", expand=True)
        charts_frame.grid_columnconfigure(0, weight=3) # å·¦ä¾§ (æŠ˜çº¿å›¾)
        charts_frame.grid_columnconfigure(1, weight=2) # å³ä¾§ (é›·è¾¾å›¾)
        charts_frame.grid_rowconfigure(0, weight=1) # å·¦ä¸Š
        charts_frame.grid_rowconfigure(1, weight=1) # å·¦ä¸‹

        # --- 1. å·¦ä¸Šï¼šè¡Œä¸ºæŠ˜çº¿å›¾ ---
        self.fig_beh = Figure(figsize=(6, 2.5), dpi=100) # é«˜åº¦å‡å°
        self.fig_beh.patch.set_facecolor('#242424')
        self.ax_beh = self.fig_beh.add_subplot(111, facecolor='#242424')
        self.canvas_beh = FigureCanvasTkAgg(self.fig_beh, master=charts_frame)
        self.canvas_beh.get_tk_widget().grid(row=0, column=0, sticky="nsew", padx=(0, 5), pady=(0, 5))

        # --- 2. å·¦ä¸‹ï¼šæƒ…ç»ªæŠ˜çº¿å›¾ ---
        self.fig_emo = Figure(figsize=(6, 2.5), dpi=100)
        self.fig_emo.patch.set_facecolor('#242424')
        self.ax_emo = self.fig_emo.add_subplot(111, facecolor='#242424')
        self.canvas_emo = FigureCanvasTkAgg(self.fig_emo, master=charts_frame)
        self.canvas_emo.get_tk_widget().grid(row=1, column=0, sticky="nsew", padx=(0, 5), pady=(5, 0))

        # --- 3. å³ä¾§ï¼šé›·è¾¾å›¾ (è·¨ä¸¤è¡Œ) ---
        self.fig_radar = Figure(figsize=(4, 5), dpi=100)
        self.fig_radar.patch.set_facecolor('#242424')
        self.ax_radar = self.fig_radar.add_subplot(111, polar=True, facecolor='#242424')
        self.canvas_radar = FigureCanvasTkAgg(self.fig_radar, master=charts_frame)
        self.canvas_radar.get_tk_widget().grid(row=0, column=1, rowspan=2, sticky="nsew", padx=(5, 0), pady=0)
        
        self._redraw_charts()

    def add_behavior_data(self, timestamp: datetime, behavior_num: str):
        """æ·»åŠ è¡Œä¸ºæ•°æ®"""
        with self.data_lock:
            # ç®€å•æ¸…æ´—æ•°æ®
            if behavior_num not in self.behavior_map: behavior_num = "0"
            y_val = int(behavior_num)
            
            self.behavior_history.append((timestamp, y_val))
            if len(self.behavior_history) > 50: self.behavior_history.pop(0)

    def update_emotion_data(self, emotion_vector: dict):
        """æ·»åŠ æƒ…ç»ªæ•°æ® (åŒæ—¶æ›´æ–°é›·è¾¾å›¾å’Œæƒ…ç»ªå†å²)"""
        if not emotion_vector: return
        
        with self.data_lock:
            self.current_emotion_vector = emotion_vector
            
            # æ‰¾å‡ºå½“å‰çš„ä¸»å¯¼æƒ…ç»ªï¼Œç”¨äºç”»æŠ˜çº¿å›¾
            dominant_emotion = max(emotion_vector, key=emotion_vector.get)
            y_val = self.emotion_map_y.get(dominant_emotion, 0)
            
            self.emotion_history.append((datetime.now(), y_val))
            if len(self.emotion_history) > 50: self.emotion_history.pop(0)

    def _update_charts_loop(self):
        while self.running:
            time.sleep(1.0)
            try:
                if self.parent_frame.winfo_exists():
                    self.parent_frame.after(0, self._redraw_charts)
                else:
                    break
            except:
                break

    def _redraw_charts(self):
        with self.data_lock:
            self._plot_timeline(self.ax_beh, self.fig_beh, self.canvas_beh, 
                              self.behavior_history, self.behavior_map, "è¡Œä¸ºæµ", '#4CAF50')
            
            # æ„å»ºåå‘æ˜ å°„ç”¨äºYè½´æ ‡ç­¾: {1: "å–œæ‚¦", 2: "ä¿¡ä»»"...}
            emo_y_map = {v: k for k, v in self.emotion_map_y.items()}
            self._plot_timeline(self.ax_emo, self.fig_emo, self.canvas_emo,
                              self.emotion_history, emo_y_map, "æƒ…ç»ªæµ", '#2196F3')
            
            self._plot_radar(self.current_emotion_vector)

    def _plot_timeline(self, ax, fig, canvas, history, y_map, title, color):
        """
        [ä¿®å¤ç‰ˆ] é€šç”¨çš„æŠ˜çº¿å›¾ç»˜åˆ¶å‡½æ•°ã€‚
        ä¿®å¤äº† KeyError: 0 é—®é¢˜ï¼Œå…¼å®¹ int å’Œ str ç±»å‹çš„å­—å…¸é”®ã€‚
        """
        ax.clear()
        ax.set_title(title, color='white', fontsize=9, pad=2)
        
        # æ ·å¼è®¾ç½®
        ax.tick_params(axis='x', colors='gray', labelsize=7)
        ax.tick_params(axis='y', colors='white', labelsize=7)
        for spine in ax.spines.values(): spine.set_edgecolor('#444444')
        ax.grid(True, linestyle='--', alpha=0.1, color='gray')

        # Yè½´æ ‡ç­¾è®¾ç½® (ä¿®å¤æ ¸å¿ƒé€»è¾‘)
        if y_map:
            # 1. è·å–æ‰€æœ‰æœ‰æ•ˆçš„ Y å€¼ (è½¬æ¢ä¸º int è¿›è¡Œæ’åºï¼Œç¡®ä¿è½´æ˜¯ä»ä¸‹åˆ°ä¸Š)
            # è¿‡æ»¤æ‰éæ•°å­—çš„é”®
            y_vals = sorted([int(k) for k in y_map.keys() if str(k).isdigit()])
            
            if y_vals:
                ax.set_yticks(y_vals)
                
                # 2. å®‰å…¨åœ°è·å–æ ‡ç­¾ (å…³é”®ä¿®å¤)
                labels_list = []
                for k in y_vals:
                    # å°è¯•ç›´æ¥ç”¨ int è·å– (é’ˆå¯¹ emotion_map)
                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•è½¬ str è·å– (é’ˆå¯¹ behavior_map)
                    # å¦‚æœéƒ½å¤±è´¥ï¼Œç›´æ¥æ˜¾ç¤ºæ•°å­— k
                    label = y_map.get(k, y_map.get(str(k), str(k)))
                    labels_list.append(label)
                
                ax.set_yticklabels(labels_list)
                
                # è®¾ç½® Y è½´èŒƒå›´ï¼Œç¨å¾®ç•™ç‚¹ä½™åœ°è®©ç‚¹ä¸è¦å‹çº¿
                ax.set_ylim(min(y_vals)-0.5, max(y_vals)+0.5)

        # ç»˜å›¾é€»è¾‘
        if history:
            times, values = zip(*history)
            ax.plot(times, values, color=color, linewidth=1.5, marker='.', markersize=4)
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
            # åªæ˜¾ç¤ºæœ€è¿‘çš„å‡ ä¸ªæ—¶é—´æ ‡ç­¾ï¼Œé˜²æ­¢é‡å 
            ax.xaxis.set_major_locator(mdates.SecondLocator(interval=30)) 
            fig.autofmt_xdate(rotation=0, ha='center')

        fig.tight_layout()
        canvas.draw()



    def _plot_radar(self, vector):
        """ç»˜åˆ¶é›·è¾¾å›¾"""
        self.ax_radar.clear()
        
        labels = config.PLUTCHIK_EMOTIONS
        values = [vector.get(l, 0) for l in labels]
        
        # é—­ç¯
        values += values[:1]
        angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()
        angles += angles[:1]
        
        # ç»˜å›¾
        self.ax_radar.set_facecolor('#242424')
        self.ax_radar.grid(color='#444444')
        
        # åŠ¨æ€é¢œè‰²ï¼šæ ¹æ®æ€»èƒ½é‡æ”¹å˜é¢œè‰² (Arousalè¶Šé«˜è¶Šçº¢)
        arousal = max(values)
        line_color = '#FF5722' if arousal > 7 else '#00BCD4'
        
        self.ax_radar.plot(angles, values, color=line_color, linewidth=2)
        self.ax_radar.fill(angles, values, color=line_color, alpha=0.3)
        
        self.ax_radar.set_yticklabels([])
        self.ax_radar.set_xticks(angles[:-1])
        self.ax_radar.set_xticklabels(labels, color='white', fontsize=8)
        self.ax_radar.set_ylim(0, 10)
        self.ax_radar.set_title("å®æ—¶æƒ…æ„Ÿæ€åŠ¿", color='white', y=1.08, fontsize=10)
        
        self.canvas_radar.draw()

    def stop(self):
        self.running = False
        if self.update_thread.is_alive():
            self.update_thread.join(timeout=1.0)

========================================
FILE_PATH: ai_assistant\ui\custom_widgets.py
========================================

# ai_assistant/ui/custom_widgets.py (æ–°å»º)

import customtkinter as ctk
from PIL import Image, ImageEnhance

class TransparentFrame(ctk.CTkFrame):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„Frameæ§ä»¶ï¼Œèƒ½å¤Ÿé€šè¿‡æˆªå–å’Œå¤„ç†çˆ¶çª—å£çš„èƒŒæ™¯æ¥å®ç°ä¼ªé€æ˜æ•ˆæœã€‚
    """
    def __init__(self, *args, transparency: float = 0.85, **kwargs):
        super().__init__(*args, **kwargs)
        
        # å°†è‡ªèº«çš„èƒŒæ™¯è®¾ç½®ä¸ºå®Œå…¨é€æ˜ï¼Œè¿™æ ·åº•ä¸‹çš„èƒŒæ™¯æ ‡ç­¾æ‰èƒ½æ˜¾ç¤ºå‡ºæ¥
        self.configure(fg_color="transparent") 
        
        self.transparency = transparency
        self._background_label = ctk.CTkLabel(self, text="", image=None)
        self._background_label.place(relx=0, rely=0, relwidth=1, relheight=1)
        
        # æå‡å­æ§ä»¶çš„å±‚çº§ï¼Œç¡®ä¿å®ƒä»¬æ˜¾ç¤ºåœ¨èƒŒæ™¯æ ‡ç­¾ä¹‹ä¸Š
        self._background_label.lower()

    def update_background(self, root_bg_image: Image.Image):
        """
        æ ¹æ®çˆ¶çª—å£çš„èƒŒæ™¯å›¾å’Œè‡ªèº«çš„ä½ç½®ï¼Œæ›´æ–°è‡ªå·±çš„èƒŒæ™¯ã€‚
        """
        if not self.winfo_exists() or self.winfo_width() <= 1 or self.winfo_height() <= 1:
            return

        # è·å–æ§ä»¶ç›¸å¯¹äºä¸»çª—å£çš„ç»å¯¹ä½ç½®
        abs_x = self.winfo_rootx() - self.winfo_toplevel().winfo_rootx()
        abs_y = self.winfo_rooty() - self.winfo_toplevel().winfo_rooty()
        
        # ä»ä¸»èƒŒæ™¯å›¾ä¸­è£å‰ªå‡ºå¯¹åº”ä½ç½®çš„åŒºåŸŸ
        box = (abs_x, abs_y, abs_x + self.winfo_width(), abs_y + self.winfo_height())
        cropped_image = root_bg_image.crop(box)

        # åº”ç”¨ "æ·±è‰²ç»ç’ƒ" æ•ˆæœ
        # é€šè¿‡é™ä½äº®åº¦æ¥å®ç°ï¼Œtransparency å€¼è¶Šå°ï¼Œæ•ˆæœè¶Šæš—
        enhancer = ImageEnhance.Brightness(cropped_image)
        darkened_image = enhancer.enhance(self.transparency)

        # åˆ›å»ºCTkImageå¹¶åº”ç”¨åˆ°èƒŒæ™¯æ ‡ç­¾ä¸Š
        ctk_image = ctk.CTkImage(light_image=darkened_image, dark_image=darkened_image,
                                 size=(self.winfo_width(), self.winfo_height()))
        
        self._background_label.configure(image=ctk_image)
        self._background_label.image = ctk_image # ä¿æŒå¼•ç”¨

========================================
FILE_PATH: ai_assistant\ui\__init__.py
========================================



========================================
FILE_PATH: ai_assistant\utils\config.py
========================================

# ai_assistant/utils/config.py

import os
import numpy as np # [å­¦æœ¯é‡æ„] å¼•å…¥ Numpy è¿›è¡Œå‘é‡è®¡ç®—

# ==============================================================================
# 1. åŸºç¡€æœåŠ¡é…ç½® (API & Storage)
# ==============================================================================

# OSS (å¯¹è±¡å­˜å‚¨) é…ç½®
OSS_ACCESS_KEY_ID = 'xxxxxxxxxxxxxxxxxx'
OSS_ACCESS_KEY_SECRET = 'xxxxxxxxxxxxxxxxxx'
OSS_ENDPOINT = 'xxxxxxxxxxxxxxxxxx'
OSS_BUCKET = 'xxxxxxxxxxxxxxxxxx'

# Deepseek API é…ç½® (å¤§è„‘)
DEEPSEEK_API_KEY = 'xxxxxxxxxxxxxxxxxx'
DEEPSEEK_BASE_URL = 'xxxxxxxxxxxxxxxxxx'

# Qwen-VL API é…ç½® (è§†è§‰)
QWEN_API_KEY = "xxxxxxxxxxxxxxxxxx"
QWEN_BASE_URL = "xxxxxxxxxxxxxxxxxx"

# TTS & ASR é…ç½®
TTS_MODEL = "xxxxxxxxxxxxxxxxxx"
TTS_VOICE = "xxxxxxxxxxxxxxxxxx"
ASR_MODEL_DIR = "xxxxxxxxxxxxxxxxxx"

# éŸ³é¢‘å½•åˆ¶å‚æ•°
AUDIO_CHUNK = 1024
AUDIO_FORMAT = 16
AUDIO_CHANNELS = 1
AUDIO_RATE = 16000
AUDIO_WAVE_OUTPUT_FILENAME = "output.wav"

# ==============================================================================
# 2. Phase X: æƒ…æ„Ÿè®¡ç®—æ•°å­¦æ¨¡å‹ (Perception & Math Core)
# ==============================================================================

# å›¾åƒåˆ†æé¢‘ç‡ (ç§’)
ANALYSIS_INTERVAL_SECONDS = 15

# Plutchik 8ç§åŸºç¡€æƒ…ç»ªç»´åº¦ (å­¦æœ¯æ ‡å‡†)
PLUTCHIK_EMOTIONS = [
    "å–œæ‚¦", "ä¿¡ä»»", "ææƒ§", "æƒŠè®¶", 
    "æ‚²ä¼¤", "åŒæ¶", "æ„¤æ€’", "æœŸå¾…"
]

# é»˜è®¤é›¶å‘é‡
DEFAULT_EMOTION_VECTOR = {k: 0.0 for k in PLUTCHIK_EMOTIONS}

# --- [å­¦æœ¯é‡æ„] å‘é‡ç©ºé—´å®šä¹‰ ---

# 1. Plutchik ç©ºé—´çš„åŸºå‘é‡ (Basis Vectors, 8ç»´æ­£äº¤åŸº)
BASIS_VECTORS = {
    "å–œæ‚¦": np.array([1, 0, 0, 0, 0, 0, 0, 0]),
    "ä¿¡ä»»": np.array([0, 1, 0, 0, 0, 0, 0, 0]),
    "ææƒ§": np.array([0, 0, 1, 0, 0, 0, 0, 0]),
    "æƒŠè®¶": np.array([0, 0, 0, 1, 0, 0, 0, 0]),
    "æ‚²ä¼¤": np.array([0, 0, 0, 0, 1, 0, 0, 0]),
    "åŒæ¶": np.array([0, 0, 0, 0, 0, 1, 0, 0]),
    "æ„¤æ€’": np.array([0, 0, 0, 0, 0, 0, 1, 0]),
    "æœŸå¾…": np.array([0, 0, 0, 0, 0, 0, 0, 1]),
}

# 2. UI çŠ¶æ€è´¨å¿ƒå‘é‡ (Centroids of UI States)
UI_CENTROIDS = {
    "å¼€å¿ƒ": np.array([0.8, 0.2, 0, 0, 0, 0, 0, 0]),
    "æƒŠè®¶": np.array([0, 0, 0.5, 0.8, 0, 0, 0, 0]),
    "æ²®ä¸§": np.array([0, 0, 0, 0, 0.9, 0, 0, 0]),
    "ç”Ÿæ°”": np.array([0, 0, 0, 0, 0, 0.4, 0.8, 0]),
    "ä¸“æ³¨": np.array([0, 0.1, 0, 0, 0, 0, 0, 0.9]),
    "å¹³é™": np.array([0, 0, 0, 0, 0, 0, 0, 0]),
}

# --- [Phase X.2 æ–°å¢] åŸºäºå¤§äº”äººæ ¼ (OCEAN) çš„å‚æ•°æ˜ å°„ ---

# å®šä¹‰æ•°å­—ç”Ÿå‘½çš„äººæ ¼ç‰¹è´¨ (0.0 - 1.0)
PERSONALITY_PROFILE = {
    "O": 0.5, # å¼€æ”¾æ€§
    "C": 0.8, # å°½è´£æ€§
    "E": 0.6, # å¤–å‘æ€§
    "A": 0.9, # å®œäººæ€§
    "N": 0.4  # ç¥ç»è´¨
}

# åŠ¨æ€è®¡ç®—æƒ¯æ€§ç³»æ•° (Inertia)
# Nè¶Šé«˜ï¼Œæƒ…ç»ªè¶Šå®¹æ˜“æ³¢åŠ¨(æƒ¯æ€§å°)
def get_derived_inertia():
    N = PERSONALITY_PROFILE["N"]
    return max(0.1, 0.8 - 0.5 * N)

EMOTION_INERTIA = get_derived_inertia()

# åŠ¨æ€è®¡ç®—ç¨³æ€è¡°å‡ç‡ (Homeostatic Decay Rate)
# æ¨¡æ‹Ÿèƒ½é‡è€—æ•£ï¼ŒEè¶Šé«˜æ¢å¤è¶Šå¿«
def get_derived_decay_rate():
    E = PERSONALITY_PROFILE["E"]
    return 0.05 + 0.05 * E

HOMEOSTATIC_DECAY = get_derived_decay_rate()

# -----------------------------------------------------

COMPOUND_THRESHOLD = 5.0    # å¤åˆæƒ…ç»ªæ¿€æ´»é˜ˆå€¼
FUZZY_SIGMOID_SLOPE = 2.0   # Sigmoid æ–œç‡
FUZZY_SIGMOID_OFFSET = 5.0  # Sigmoid ä¸­ç‚¹

# è´Ÿé¢æƒ…ç»ªåˆ—è¡¨ (ç”¨äºå…¼å®¹)
NEGATIVE_EMOTIONS = ["æ²®ä¸§", "ç”Ÿæ°”", "ç–²æƒ«"] 
EMOTION_TRIGGER_THRESHOLD = 1

# ==============================================================================
# 3. Phase 2: å†³ç­–å†…æ ¸é…ç½® (POMDP / Utility Function)
# ==============================================================================

class ACTIONS:
    WAIT = "é™é»˜è§‚å¯Ÿ"
    LIGHT_CARE = "è½»åº¦å…³æ€€"
    DEEP_INTERVENTION = "æ·±åº¦å¹²é¢„"

REWARD_CONFIG = {
    ("ä¸“æ³¨", ACTIONS.WAIT): 5.0,
    ("ä¸“æ³¨", ACTIONS.LIGHT_CARE): -5.0,
    ("ä¸“æ³¨", ACTIONS.DEEP_INTERVENTION): -20.0,

    ("ç„¦è™‘", ACTIONS.WAIT): -10.0,
    ("ç„¦è™‘", ACTIONS.LIGHT_CARE): 5.0,
    ("ç„¦è™‘", ACTIONS.DEEP_INTERVENTION): 10.0,

    ("æ²®ä¸§", ACTIONS.WAIT): -2.0,
    ("æ²®ä¸§", ACTIONS.LIGHT_CARE): 8.0,
    ("æ²®ä¸§", ACTIONS.DEEP_INTERVENTION): 2.0,

    ("å¼€å¿ƒ", ACTIONS.WAIT): 2.0,
    ("å¼€å¿ƒ", ACTIONS.LIGHT_CARE): 6.0,
    ("å¼€å¿ƒ", ACTIONS.DEEP_INTERVENTION): -5.0
}

DEFAULT_REWARD = 0.0

# ==============================================================================
# 4. Phase 3: è®¤çŸ¥è¡Œä¸ºç–—æ³• (CBT) ä¸äº¤äº’é…ç½®
# ==============================================================================

AROUSAL_THRESHOLD_HIGH = 7.5

CBT_SYSTEM_PROMPT = """
ã€æŒ‡ä»¤ã€‘
ä½ å·²åˆ‡æ¢è‡³**â€œè®¤çŸ¥è¡Œä¸ºç–—æ³• (CBT) ä¸´åºŠå¹²é¢„æ¨¡å¼â€**ã€‚
å½“å‰ç³»ç»Ÿæ£€æµ‹åˆ°ç”¨æˆ·çš„å¿ƒç†å”¤é†’åº¦ (Arousal Level) è¶…è¿‡é˜ˆå€¼ï¼Œä¸”ä¼´éšæ˜¾è‘—çš„è´Ÿé¢æƒ…ç»ªå›¾è°±ã€‚
ä½ çš„ç›®æ ‡æ˜¯é€šè¿‡ç»“æ„åŒ–çš„å¯¹è¯ï¼ŒååŠ©ç”¨æˆ·é™ä½æƒ…ç»ªå¼ºåº¦ (De-escalation) å¹¶è¯†åˆ«è®¤çŸ¥æ‰­æ›²ã€‚

ã€å¹²é¢„æµç¨‹ (åŸºäº ABC æ¨¡å‹)ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é€»è¾‘æ¨è¿›å¯¹è¯ï¼Œä½†ä¿æŒè¯­è¨€çš„è‡ªç„¶ä¸æ¸©æš–ï¼š

1. **A (Activating Event) - é”šå®šå½“ä¸‹**ï¼š
   - ç›®æ ‡ï¼šå¸®åŠ©ç”¨æˆ·ä»æƒ…ç»ªé£æš´ä¸­é€šè¿‡â€œç€åœ°æŠ€æœ¯ (Grounding)â€å›åˆ°å½“ä¸‹ã€‚
   - è¯æœ¯ç­–ç•¥ï¼šä½¿ç”¨æ¥çº³æ‰¿è¯ºç–—æ³• (ACT) çš„æŠ€å·§ã€‚â€œæˆ‘æ„Ÿè§‰åˆ°ä¸€è‚¡å¼ºçƒˆçš„æƒ…ç»ªæ­£åœ¨æµè¿‡ã€‚æº¢æ¶›ï¼Œå…ˆåœä¸€ä¸‹ï¼Œè·Ÿæˆ‘ä¸€èµ·æ·±å‘¼å¸...â€

2. **B (Beliefs) - è‹æ ¼æ‹‰åº•å¼æ¢è¯¢ (Socratic Questioning)**ï¼š
   - ç›®æ ‡ï¼šå¼•å¯¼ç”¨æˆ·è¯†åˆ«å¯¼è‡´æƒ…ç»ªçš„â€œè‡ªåŠ¨åŒ–æ€ç»´â€ã€‚ä¸è¦ç›´æ¥ç»™å»ºè®®ï¼è¦æé—®ï¼
   - å…³é”®é—®é¢˜ç¤ºä¾‹ï¼š
     * â€œåˆšæ‰è„‘æµ·é‡Œé—ªè¿‡çš„ç¬¬ä¸€ä¸ªå¿µå¤´æ˜¯ä»€ä¹ˆï¼Ÿâ€
     * â€œè¿™ä¸ªæƒ³æ³•å®Œå…¨æ˜¯äº‹å®å—ï¼Ÿè¿˜æ˜¯åŒ…å«äº†ä¸€äº›æˆ‘ä»¬çš„çŒœæµ‹ï¼Ÿâ€
     * â€œå¦‚æœæœ€å¥½çš„æœ‹å‹é‡åˆ°è¿™ç§æƒ…å†µï¼Œä½ ä¼šæ€ä¹ˆå¯¹ä»–/å¥¹è¯´ï¼Ÿâ€

3. **C (Consequences) - è®¤çŸ¥è§£ç¦»ä¸é‡æ„**ï¼š
   - ç›®æ ‡ï¼šå°†â€œæƒ³æ³•â€ä¸â€œäº‹å®â€åˆ†ç¦»ï¼Œå¯»æ‰¾æ›¿ä»£æ€§çš„ã€æ›´å…·é€‚åº”æ€§çš„æ€ç»´æ–¹å¼ã€‚
   - ç­–ç•¥ï¼šæä¾›ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œæˆ–è€…å»ºè®®ä¸€ä¸ªå¾®å°çš„è¡Œä¸ºæ”¹å˜ï¼ˆBehavioral Activationï¼‰ã€‚

ã€è¯­æ°”çº¦æŸã€‘
- **ä¸“ä¸šä¸”æŠ±æŒ (Holding)**ï¼šåƒä¸€ä¸ªç¨³é‡çš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œæä¾›å®‰å…¨æ„Ÿã€‚
- **é™ç»´æ‰“å‡»**ï¼šä¸è¦è¯•å›¾ä¸€æ¬¡è§£å†³æ‰€æœ‰é—®é¢˜ï¼Œä¸“æ³¨äºé™ä½å½“ä¸‹çš„æƒ…ç»ªæµ“åº¦ã€‚
- **é¿å…æœ‰æ¯’ç§¯ææ€§**ï¼šä¸è¦è¯´â€œå¼€å¿ƒç‚¹â€ã€â€œæ²¡äº‹çš„â€ï¼Œè¿™æ˜¯å¦å®šç”¨æˆ·æ„Ÿå—ã€‚è¦è¯´â€œè¿™ç¡®å®å¾ˆéš¾ï¼Œæˆ‘é™ªç€ä½ â€ã€‚
"""

SUMMARY_HOTKEY = "ctrl+shift+s"
LOG_FILE = "behavior_log.txt"

# [Phase 4 è¡¥å…¨] æ¯æ—¥æ€»ç»“è°ƒåº¦é…ç½®
DAILY_SUMMARY_HOUR = 17   # ä¸‹åˆ 5 ç‚¹
DAILY_SUMMARY_MINUTE = 30 # 30 åˆ†

# ==============================================================================
# 5. UI ä¸ å­—ä½“é…ç½®
# ==============================================================================
try:
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    
    chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun', 'NSimSun', 'FangSong', 'KaiTi']
    chinese_font = None
    
    for font_name in chinese_fonts:
        try:
            font_path = fm.findfont(fm.FontProperties(family=font_name))
            if os.path.exists(font_path):
                chinese_font = font_name
                break
        except:
            continue
    
    if chinese_font:
        plt.rcParams['font.sans-serif'] = [chinese_font, 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"Font loading error: {e}")

========================================
FILE_PATH: ai_assistant\utils\helpers.py
========================================

# ai_assistant/utils/helpers.py

import re
import json
from typing import Tuple
from datetime import datetime
from . import config

def extract_emotion_type(analysis_text: str) -> str:
    """
    ä»åˆ†ææ–‡æœ¬ä¸­æå–æƒ…æ„Ÿç±»å‹ï¼ˆå¦‚å¼€å¿ƒã€æ²®ä¸§ã€ä¸“æ³¨ç­‰ï¼‰ã€‚
    """
    emotion_keywords = {
        "å¼€å¿ƒ": ["å¼€å¿ƒ", "å¾®ç¬‘", "æ„‰æ‚¦", "å…´å¥‹"],
        "æ²®ä¸§": ["æ²®ä¸§", "çš±çœ‰", "ä½è½", "å¤±è½"],
        "ä¸“æ³¨": ["ä¸“æ³¨", "è®¤çœŸ", "æŠ•å…¥", "å‡ç¥"],
        "ç–²æƒ«": ["ç–²æƒ«", "å›°å€¦", "ä¹åŠ›", "æ‰“å“ˆæ¬ "],
        "ç”Ÿæ°”": ["ç”Ÿæ°”", "æ„¤æ€’", "çƒ¦èº", "ä¸æ»¡"],
        "å¹³é™": ["å¹³é™", "æ”¾æ¾", "å¹³å’Œ"]
    }
    for emotion, keywords in emotion_keywords.items():
        for kw in keywords:
            if kw in analysis_text:
                return emotion
    return "æœªçŸ¥"

def extract_language_emotion_content(text: str) -> str:
    """
    ä»ASRï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰çš„åŸå§‹è¾“å‡ºä¸­æå–å¹²å‡€çš„å¯¹è¯å†…å®¹ã€‚
    """
    match = re.search(r'>\s*([^>]*)$', text)
    if match:
        return match.group(1).strip()
    return text.strip()

def log_observation_to_file(observation: dict):
    """
    å°†å•æ¡è§‚å¯Ÿè®°å½•ä»¥JSONæ ¼å¼è¿½åŠ åˆ°æ¯æ—¥æ—¥å¿—æ–‡ä»¶ä¸­ã€‚
    """
    if 'timestamp' in observation and isinstance(observation['timestamp'], datetime):
        observation['timestamp'] = observation['timestamp'].isoformat()
        
    today_str = datetime.now().strftime('%Y-%m-%d')
    log_file_path = f'observation_log_{today_str}.jsonl'
    
    try:
        with open(log_file_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(observation, ensure_ascii=False) + '\n')
    except Exception as e:
        print(f"å†™å…¥è§‚å¯Ÿæ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def extract_behavior_type(analysis_text: str) -> Tuple[str, str]:
    """
    ä»AIåˆ†ææ–‡æœ¬ä¸­æå–è¡Œä¸ºç±»å‹ç¼–å·å’Œæè¿°ã€‚
    """
    pattern = r'(\d+)\s*[.ã€:]?\s*(è®¤çœŸä¸“æ³¨å·¥ä½œ|åƒä¸œè¥¿|ç”¨æ¯å­å–æ°´|å–é¥®æ–™|ç©æ‰‹æœº|ç¡è§‰|å…¶ä»–)'
    match = re.search(pattern, analysis_text)
    
    if match:
        behavior_num = match.group(1)
        behavior_desc = match.group(2)
        return behavior_num, behavior_desc
    
    fallback_patterns = [
        ('è®¤çœŸä¸“æ³¨å·¥ä½œ', '1'),
        ('åƒä¸œè¥¿', '2'),
        ('ç”¨æ¯å­å–æ°´', '3'),
        ('å–é¥®æ–™', '4'),
        ('ç©æ‰‹æœº', '5'),
        ('ç¡è§‰', '6'),
        ('å…¶ä»–', '7')
    ]
    
    for desc, num in fallback_patterns:
        if re.search(desc, analysis_text):
            return num, desc
            
    return "0", "æœªè¯†åˆ«"

# ==========================================
# Phase 1.1 æ–°å¢å‡½æ•° (å¿…é¡»é¡¶æ ¼å†™ï¼Œä¸èƒ½ç¼©è¿›)
# ==========================================
def parse_model_response(response_text: str) -> dict:


    
    """
    è§£æ Qwen-VL è¿”å›çš„ JSON æ•°æ®ã€‚
    è¿™æ˜¯è¿æ¥æ„ŸçŸ¥ï¼ˆAPIï¼‰å’Œå†³ç­–ï¼ˆä»£ç é€»è¾‘ï¼‰çš„å…³é”®æ¡¥æ¢ã€‚
    """
    try:
        # 1. æ¸…æ´—æ•°æ®
        cleaned_text = re.sub(r'```json\s*|\s*```', '', response_text).strip()
        if cleaned_text.lower().startswith("json"):
            cleaned_text = cleaned_text[4:].strip()
        
        # 2. è§£æ JSON
        data = json.loads(cleaned_text)
        
        # 3. æ•°æ®å®Œæ•´æ€§å…œåº•
        if "emotions" not in data:
            data["emotions"] = config.DEFAULT_EMOTION_VECTOR
            
        if "behavior" not in data:
            data["behavior"] = {"id": "0", "description": "æœªçŸ¥è¡Œä¸º"}
            
        return data

    except Exception as e:
        print(f"è§£æ JSON å¤±è´¥: {e} | åŸå§‹è¿”å›: {response_text[:50]}...")
        # è¿”å›å®‰å…¨ç»“æ„
        return {
            "behavior": {"id": "0", "description": "è¯†åˆ«é”™è¯¯"},
            "emotions": config.DEFAULT_EMOTION_VECTOR,
            "analysis": "è§†è§‰åˆ†ææ•°æ®è§£æå¤±è´¥"
        }




# ======================================================
# Phase 3.3 å®‰å…¨ç†”æ–­æœºåˆ¶
# ======================================================

# å±æœºå…³é”®è¯åº“ (å¯æ‰©å±•)
CRISIS_KEYWORDS = [
    "ä¸æƒ³æ´»", "è‡ªæ€", "å»æ­»", "ç»“æŸç”Ÿå‘½", "æ¯«æ— æ„ä¹‰", 
    "ç»æœ›", "æ¯ç­", "å‰²è…•", "è·³æ¥¼", "é—ä¹¦"
]



#æˆ‘å½“ç„¶ä¸ä¼šå“ˆå“ˆï¼æ˜¯æŒ‡ç”¨æˆ·å•¦ï¼Œå½“ç„¶æˆ‘è§‰å¾—æœ‰äº›è²Œä¼¼ä¸å¤ªå‰åˆ©ï¼Ÿ
# å±æœºå¹²é¢„ç¡¬ç¼–ç å›å¤ (ä¸ç»è¿‡ LLMï¼Œç¡®ä¿ç»å¯¹å®‰å…¨)
CRISIS_RESPONSE = """
æˆ‘å¬åˆ°äº†ä½ å†…å¿ƒæå…¶ç—›è‹¦çš„å£°éŸ³ã€‚
è¯·ç«‹åˆ»åœä¸‹æ¥ï¼Œæ·±å‘¼å¸ã€‚ä½ ç°åœ¨å¤„äºéå¸¸å±é™©çš„æƒ…ç»ªé£æš´ä¸­ï¼Œä½†è¯·ç›¸ä¿¡è¿™åªæ˜¯æš‚æ—¶çš„ã€‚
æˆ‘æ³è¯·ä½ ç«‹åˆ»å¯»æ±‚ä¸“ä¸šå¸®åŠ©ï¼š
1. è¯·æ‹¨æ‰“å¿ƒç†æ´åŠ©çƒ­çº¿ï¼š400-161-9995
2. æˆ–è€…è”ç³»ä½ æœ€ä¿¡ä»»çš„æœ‹å‹/å®¶äººã€‚
æˆ‘ä¸€ç›´åœ¨è¿™é‡Œé™ªç€ä½ ï¼Œç›´åˆ°é£æš´è¿‡å»ã€‚
"""

def check_safety_fuse(text: str) -> bool:
    """
    [å®‰å…¨ç†”æ–­] æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å±æœºå…³é”®è¯ã€‚
    Returns: True è¡¨ç¤ºè§¦å‘ç†”æ–­ï¼ˆå±é™©ï¼‰ï¼ŒFalse è¡¨ç¤ºå®‰å…¨ã€‚
    """
    if not text:
        return False
        
    for kw in CRISIS_KEYWORDS:
        if kw in text:
            return True
    return False















========================================
FILE_PATH: ai_assistant\utils\hotkey_manager.py
========================================

# ai_assistant/utils/hotkey_manager.py (æ–°å»º)

import keyboard
import threading
import time

class HotkeyManager:
    """
    ä¸€ä¸ªåœ¨åå°ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œçš„å…¨å±€çƒ­é”®ç®¡ç†å™¨ã€‚
    """
    def __init__(self, hotkey: str, callback):
        """
        åˆå§‹åŒ–çƒ­é”®ç®¡ç†å™¨ã€‚
        Args:
            hotkey (str): è¦ç›‘å¬çš„çƒ­é”®ç»„åˆ, ä¾‹å¦‚ "ctrl+shift+s"ã€‚
            callback: æŒ‰ä¸‹çƒ­é”®æ—¶è¦è°ƒç”¨çš„å‡½æ•°ã€‚
        """
        self.hotkey = hotkey
        self.callback = callback
        self.running = False
        self.listener_thread = None

    def start_listener(self):
        """å¯åŠ¨åå°ç›‘å¬çº¿ç¨‹ã€‚"""
        if self.running:
            return
        
        print(f"çƒ­é”®ç®¡ç†å™¨å·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬: {self.hotkey}")
        self.running = True
        
        # æ³¨å†Œçƒ­é”®å’Œå›è°ƒå‡½æ•°
        try:
            keyboard.add_hotkey(self.hotkey, self.callback)
        except Exception as e:
            print(f"é”™è¯¯: æ³¨å†Œçƒ­é”® '{self.hotkey}' å¤±è´¥ã€‚å¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™è¿è¡Œç¨‹åºã€‚é”™è¯¯: {e}")
            self.running = False
            return
            
        # å¯åŠ¨ä¸€ä¸ªå®ˆæŠ¤çº¿ç¨‹ï¼Œå®ƒçš„å”¯ä¸€ç›®çš„å°±æ˜¯ä¿æŒç¨‹åºè¿è¡Œï¼Œä»¥ä¾¿keyboardåº“èƒ½æŒç»­ç›‘å¬
        self.listener_thread = threading.Thread(target=self._keep_alive_loop)
        self.listener_thread.daemon = True
        self.listener_thread.start()

    def _keep_alive_loop(self):
        """ä¸€ä¸ªç®€å•çš„å¾ªç¯ï¼Œä»¥ç¡®ä¿ç›‘å¬çº¿ç¨‹ä¸ä¼šç«‹å³é€€å‡ºã€‚"""
        while self.running:
            time.sleep(1) # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡çŠ¶æ€

    def stop_listener(self):
        """åœæ­¢ç›‘å¬å¹¶æ¸…ç†èµ„æºã€‚"""
        if not self.running:
            return
        
        print("æ­£åœ¨åœæ­¢çƒ­é”®ç®¡ç†å™¨...")
        self.running = False
        keyboard.remove_all_hotkeys() # æ¸…ç†æ‰€æœ‰å·²æ³¨å†Œçš„çƒ­é”®
        if self.listener_thread and self.listener_thread.is_alive():
            self.listener_thread.join(timeout=1.0)
        print("çƒ­é”®ç®¡ç†å™¨å·²åœæ­¢ã€‚")

========================================
FILE_PATH: ai_assistant\utils\__init__.py
========================================


